{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Full: Random forest with TFIDF + complexity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk import word_tokenize  \n",
    "from nltk.data import load  \n",
    "from nltk.stem import SnowballStemmer  \n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, accuracy_score\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish tokenization\n",
    "\n",
    "    - Spanish stopwords\n",
    "    - Stems\n",
    "    - Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword list to use\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "#Spanish stemmer:\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):  \n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "#Punctuation to remove\n",
    "non_words = list(punctuation)\n",
    "\n",
    "#Adding spanish punctuation\n",
    "non_words.extend(['¿', '¡'])  \n",
    "non_words.extend(map(str,range(10)))\n",
    "\n",
    "def tokenize(text):  \n",
    "    #Remove punctuation\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    #Tokenize\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    #Stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(  \n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = spanish_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pipe11/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "df_vectorized = tfidf_vectorizer.fit_transform(df['Text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dense dataframe Sparse, and Combine with TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoder\n",
    "labelencoder = LabelEncoder()\n",
    "df['Labels'] = labelencoder.fit_transform(df['Category'])\n",
    "y = df['Labels']\n",
    "\n",
    "# combine TF-IDF features with Dense features as a dataframe Sparse\n",
    "categorical_features = ['sentences', 'n_words', 'avg_words_sent', 'avg_word_size', 'TTR']\n",
    "\n",
    "X = hstack([csr_matrix(df[categorical_features].values), df_vectorized[0:]])\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into testing set, training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': True, \n",
    "    'max_depth': 95, \n",
    "    'max_features': 'auto', \n",
    "    'min_samples_leaf': 1, \n",
    "    'min_samples_split': 4, \n",
    "    'n_estimators': int(1800)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8366197183098592"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier(bootstrap = True, max_depth = 95, max_features = 'auto', min_samples_leaf = 1, \n",
    "                            min_samples_split = 4, n_estimators = 1800)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_rf.predict(X_test)\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pipe11/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=95, min_samples_split=4, n_estimators=1800)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/pipe11/TFM_fake_news_detector/data/corpus_spanish.csv', index_col = 0)\n",
    "\n",
    "# Label encoder\n",
    "labelencoder = LabelEncoder()\n",
    "df['Labels'] = labelencoder.fit_transform(df['Category'])\n",
    "y = df['Labels']\n",
    "\n",
    "df_text = pd.DataFrame()\n",
    "df_text['Text'] = text\n",
    "\n",
    "df = pd.concat([df['Text'], df_text], axis = 0)\n",
    "\n",
    "########## TFIDF-Vectorizer ##########\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(  \n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = spanish_stopwords)\n",
    "\n",
    "df_vectorized = tfidf_vectorizer.fit_transform(df['Text']) \n",
    "\n",
    "########## Combine features ##########\n",
    "\n",
    "# label encoder\n",
    "labelencoder = LabelEncoder()\n",
    "df['Labels'] = labelencoder.fit_transform(df['Category'])\n",
    "y = df['Labels']\n",
    "\n",
    "# combine TF-IDF features with Dense features as a dataframe Sparse\n",
    "categorical_features = ['sentences', 'n_words', 'avg_words_sent', 'avg_word_size', 'TTR']\n",
    "\n",
    "X = hstack([csr_matrix(df[categorical_features].values), df_vectorized[0:]])\n",
    "\n",
    "gc.collect();\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 43)\n",
    "\n",
    "########## Model ###########\n",
    "\n",
    "model_rf = RandomForestClassifier(bootstrap = True, max_depth = 95, max_features = 'auto', min_samples_leaf = 1, \n",
    "                        min_samples_split = 4, n_estimators = 1800)\n",
    "\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process one new and detect it with the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%` not found.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk import word_tokenize  \n",
    "from nltk.data import load  \n",
    "from nltk.stem import SnowballStemmer  \n",
    "from string import punctuation\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "####### Feature text extraction #######\n",
    "\n",
    "def extract_features(text):\n",
    "    \n",
    "    df = pd.read_csv('/home/pipe11/TFM_fake_news_detector/data/corpus_spanish.csv')\n",
    "\n",
    "    # Label encoder\n",
    "    labelencoder = LabelEncoder()\n",
    "    df['Labels'] = labelencoder.fit_transform(df['Category'])\n",
    "    y = df['Labels']\n",
    "\n",
    "    # index for later sparse Matrix\n",
    "    traindex = pd.RangeIndex(start = 0, stop = 971, step = 1)\n",
    "    predictindex = pd.RangeIndex(start = 0, stop = 972, step = 1)\n",
    "\n",
    "    df_text = pd.DataFrame([[text]], columns = ['Text'])\n",
    "    \n",
    "    # concat the new to predict at the end of the df\n",
    "    df_corpus = pd.concat([df[['Text']], df_text], axis = 0)\n",
    "\n",
    "    df_corpus.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df_features = pd.DataFrame()\n",
    "\n",
    "    n_sentences = []\n",
    "    n_words = []\n",
    "    avg_words_sent = []\n",
    "    avg_word_size = []\n",
    "    type_token_ratio = []\n",
    "    list_text = []\n",
    "\n",
    "    for i, row in df_corpus.iterrows():\n",
    "        text = df_corpus['Text'].iloc[i]\n",
    "\n",
    "        text = text.replace(r\"http\\S+\", \"\")\n",
    "        text = text.replace(r\"http\", \"\")\n",
    "        text = text.replace(r\"@\\S+\", \"\")\n",
    "        text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "        text = text.lower()\n",
    "\n",
    "        sent_tokens = nltk.sent_tokenize(text)\n",
    "\n",
    "        #Number of sentences\n",
    "        number_sentences = len(sent_tokens)\n",
    "\n",
    "        word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        stop_words = stopwords.words('spanish')\n",
    "        stop_words.extend(list(punctuation))\n",
    "        stop_words.extend(['¿', '¡', '\"', '``']) \n",
    "        stop_words.extend(map(str,range(10)))\n",
    "\n",
    "        filtered_tokens = [i for i in word_tokens if i not in stop_words]\n",
    "\n",
    "        #number of tokens\n",
    "        number_words = len(filtered_tokens)\n",
    "\n",
    "        # average words per sentence\n",
    "        avg_word_sentences = (float(number_words)/number_sentences)\n",
    "\n",
    "        # average word size\n",
    "        word_size = sum(len(word) for word in filtered_tokens) / number_words\n",
    "\n",
    "        # type token ratio\n",
    "        types = nltk.Counter(filtered_tokens)\n",
    "        TTR = (len(types) / number_words) * 100\n",
    "\n",
    "        n_sentences.append(number_sentences)\n",
    "        n_words.append(number_words)\n",
    "        avg_words_sent.append(avg_word_sentences)\n",
    "        avg_word_size.append(word_size)\n",
    "        type_token_ratio.append(TTR)\n",
    "        list_text.append(text)\n",
    "\n",
    "    df_features['sentences'] = n_sentences\n",
    "    df_features['n_words'] = n_words\n",
    "    df_features['avg_words_sent'] = avg_words_sent\n",
    "    df_features['avg_word_size'] = avg_word_size\n",
    "    df_features['TTR'] = type_token_ratio\n",
    "    df_features['Text'] = list_text\n",
    "\n",
    "    return df_features, y, traindex, predictindex\n",
    "\n",
    "####### TFIDF Transformation to text ########\n",
    "\n",
    "#Stopword list to use\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "#Spanish stemmer:\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):  \n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "#Punctuation to remove\n",
    "non_words = list(punctuation)\n",
    "\n",
    "#Adding spanish punctuation\n",
    "non_words.extend(['¿', '¡'])  \n",
    "non_words.extend(map(str,range(10)))\n",
    "\n",
    "def tokenize(text):  \n",
    "    #Remove punctuation\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    #Tokenize\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    #Stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems\n",
    "\n",
    "def tfidf_transformer(df_features):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(  \n",
    "                    analyzer = 'word',\n",
    "                    tokenizer = tokenize,\n",
    "                    lowercase = True,\n",
    "                    stop_words = spanish_stopwords)\n",
    "\n",
    "    text_vectorized = tfidf_vectorizer.fit_transform(df_features['Text'])\n",
    "\n",
    "    return text_vectorized\n",
    "\n",
    "####### Combine TF-IDF and dense features #######\n",
    "\n",
    "def feature_combiner(text_vectorized, df_features, traindex):\n",
    "\n",
    "    categorical_features = ['sentences', 'n_words', 'avg_words_sent', 'avg_word_size', 'TTR']\n",
    "\n",
    "    X = hstack([csr_matrix(df_features[categorical_features].loc[traindex, :].values), text_vectorized[0: traindex.shape[0]]])\n",
    "    X_text = hstack([csr_matrix(df_features[categorical_features].loc[[traindex.shape[0]], :].values), text_vectorized[traindex.shape[0]:]])\n",
    "\n",
    "    gc.collect();\n",
    "\n",
    "    return X, X_text\n",
    "\n",
    "####### Model training #######\n",
    "\n",
    "def model_predictor(X, y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 43)\n",
    "\n",
    "\n",
    "    model_rf = RandomForestClassifier(bootstrap = True, max_depth = 95, max_features = 'auto', min_samples_leaf = 1, \n",
    "                            min_samples_split = 4, n_estimators = 1800)\n",
    "\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    \n",
    "    return model_rf\n",
    "\n",
    "####### Outer function #######\n",
    "\n",
    "def fake_news_detector():\n",
    "    text = input('Paste the text content of a new: ')\n",
    "    df_features, y, traindex, predictindex = extract_features(text)\n",
    "    text_vectorized = tfidf_transformer(df_features)\n",
    "    X, X_text = feature_combiner(text_vectorized, df_features, traindex)\n",
    "    model_rf = model_predictor(X, y)\n",
    "    \n",
    "    if model_rf.predict(X_text) == 0:\n",
    "        return print('This new smells fake!')\n",
    "    else:\n",
    "        return print('This new is totally true!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste the text content of a new: La Casa Real ha puesto fin este lunes a la incertidumbre sobre el paradero del rey emérito. En un brevísimo comunicado, vía SMS, el Palacio de la Zarzuela comunica que “S. M. el Rey Juan Carlos ha indicado a la Casa de Su Majestad el Rey que comunique que el pasado día 3 del presente mes de agosto se trasladó a Emiratos Árabes Unidos (EUA), donde permanece en la actualidad”.  La Casa Real no especifica durante cuánto tiempo permanecerá el padre del Rey en este país del golfo Pérsico, ni si será su destino definitivo, lo que fuentes próximas a Juan Carlos I descartan. Sí quiere dejar claro que ha sido decisión del rey emérito dar a conocer dónde se encuentra, tal como indicó desde el principio la Casa Real, que alegó que correspondía a Juan Carlos I revelar su localización, si finalmente decidía hacerlo.  El paradero de Juan Carlos I se ha hecho público dos semanas después de que también la Casa del Real informara de que el rey emérito había comunicado a su hijo su “medida decisión de trasladarse, en estos momentos, fuera de España”, sin mayores precisiones.  El hecho de que el padre de Felipe VI viajara de incógnito daba argumentos a quienes a quienes lo presentaban como un fugitivo y había generado una creciente incomodidad en el Gobierno y en la propia Casa del Rey, poniendo en riesgo el objetivo de la salida de España de Juan Carlos I: salvaguardar a la institución del escándalo provocado por la investigación de sus cuentas en paraísos fiscales.  Juan Carlos I abandonó el Palacio de la Zarzuela el día 2 y pasó esa noche en casa de su amigo Pedro Campos en Sanxenxo (Pontevedra), tras cenar con sus compañeros de regatas. Al día siguiente, tomó un jet privado que le recogió en el aeropuerto de Vigo y le trasladó a Abu Dabi, capital de Emiratos, donde permanece todavía, según el comunicado de la Casa del Rey.  El alquiler del avión que le llevó a Emiratos, un modelo Global 6500 con matrícula 9H-VBIG de la compañía TAG Aviation, costó unos 140.000 euros, que no se sabe quien ha pagado, según reveló el diario Abc. El portal de noticias NIUS, de Mediaset, publicó una fotografía en la que se veía a Juan Carlos I, con mascarilla, descendiendo las escalerillas de un ‘jet’, supuestamente en el aeropuerto AZI de Abu Dabi, reservado para vuelos de negocios.  El rey Juan Carlos tiene una estrecha amistad con el príncipe heredero Mohamed bin Zayeb Al Hahyan, quien podría ser su anfitrión. La última vez que visitó Abu Dabi fue en noviembre del año pasado, para asistir al gran premio de Fórmula 1. En 2011, Mohammed bin Rashid, jeque de Dubái, otro de los emiratos que forma los EAU, le regaló dos deportivos de la marca Ferrari que el entonces Rey cedió a Patrimonio Nacional y finalmente fueron subastados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pipe11/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This new is totally true!\n"
     ]
    }
   ],
   "source": [
    "fake_news_detector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
