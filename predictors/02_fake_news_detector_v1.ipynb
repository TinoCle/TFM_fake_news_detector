{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk import word_tokenize  \n",
    "from nltk.data import load  \n",
    "from nltk.stem import SnowballStemmer  \n",
    "from string import punctuation\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "####### Feature text extraction #######\n",
    "\n",
    "def extract_features(text):\n",
    "    \n",
    "    df = pd.read_csv('/home/pipe11/TFM_fake_news_detector/data/corpus_spanish.csv')\n",
    "\n",
    "    # Label encoder\n",
    "    labelencoder = LabelEncoder()\n",
    "    df['Labels'] = labelencoder.fit_transform(df['Category'])\n",
    "    y = df['Labels']\n",
    "\n",
    "    # index for later sparse Matrix\n",
    "    traindex = pd.RangeIndex(start = 0, stop = 971, step = 1)\n",
    "    predictindex = pd.RangeIndex(start = 0, stop = 972, step = 1)\n",
    "\n",
    "    df_text = pd.DataFrame([[text]], columns = ['Text'])\n",
    "    \n",
    "    # concat the new to predict at the end of the df\n",
    "    df_corpus = pd.concat([df[['Text']], df_text], axis = 0)\n",
    "\n",
    "    df_corpus.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df_features = pd.DataFrame()\n",
    "\n",
    "    n_sentences = []\n",
    "    n_words = []\n",
    "    avg_words_sent = []\n",
    "    avg_word_size = []\n",
    "    type_token_ratio = []\n",
    "    list_text = []\n",
    "\n",
    "    for i, row in df_corpus.iterrows():\n",
    "        text = df_corpus['Text'].iloc[i]\n",
    "\n",
    "        text = text.replace(r\"http\\S+\", \"\")\n",
    "        text = text.replace(r\"http\", \"\")\n",
    "        text = text.replace(r\"@\\S+\", \"\")\n",
    "        text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "        text = text.lower()\n",
    "\n",
    "        sent_tokens = nltk.sent_tokenize(text)\n",
    "\n",
    "        #Number of sentences\n",
    "        number_sentences = len(sent_tokens)\n",
    "\n",
    "        word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        stop_words = stopwords.words('spanish')\n",
    "        stop_words.extend(list(punctuation))\n",
    "        stop_words.extend(['¿', '¡', '\"', '``']) \n",
    "        stop_words.extend(map(str,range(10)))\n",
    "\n",
    "        filtered_tokens = [i for i in word_tokens if i not in stop_words]\n",
    "\n",
    "        #number of tokens\n",
    "        number_words = len(filtered_tokens)\n",
    "\n",
    "        # average words per sentence\n",
    "        avg_word_sentences = (float(number_words)/number_sentences)\n",
    "\n",
    "        # average word size\n",
    "        word_size = sum(len(word) for word in filtered_tokens) / number_words\n",
    "\n",
    "        # type token ratio\n",
    "        types = nltk.Counter(filtered_tokens)\n",
    "        TTR = (len(types) / number_words) * 100\n",
    "\n",
    "        n_sentences.append(number_sentences)\n",
    "        n_words.append(number_words)\n",
    "        avg_words_sent.append(avg_word_sentences)\n",
    "        avg_word_size.append(word_size)\n",
    "        type_token_ratio.append(TTR)\n",
    "        list_text.append(text)\n",
    "\n",
    "    df_features['sentences'] = n_sentences\n",
    "    df_features['n_words'] = n_words\n",
    "    df_features['avg_words_sent'] = avg_words_sent\n",
    "    df_features['avg_word_size'] = avg_word_size\n",
    "    df_features['TTR'] = type_token_ratio\n",
    "    df_features['Text'] = list_text\n",
    "\n",
    "    return df_features, y, traindex, predictindex\n",
    "\n",
    "####### TFIDF Transformation to text ########\n",
    "\n",
    "#Stopword list to use\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "#Spanish stemmer:\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):  \n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "#Punctuation to remove\n",
    "non_words = list(punctuation)\n",
    "\n",
    "#Adding spanish punctuation\n",
    "non_words.extend(['¿', '¡'])  \n",
    "non_words.extend(map(str,range(10)))\n",
    "\n",
    "def tokenize(text):  \n",
    "    #Remove punctuation\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    #Tokenize\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    #Stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems\n",
    "\n",
    "def tfidf_transformer(df_features):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(  \n",
    "                    analyzer = 'word',\n",
    "                    tokenizer = tokenize,\n",
    "                    lowercase = True,\n",
    "                    stop_words = spanish_stopwords)\n",
    "\n",
    "    text_vectorized = tfidf_vectorizer.fit_transform(df_features['Text'])\n",
    "\n",
    "    return text_vectorized\n",
    "\n",
    "####### Combine TF-IDF and dense features #######\n",
    "\n",
    "def feature_combiner(text_vectorized, df_features, traindex):\n",
    "\n",
    "    categorical_features = ['sentences', 'n_words', 'avg_words_sent', 'avg_word_size', 'TTR']\n",
    "\n",
    "    X = hstack([csr_matrix(df_features[categorical_features].loc[traindex, :].values), text_vectorized[0: traindex.shape[0]]])\n",
    "    X_text = hstack([csr_matrix(df_features[categorical_features].loc[[traindex.shape[0]], :].values), text_vectorized[traindex.shape[0]:]])\n",
    "\n",
    "    gc.collect();\n",
    "\n",
    "    return X, X_text\n",
    "\n",
    "####### Model training #######\n",
    "\n",
    "def model_predictor(X, y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 43)\n",
    "\n",
    "\n",
    "    model_rf = RandomForestClassifier(bootstrap = True, max_depth = 95, max_features = 'auto', min_samples_leaf = 1, \n",
    "                            min_samples_split = 4, n_estimators = 1800)\n",
    "\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    \n",
    "    return model_rf\n",
    "\n",
    "####### Outer function #######\n",
    "\n",
    "def fake_news_detector():\n",
    "    text = input('Paste the text content of a new: ')\n",
    "    df_features, y, traindex, predictindex = extract_features(text)\n",
    "    text_vectorized = tfidf_transformer(df_features)\n",
    "    X, X_text = feature_combiner(text_vectorized, df_features, traindex)\n",
    "    model_rf = model_predictor(X, y)\n",
    "    \n",
    "    if model_rf.predict(X_text) == 0:\n",
    "        return print('This new smells fake!')\n",
    "    else:\n",
    "        return print('This new is totally true!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
