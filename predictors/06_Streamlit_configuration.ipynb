{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a streamlit app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Using in my case streamlit version 0.65.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import FreqDist\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/95/c1f097bfd0ea06f97d02e09e6e0af9bfa4da2c1e761112d5916bfd3bf846/streamlit-0.65.2-py2.py3-none-any.whl (7.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.2MB 166kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting base58 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/03/58572025c77b9e6027155b272a1b96298e711cd4f95c24967f7137ab0c4b/base58-2.0.1-py3-none-any.whl\n",
      "Collecting boto3 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/88/cc/8d50129435182a8966d812b0843b81ea33cf96faa4a4abeab8400fcf9eb8/boto3-1.14.48.tar.gz (97kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 2.3MB/s a 0:00:011\n",
      "\u001b[?25hCollecting click>=7.0 (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl\n",
      "Collecting tzlocal (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/5d/94/d47b0fd5988e6b7059de05720a646a2930920fff247a826f61674d436ba4/tzlocal-2.1-py2.py3-none-any.whl\n",
      "Collecting pandas>=0.21.0 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/c6/9ac4ae44c24c787a1738e5fb34dd987ada6533de5905a041aa6d5bea4553/pandas-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.5MB 125kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting requests (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl\n",
      "Collecting numpy (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pillow>=6.2.0 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/30/bf/92385b4262178ca22b34f82e0e09c2922eb351fe39f3cc7b8ba9ea555b41/Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.2MB 445kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting altair>=3.2.0 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/01/55/0bb2226e34f21fa549c3f4557b4f154a5632f61132a969da17c95ca8eab9/altair-4.1.0-py3-none-any.whl (727kB)\n",
      "\u001b[K    100% |████████████████████████████████| 737kB 1.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting botocore>=1.13.44 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/6c/3222ef7fbd77d89a374844b6f718f21993a7704496f4bf52251d573e2049/botocore-1.17.48-py2.py3-none-any.whl (6.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.5MB 171kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting enum-compat (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/55/ae/467bc4509246283bb59746e21a1a2f5a8aecbef56b1fa6eaca78cd438c8b/enum_compat-0.0.3-py3-none-any.whl\n",
      "Collecting astor (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting blinker (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz\n",
      "Collecting watchdog (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz\n",
      "Collecting tornado>=5.0 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/84/119a46d494f008969bf0c775cb2c6b3579d3c4cc1bb1b41a022aa93ee242/tornado-6.0.4.tar.gz (496kB)\n",
      "\u001b[K    100% |████████████████████████████████| 501kB 1.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.0 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/30/79/510974552cebff2ba04038544799450defe75e96ea5f1675dbf72cc8744f/protobuf-3.13.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 745kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting packaging (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
      "Collecting pyarrow (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/3a/9b/887d1d03d3d43706dee3a71cdad9f9bbb8fe74fc93d8db5d663f5bf34e48/pyarrow-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (16.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 16.6MB 72kB/s eta 0:00:011    96% |██████████████████████████████▉ | 16.0MB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools>=4.0 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\n",
      "Collecting toml (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/9f/e1/1b40b80f2e1663a6b9f497123c11d7d988c0919abbf3c3f2688e448c5363/toml-0.10.1-py2.py3-none-any.whl\n",
      "Collecting pydeck>=0.1.dev5 (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/51/1e/296f4108bf357e684617a776ecaf06ee93b43e30c35996dfac1aa985aa6c/pydeck-0.5.0b1-py2.py3-none-any.whl (4.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.4MB 309kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil (from streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting validators (from streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/89/3b/23e14394d0a719d1a9f2e1944a1d227ac7107a3383aa7e8eba60003e7266/validators-0.18.0-py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Collecting pytz (from tzlocal->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl\n",
      "Collecting chardet<4,>=3.0.2 (from requests->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5 (from requests->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl\n",
      "Collecting jsonschema (from altair>=3.2.0->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl\n",
      "Collecting toolz (from altair>=3.2.0->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/8e/037b9ba5c6a5739ef0dcde60578c64d49f45f64c5e5e886531bfbc39157f/toolz-0.10.0.tar.gz\n",
      "Collecting jinja2 (from altair>=3.2.0->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl\n",
      "Collecting entrypoints (from altair>=3.2.0->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/ac/c6/44694103f8c221443ee6b0041f69e2740d89a25641e62fb4f2ee568f2f9c/entrypoints-0.3-py2.py3-none-any.whl\n",
      "Collecting docutils<0.16,>=0.10 (from botocore>=1.13.44->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 907kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pathtools>=0.1.1 (from watchdog->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
      "Collecting six>=1.9 (from protobuf>=3.6.0->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting setuptools (from protobuf>=3.6.0->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/a9/5dc32465951cf4812e9e93b4ad2d314893c2fa6d5f66ce5c057af6e76d85/setuptools-49.6.0-py3-none-any.whl\n",
      "Collecting pyparsing>=2.0.2 (from packaging->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n",
      "Collecting ipykernel>=5.1.2; python_version >= \"3.4\" (from pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/52/19/c2812690d8b340987eecd2cbc18549b1d130b94c5d97fcbe49f5f8710edf/ipykernel-5.3.4-py3-none-any.whl (120kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ipywidgets>=7.0.0 (from pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/56/a0/dbcf5881bb2f51e8db678211907f16ea0a182b232c591a6d6f276985ca95/ipywidgets-7.5.1-py2.py3-none-any.whl\n",
      "Collecting traitlets>=4.3.2 (from pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/ca/ab/872a23e29cec3cf2594af7e857f18b687ad21039c1f9b922fac5b9b142d5/traitlets-4.3.3-py2.py3-none-any.whl\n",
      "Collecting decorator>=3.4.0 (from validators->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting pyrsistent>=0.14.0 (from jsonschema->altair>=3.2.0->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/9f/0d/cbca4d0bbc5671822a59f270e4ce3f2195f8a899c97d0d5abb81b191efb5/pyrsistent-0.16.0.tar.gz\n",
      "Collecting importlib-metadata; python_version < \"3.8\" (from jsonschema->altair>=3.2.0->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/8e/58/cdea07eb51fc2b906db0968a94700866fc46249bdc75cac23f9d13168929/importlib_metadata-1.7.0-py2.py3-none-any.whl\n",
      "Collecting attrs>=17.4.0 (from jsonschema->altair>=3.2.0->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/e1/3aa3b03e7643ffd6e499b203fd2a44f79893443e8b0b520d05d3e5c638d5/attrs-20.1.0-py2.py3-none-any.whl (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=0.23 (from jinja2->altair>=3.2.0->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting ipython>=5.0.0 (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/6a/210816c943c9aeeb29e4e18a298f14bf0e118fe222a23e13bfcc2d41b0a4/ipython-7.16.1-py3-none-any.whl (785kB)\n",
      "\u001b[K    100% |████████████████████████████████| 788kB 978kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-client (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/2e/6d48ae4ef0c9aa1383b3186349472a01bb38dacb2162a4a4370525d3f2a4/jupyter_client-6.1.6-py3-none-any.whl (108kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 2.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting nbformat>=4.2.0 (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/4d/d1/b568bd35f95321f152f594b3647cd080e96d3347843ff2fa34dce871b8bf/nbformat-5.0.7-py3-none-any.whl (170kB)\n",
      "\u001b[K    100% |████████████████████████████████| 174kB 1.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting widgetsnbextension~=3.5.0 (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/6c/7b/7ac231c20d2d33c445eaacf8a433f4e22c60677eb9776c7c5262d7ddee2d/widgetsnbextension-3.5.1-py2.py3-none-any.whl\n",
      "Collecting ipython-genutils (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->jsonschema->altair>=3.2.0->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
      "Collecting pickleshare (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl\n",
      "Collecting backcall (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/1c/ff6546b6c12603d8dd1070aa3c3d273ad4c07f5771689a7b69a550e8c951/backcall-0.2.0-py2.py3-none-any.whl\n",
      "Collecting jedi>=0.10 (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/d4/36136b18daae06ad798966735f6c3fb96869c1be9f8245d2a8f556e40c36/jedi-0.17.2-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 719kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pygments (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/68/106af3ae51daf807e9cdcba6a90e518954eb8b70341cee52995540a53ead/Pygments-2.6.1-py3-none-any.whl (914kB)\n",
      "\u001b[K    100% |████████████████████████████████| 921kB 884kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pexpect; sys_platform != \"win32\" (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/72/65/a3ef98b56d57a6d0a04cea5810ecbf3700a225d296ca298b3442dddebb42/prompt_toolkit-3.0.6-py3-none-any.whl (354kB)\n",
      "\u001b[K    100% |████████████████████████████████| 358kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-core>=4.6.0 (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/63/0d/df2d17cdf389cea83e2efa9a4d32f7d527ba78667e0153a8e676e957b2f7/jupyter_core-4.6.3-py2.py3-none-any.whl\n",
      "Collecting pyzmq>=13 (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/56/ff/34bf45e5cf8367edcf4946b26690f0982b3ec701b0a655edfe562d29e246/pyzmq-19.0.2-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.1MB 993kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting notebook>=4.4.1 (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/00/1be79c61e2dfedb29c430b0e08f9fd2cb6ee4f6be92ba6f185dd6bb00052/notebook-6.1.3-py3-none-any.whl (9.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 9.4MB 146kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting parso<0.8.0,>=0.7.0 (from jedi>=0.10->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/d1/e635bdde32890db5aeb2ffbde17e74f68986305a4466b0aa373b861e3f00/parso-0.7.1-py2.py3-none-any.whl (109kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 2.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ptyprocess>=0.5 (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached https://files.pythonhosted.org/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whl\n",
      "Collecting wcwidth (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/59/7c/e39aca596badaf1b78e8f547c807b04dae603a433d3e7a7e04d67f2ef3e5/wcwidth-0.2.5-py2.py3-none-any.whl\n",
      "Collecting Send2Trash (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/49/46/c3dc27481d1cc57b9385aff41c474ceb7714f7935b1247194adae45db714/Send2Trash-1.5.0-py3-none-any.whl\n",
      "Collecting argon2-cffi (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/e0/d7/5da06217807106ed6d7b4f5ccb8ec5e3f9ec969217faad4b5d1af0b55101/argon2_cffi-20.1.0-cp35-abi3-manylinux1_x86_64.whl (97kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 1.3MB/s a 0:00:011\n",
      "\u001b[?25hCollecting terminado>=0.8.3 (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/ff/96/1d9a2c23990aea8f8e0b5c3b6627d03196a73771a17a2d9860bbe9823ab6/terminado-0.8.3-py2.py3-none-any.whl\n",
      "Collecting prometheus-client (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/3f/0e/554a265ffdc56e1494ef08e18f765b0cdec78797f510c58c45cf37abb4f4/prometheus_client-0.8.0-py2.py3-none-any.whl\n",
      "Collecting nbconvert (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/79/6c/05a569e9f703d18aacb89b7ad6075b404e8a4afde2c26b73ca77bb644b14/nbconvert-5.6.1-py2.py3-none-any.whl\n",
      "Collecting cffi>=1.0.0 (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/82/7e/9cc46f072c9a414b5a6e08c5c2da5db3bff2601e69c4a6d4f6a34e6f9cfc/cffi-1.14.2-cp36-cp36m-manylinux1_x86_64.whl (400kB)\n",
      "\u001b[K    100% |████████████████████████████████| 409kB 1.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pandocfilters>=1.4.1 (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/4c/ea/236e2584af67bb6df960832731a6e5325fd4441de001767da328c33368ce/pandocfilters-1.4.2.tar.gz\n",
      "Collecting mistune<2,>=0.8.1 (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/09/ec/4b43dae793655b7d8a25f76119624350b4d65eb663459eb9603d7f1f0345/mistune-0.8.4-py2.py3-none-any.whl\n",
      "Collecting defusedxml (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/06/74/9b387472866358ebc08732de3da6dc48e44b0aacd2ddaa5cb85ab7e986a2/defusedxml-0.6.0-py2.py3-none-any.whl\n",
      "Collecting testpath (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/1b/9e/1a170feaa54f22aeb5a5d16c9015e82234275a3c8ab630b552493f9cb8a9/testpath-0.4.4-py2.py3-none-any.whl\n",
      "Collecting bleach (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/9a/1e/7d6cb3b27cd2c490558349ca5d5cc05b390b017da1c704cac807ac8bd9fb/bleach-3.1.5-py2.py3-none-any.whl\n",
      "Collecting pycparser (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/e7/d9c3a176ca4b02024debf82342dab36efadfc5776f9c8db077e8f6e71821/pycparser-2.20-py2.py3-none-any.whl (112kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting webencodings (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit)\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: boto3, blinker, watchdog, tornado, toolz, pathtools, pyrsistent, pandocfilters\n",
      "  Running setup.py bdist_wheel for boto3 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/pipe11/.cache/pip/wheels/5a/8b/e0/96a564beb67f86e54a15096020c3bfc955f464fe633373e242\n",
      "  Running setup.py bdist_wheel for blinker ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/pipe11/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n",
      "  Running setup.py bdist_wheel for watchdog ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/pipe11/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
      "  Running setup.py bdist_wheel for tornado ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/pipe11/.cache/pip/wheels/93/84/2f/409c7b2bb3afc3aa727f7ee8787975e0793f74d1165f4d0104\n",
      "  Running setup.py bdist_wheel for toolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/pipe11/.cache/pip/wheels/e1/8b/65/3294e5b727440250bda09e8c0153b7ba19d328f661605cb151\n",
      "  Running setup.py bdist_wheel for pathtools ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/pipe11/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
      "  Running setup.py bdist_wheel for pyrsistent ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/pipe11/.cache/pip/wheels/c2/85/ad/bc6d41e2c4b35c9fdfed48f0fcd411ffc4164e67755ddf9ebb\n",
      "  Running setup.py bdist_wheel for pandocfilters ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/pipe11/.cache/pip/wheels/39/01/56/f1b08a6275acc59e846fa4c1e1b65dbc1919f20157d9e66c20\n",
      "Successfully built boto3 blinker watchdog tornado toolz pathtools pyrsistent pandocfilters\n",
      "Installing collected packages: base58, six, python-dateutil, urllib3, jmespath, docutils, botocore, s3transfer, boto3, click, pytz, tzlocal, numpy, pandas, chardet, idna, certifi, requests, pillow, pyrsistent, setuptools, zipp, importlib-metadata, attrs, jsonschema, toolz, MarkupSafe, jinja2, entrypoints, altair, enum-compat, astor, blinker, pathtools, watchdog, tornado, protobuf, pyparsing, packaging, pyarrow, cachetools, toml, decorator, ipython-genutils, traitlets, pickleshare, backcall, parso, jedi, pygments, ptyprocess, pexpect, wcwidth, prompt-toolkit, ipython, jupyter-core, pyzmq, jupyter-client, ipykernel, nbformat, Send2Trash, pycparser, cffi, argon2-cffi, terminado, prometheus-client, pandocfilters, mistune, defusedxml, testpath, webencodings, bleach, nbconvert, notebook, widgetsnbextension, ipywidgets, pydeck, validators, streamlit\n",
      "Successfully installed MarkupSafe-1.1.1 Send2Trash-1.5.0 altair-4.1.0 argon2-cffi-20.1.0 astor-0.8.1 attrs-20.1.0 backcall-0.2.0 base58-2.0.1 bleach-3.1.5 blinker-1.4 boto3-1.14.48 botocore-1.17.48 cachetools-4.1.1 certifi-2020.6.20 cffi-1.14.2 chardet-3.0.4 click-7.1.2 decorator-4.4.2 defusedxml-0.6.0 docutils-0.15.2 entrypoints-0.3 enum-compat-0.0.3 idna-2.10 importlib-metadata-1.7.0 ipykernel-5.3.4 ipython-7.16.1 ipython-genutils-0.2.0 ipywidgets-7.5.1 jedi-0.17.2 jinja2-2.11.2 jmespath-0.10.0 jsonschema-3.2.0 jupyter-client-6.1.6 jupyter-core-4.6.3 mistune-0.8.4 nbconvert-5.6.1 nbformat-5.0.7 notebook-6.1.3 numpy-1.19.1 packaging-20.4 pandas-1.1.1 pandocfilters-1.4.2 parso-0.7.1 pathtools-0.1.2 pexpect-4.8.0 pickleshare-0.7.5 pillow-7.2.0 prometheus-client-0.8.0 prompt-toolkit-3.0.6 protobuf-3.13.0 ptyprocess-0.6.0 pyarrow-1.0.1 pycparser-2.20 pydeck-0.5.0b1 pygments-2.6.1 pyparsing-2.4.7 pyrsistent-0.16.0 python-dateutil-2.8.1 pytz-2020.1 pyzmq-19.0.2 requests-2.24.0 s3transfer-0.3.3 setuptools-49.6.0 six-1.15.0 streamlit-0.65.2 terminado-0.8.3 testpath-0.4.4 toml-0.10.1 toolz-0.10.0 tornado-6.0.4 traitlets-4.3.3 tzlocal-2.1 urllib3-1.25.10 validators-0.18.0 watchdog-0.10.3 wcwidth-0.2.5 webencodings-0.5.1 widgetsnbextension-3.5.1 zipp-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit, version 0.65.2\r\n"
     ]
    }
   ],
   "source": [
    "!streamlit --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index:\n",
    "### 1. Functions for the app:\n",
    "\n",
    "- [`Get news_features_function()`](#Get-news_features-function:-Necessary-to-extract-features-from-the-headline-and-new's-content)\n",
    "- [`Get_predictions_function()`](#Get_predictions_function)\n",
    "\n",
    "### [2. Extract news with url](#2.-Extract-news-with-url)\n",
    "\n",
    "### [3. Streamlit configuration](#3.-Streamlit-config)\n",
    "\n",
    "### [4. Full script](#4.-Full-script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_news_features function()`: Necessary to extract features from the headline and new's content\n",
    "Also we will need `getn_syllables()`if we want that `get_news_features function()` function works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @st.cache(show_spinner = False)\n",
    "def get_nsyllables(text):\n",
    "    from syltippy import syllabize\n",
    "    \n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(r'[^ \\nA-Za-z0-9ÁÉÍÓÚÑáéíóúñ/]+', '', text)\n",
    "    \n",
    "    n_syllables = len(syllabize(text)[0])\n",
    "    \n",
    "    return n_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @st.cache(show_spinner = False)\n",
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "def get_news_features(headline, text):\n",
    "    \n",
    "    nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "    ## headline ##\n",
    "    headline = re.sub(r\"http\\S+\", \"\", headline)\n",
    "    headline = re.sub(r\"http\", \"\", headline)\n",
    "    headline = re.sub(r\"@\\S+\", \"\", headline)\n",
    "    headline = re.sub(\"\\n\", \" \", headline)\n",
    "    headline = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", headline)\n",
    "    headline = headline.replace(r\"*NUMBER*\", \"número\")\n",
    "    headline = headline.replace(r\"*PHONE*\", \"número\")\n",
    "    headline = headline.replace(r\"*EMAIL*\", \"email\")\n",
    "    headline = headline.replace(r\"*URL*\", \"url\")\n",
    "    headline_new = headline.lower()\n",
    "    doc_h = nlp(headline_new)\n",
    "\n",
    "    list_tokens_h = []\n",
    "    list_tags_h = []\n",
    "\n",
    "    for sentence_h in doc_h.sents:\n",
    "        for token in sentence_h:\n",
    "            list_tokens_h.append(token.text)\n",
    "\n",
    "    fdist_h = FreqDist(list_tokens_h)\n",
    "    syllables_h = get_nsyllables(headline)\n",
    "    words_h = len(list_tokens_h)\n",
    "\n",
    "    # headline complexity features\n",
    "    avg_word_size_h = round(sum(len(word) for word in list_tokens_h) / words_h, 2)\n",
    "    avg_syllables_word_h = round(syllables_h / words_h, 2)\n",
    "    unique_words_h = round((len(fdist_h.hapaxes()) / words_h) * 100, 2)\n",
    "    mltd_h = round(ld.mtld(list_tokens_h), 2)\n",
    "    ttr_h = round(ld.ttr(list_tokens_h) * 100, 2)\n",
    "\n",
    "    ## text content##     \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"http\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "\n",
    "    # to later calculate upper case letters ratio\n",
    "    alph = list(filter(str.isalpha, text))\n",
    "    text_lower = text.lower()\n",
    "    doc = nlp(text_lower)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    list_entities = []\n",
    "    sents = 0\n",
    "\n",
    "    for entity in doc.ents:\n",
    "        list_entities.append(entity.label_)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "\n",
    "    # Calculate entities, pos, tag, freq, syllables, words and quotes\n",
    "    entities = len(list_entities)\n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "    syllables = get_nsyllables(text)\n",
    "    words = len(list_tokens)\n",
    "    quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "\n",
    "    # complexity features\n",
    "    avg_word_sentence = round(words / sents, 2)\n",
    "    avg_word_size = round(sum(len(word) for word in list_tokens) / words, 2)\n",
    "    avg_syllables_word = round(syllables / words, 2)\n",
    "    unique_words = round((len(fdist.hapaxes()) / words) * 100, 2)\n",
    "    ttr = round(ld.ttr(list_tokens) * 100, 2)\n",
    "\n",
    "    # readability spanish test\n",
    "    huerta_score = round(206.84 - (60 * avg_syllables_word) - (1.02 * avg_word_sentence), 2)\n",
    "    szigriszt_score = round(206.835 - ((62.3 * syllables) / words) - (words / sents), 2)\n",
    "\n",
    "    # stylometric features\n",
    "    mltd = round(ld.mtld(list_tokens), 2)\n",
    "    upper_case_ratio = round(sum(map(str.isupper, alph)) / len(alph) * 100, 2)\n",
    "    entity_ratio = round((entities / words) * 100, 2)\n",
    "    quotes_ratio = round((quotes / words) * 100, 2)\n",
    "    propn_ratio = round((n_pos['PROPN'] / words) * 100 , 2)\n",
    "    noun_ratio = round((n_pos['NOUN'] / words) * 100, 2) \n",
    "    pron_ratio = round((n_pos['PRON'] / words) * 100, 2)\n",
    "    adp_ratio = round((n_pos['ADP'] / words) * 100, 2)\n",
    "    det_ratio = round((n_pos['DET'] / words) * 100, 2)\n",
    "    punct_ratio = round((n_pos['PUNCT'] / words) * 100, 2)\n",
    "    verb_ratio = round((n_pos['VERB'] / words) * 100, 2)\n",
    "    adv_ratio = round((n_pos['ADV'] / words) * 100, 2)\n",
    "    sym_ratio = round((n_tag['SYM'] / words) * 100, 2)\n",
    "\n",
    "    # create df_features\n",
    "    df_features = pd.DataFrame({'words_h': words_h, 'word_size_h': [avg_word_size_h],'avg_syllables_word_h': [avg_syllables_word_h],\n",
    "                                'unique_words_h': [unique_words_h], 'ttr_h': ttr_h, 'mltd_h': [mltd_h], 'sents': sents, 'words': words,\n",
    "                                'avg_words_sent': [avg_word_sentence], 'avg_word_size': [avg_word_size], \n",
    "                                'avg_syllables_word': avg_syllables_word, 'unique_words': [unique_words], \n",
    "                                'ttr': [ttr], 'huerta_score': [huerta_score], 'szigriszt_score': [szigriszt_score],\n",
    "                                'mltd': [mltd], 'upper_case_ratio': [upper_case_ratio], 'entity_ratio': [entity_ratio],\n",
    "                                'quotes': quotes, 'quotes_ratio': [quotes_ratio], 'propn_ratio': [propn_ratio], \n",
    "                                'noun_ratio': [noun_ratio], 'pron_ratio': [pron_ratio], 'adp_ratio': [adp_ratio],\n",
    "                                'det_ratio': [det_ratio], 'punct_ratio': [punct_ratio], 'verb_ratio': [verb_ratio],\n",
    "                                'adv_ratio': [adv_ratio], 'sym_ratio': [sym_ratio]})\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Get_predictions function()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @st.cache(show_spinner = False)\n",
    "def get_predictions(pickle_file, df_features):\n",
    "    with st.spinner(\"Detectando...\"):\n",
    "        import pickle\n",
    "        model = pickle.load(open(pickle_file, 'rb'))\n",
    "\n",
    "        # prediction\n",
    "        prediction = (model.predict(df_features)[0])\n",
    "        prob_fake = (model.predict_proba(df_features)[0][0])*100\n",
    "        prob_real = (model.predict_proba(df_features)[0][1])*100\n",
    "    \n",
    "    return prediction, prob_fake, prob_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract news with url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿How to extract the headline and New's content directly with and url?\n",
    "\n",
    "This is possible with __[Newspaper3k: Article scraping & curation](https://github.com/codelucas/newspaper)__ library for python that extracts the headline and the new's content from any newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert url: https://edition.cnn.com/2020/09/05/politics/michael-cohen-book-trump-white-house/index.html\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "url = input('Insert url: ')\n",
    "\n",
    "article = Article(url, language = 'es')\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "df_features = get_news_features(article.title, article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractResult(subdomain='www', domain='eldiario', suffix='es')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tldextract\n",
    "\n",
    "url = 'https://www.eldiario.es/ballenablanca/biodiversidad/estudio-publicado-nature-advierte-impacto-biodiversidad-mineria-producir-renovables_1_6201934.html'\n",
    "\n",
    "ext = tldextract.extract(url)\n",
    "ext.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news(url):\n",
    "    with st.spinner(\"Detectando...\"):\n",
    "        from newspaper import Article\n",
    "        import tldextract\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        ext = tldextract.extract(url)\n",
    "        newspaper = ext.domain\n",
    "        \n",
    "        headline = article.title\n",
    "        text = article.text\n",
    "        image_url = article.top_image\n",
    "        \n",
    "        if article.author == []:\n",
    "            author = []\n",
    "        else:\n",
    "            author = article.author\n",
    "            \n",
    "        return headline, text, author, newspaper, image_url   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streamlit config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-811752dd90ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# display title and description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fake news predictor based in spanish language news\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0mMaster\u001b[0m \u001b[0mThesis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mData\u001b[0m \u001b[0mScience\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mKSchool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "# page configuration\n",
    "page_title = 'Fake News Detector'\n",
    "layout = 'wide'\n",
    "initial_sidebar_state = 'expanded'\n",
    "\n",
    "# display title and description\n",
    "st.title(\"Fake news predictor based in spanish language news\")\n",
    "\"\"\"\n",
    "Master Thesis in Data Science - KSchool\n",
    "\n",
    "This app is powered with a Machine Learning Algorithm to detect which news in spanish language are fake or legitimate\n",
    "Insert the new's headline and new's content text, and lets detect!\n",
    "\"\"\"\n",
    "\n",
    "url = st.text_input(\"Inserta el enlace de la noticia: \")\n",
    "\n",
    "headline_text = st.text_input(\"Pega el titular de la noticia: \")\n",
    "news_text = st.text_input(\"Pega el texto de la noticia: \")\n",
    "\n",
    "\n",
    "if prob_fake >= 65:\n",
    "    st.write('Esta noticia es falsa.\\nCon una probabilidad del %.0f%%' % prob_fake)\n",
    "\n",
    "elif (65 >= prob_fake >= 35):\n",
    "    st.write('Esta noticia es engañosa o una opinión. \\nTiene una probabilidad de %.0f%% de ser verdadera' % prob_real)\n",
    "    \n",
    "else:\n",
    "    st.write('Esta noticia es verdadera.\\nCon una probabilidad del %.0f%%' % prob_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'lexical_diversity' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f434a206050f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlexical_diversity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexical_diversity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'lexical_diversity' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import lexical_diversity\n",
    "\n",
    "print(lexical_diversity.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fake_news_detector_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fake_news_detector_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "#need to switch to es_core_news_md pacy model. es_core_news_lg too large for Heroku.\n",
    "import es_core_news_md\n",
    "from nltk import FreqDist\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "#### extract news with url ####\n",
    "\n",
    "@st.cache(show_spinner = False)\n",
    "def extract_news(url):\n",
    "    with st.spinner(\"Detectando...\"):\n",
    "        from newspaper import Article\n",
    "        import tldextract\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        ext = tldextract.extract(url)\n",
    "        newspaper = ext.domain\n",
    "        image_url = article.top_image\n",
    "        \n",
    "        headline = article.title\n",
    "        text = article.text\n",
    "        \n",
    "        if article.authors == []:\n",
    "            author = []\n",
    "        else:\n",
    "            author = article.authors\n",
    "            \n",
    "        return headline, text, author, newspaper, image_url\n",
    "\n",
    "@st.cache(show_spinner = False)\n",
    "def get_nsyllables(text):\n",
    "    from syltippy import syllabize\n",
    "    \n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(r'[^ \\nA-Za-z0-9ÁÉÍÓÚÑáéíóúñ/]+', '', text)\n",
    "    \n",
    "    n_syllables = len(syllabize(text)[0])\n",
    "    \n",
    "    return n_syllables\n",
    "\n",
    "\n",
    "#### text features ####\n",
    "\n",
    "@st.cache(show_spinner = False)\n",
    "def get_news_features(headline, text):\n",
    "    \n",
    "    nlp = es_core_news_md.load()\n",
    "\n",
    "    ## headline ##\n",
    "    headline = re.sub(r\"http\\S+\", \"\", headline)\n",
    "    headline = re.sub(r\"http\", \"\", headline)\n",
    "    headline = re.sub(r\"@\\S+\", \"\", headline)\n",
    "    headline = re.sub(\"\\n\", \" \", headline)\n",
    "    headline = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", headline)\n",
    "    headline = headline.replace(r\"*NUMBER*\", \"número\")\n",
    "    headline = headline.replace(r\"*PHONE*\", \"número\")\n",
    "    headline = headline.replace(r\"*EMAIL*\", \"email\")\n",
    "    headline = headline.replace(r\"*URL*\", \"url\")\n",
    "    headline_new = headline.lower()\n",
    "    doc_h = nlp(headline_new)\n",
    "\n",
    "    list_tokens_h = []\n",
    "    list_tags_h = []\n",
    "\n",
    "    for sentence_h in doc_h.sents:\n",
    "        for token in sentence_h:\n",
    "            list_tokens_h.append(token.text)\n",
    "\n",
    "    fdist_h = FreqDist(list_tokens_h)\n",
    "    syllables_h = get_nsyllables(headline)\n",
    "    words_h = len(list_tokens_h)\n",
    "\n",
    "    # headline complexity features\n",
    "    avg_word_size_h = round(sum(len(word) for word in list_tokens_h) / words_h, 2)\n",
    "    avg_syllables_word_h = round(syllables_h / words_h, 2)\n",
    "    unique_words_h = round((len(fdist_h.hapaxes()) / words_h) * 100, 2)\n",
    "    mltd_h = round(ld.mtld(list_tokens_h), 2)\n",
    "    ttr_h = round(ld.ttr(list_tokens_h) * 100, 2)\n",
    "\n",
    "    ## text content##     \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"http\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "\n",
    "    # to later calculate upper case letters ratio\n",
    "    alph = list(filter(str.isalpha, text))\n",
    "    text_lower = text.lower()\n",
    "    doc = nlp(text_lower)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    list_entities = []\n",
    "    sents = 0\n",
    "\n",
    "    for entity in doc.ents:\n",
    "        list_entities.append(entity.label_)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "\n",
    "    # Calculate entities, pos, tag, freq, syllables, words and quotes\n",
    "    entities = len(list_entities)\n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "    syllables = get_nsyllables(text)\n",
    "    words = len(list_tokens)\n",
    "    quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "\n",
    "    # complexity features\n",
    "    avg_word_sentence = round(words / sents, 2)\n",
    "    avg_word_size = round(sum(len(word) for word in list_tokens) / words, 2)\n",
    "    avg_syllables_word = round(syllables / words, 2)\n",
    "    unique_words = round((len(fdist.hapaxes()) / words) * 100, 2)\n",
    "    ttr = round(ld.ttr(list_tokens) * 100, 2)\n",
    "\n",
    "    # readability spanish test\n",
    "    huerta_score = round(206.84 - (60 * avg_syllables_word) - (1.02 * avg_word_sentence), 2)\n",
    "    szigriszt_score = round(206.835 - ((62.3 * syllables) / words) - (words / sents), 2)\n",
    "\n",
    "    # stylometric features\n",
    "    mltd = round(ld.mtld(list_tokens), 2)\n",
    "    upper_case_ratio = round(sum(map(str.isupper, alph)) / len(alph) * 100, 2)\n",
    "    entity_ratio = round((entities / words) * 100, 2)\n",
    "    quotes_ratio = round((quotes / words) * 100, 2)\n",
    "    propn_ratio = round((n_pos['PROPN'] / words) * 100 , 2)\n",
    "    noun_ratio = round((n_pos['NOUN'] / words) * 100, 2) \n",
    "    pron_ratio = round((n_pos['PRON'] / words) * 100, 2)\n",
    "    adp_ratio = round((n_pos['ADP'] / words) * 100, 2)\n",
    "    det_ratio = round((n_pos['DET'] / words) * 100, 2)\n",
    "    punct_ratio = round((n_pos['PUNCT'] / words) * 100, 2)\n",
    "    verb_ratio = round((n_pos['VERB'] / words) * 100, 2)\n",
    "    adv_ratio = round((n_pos['ADV'] / words) * 100, 2)\n",
    "    sym_ratio = round((n_tag['SYM'] / words) * 100, 2)\n",
    "\n",
    "    # create df_features\n",
    "    df_features = pd.DataFrame({'words_h': words_h, 'word_size_h': [avg_word_size_h],'avg_syllables_word_h': [avg_syllables_word_h],\n",
    "                                'unique_words_h': [unique_words_h], 'ttr_h': ttr_h, 'mltd_h': [mltd_h], 'sents': sents, 'words': words,\n",
    "                                'avg_words_sent': [avg_word_sentence], 'avg_word_size': [avg_word_size], \n",
    "                                'avg_syllables_word': avg_syllables_word, 'unique_words': [unique_words], \n",
    "                                'ttr': [ttr], 'huerta_score': [huerta_score], 'szigriszt_score': [szigriszt_score],\n",
    "                                'mltd': [mltd], 'upper_case_ratio': [upper_case_ratio], 'entity_ratio': [entity_ratio],\n",
    "                                'quotes': quotes, 'quotes_ratio': [quotes_ratio], 'propn_ratio': [propn_ratio], \n",
    "                                'noun_ratio': [noun_ratio], 'pron_ratio': [pron_ratio], 'adp_ratio': [adp_ratio],\n",
    "                                'det_ratio': [det_ratio], 'punct_ratio': [punct_ratio], 'verb_ratio': [verb_ratio],\n",
    "                                'adv_ratio': [adv_ratio], 'sym_ratio': [sym_ratio]})\n",
    "    \n",
    "    return df_features  \n",
    "\n",
    "\n",
    "#### predictions ####\n",
    "\n",
    "@st.cache(show_spinner = False)\n",
    "def get_predictions(pickle_file, df_features):\n",
    "    with st.spinner(\"Detectando...\"):\n",
    "        import pickle\n",
    "        model = pickle.load(open(pickle_file, 'rb'))\n",
    "        \n",
    "        numeric_features = ['words_h', 'word_size_h', 'avg_syllables_word_h', 'unique_words_h', 'ttr_h', 'mltd_h', 'sents',\n",
    "                            'words', 'avg_words_sent', 'avg_word_size', 'avg_syllables_word', 'unique_words', 'ttr', 'mltd', \n",
    "                            'huerta_score', 'szigriszt_score','upper_case_ratio', 'entity_ratio', 'quotes', 'quotes_ratio',\n",
    "                            'propn_ratio', 'noun_ratio', 'adp_ratio', 'det_ratio', 'punct_ratio', 'pron_ratio', 'verb_ratio', \n",
    "                            'adv_ratio', 'sym_ratio']\n",
    "\n",
    "        # prediction\n",
    "        prediction = (model.predict(df_features[numeric_features])[0])\n",
    "        prob_fake = (model.predict_proba(df_features[numeric_features])[0][0])*100\n",
    "        prob_real = (model.predict_proba(df_features[numeric_features])[0][1])*100\n",
    "    \n",
    "    return prob_fake, prob_real\n",
    "\n",
    "#### streamlit configuration ####\n",
    "\n",
    "# load pickle file \n",
    "pickle_file = './fake_news_predictorv3.pkl'\n",
    "\n",
    "# page configuration\n",
    "page_title = 'Fake News Detector'\n",
    "layout = 'wide'\n",
    "initial_sidebar_state = 'expanded'\n",
    "\n",
    "# display title and description\n",
    "st.title(\"Detector de\")\n",
    "st.image('./FAKE_NEWS_title.png', use_column_width = True, width = None, output_format = 'auto')\n",
    "\n",
    "# text input for headline and new's content\n",
    "url = st.text_input(\"Pega el enlace de la noticia.\")\n",
    "\n",
    "## run functions##\n",
    "if (url != \"\"):\n",
    "    \n",
    "    headline, text, author, newspaper, image_url = extract_news(url)\n",
    "    df_features = get_news_features(headline, text)\n",
    "    prob_fake, prob_real = get_predictions(pickle_file, df_features)\n",
    "\n",
    "\n",
    "    if prob_fake >= 65:\n",
    "        st.error('¡¡Esta noticia es **FALSA**!! :heavy_multiplication_x: \\nCon una probabilidad del %d%%.' % int(prob_fake))\n",
    "\n",
    "    elif (65 >= prob_fake >= 35):\n",
    "        st.warning('¡Esta noticia es **ENGAÑOSA**! :exclamation: \\nTiene una probabilidad de %d%% de ser verdadera.' % int(prob_real))\n",
    "\n",
    "    else:\n",
    "        st.success('Esta noticia es **VERDADERA** :white_check_mark: \\nCon una probabilidad del %d%%.' % int(prob_real))\n",
    "    \n",
    "    st.title(headline)\n",
    "    st.write(newspaper)\n",
    "    st.image(image_url, use_column_width = True, width = None)\n",
    "    if author == []:\n",
    "        st.write('Autor no encontrado')\n",
    "    else:\n",
    "        st.write('Autor: ', author[0])\n",
    "    st.write('Noticia: ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.43.219:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://176.87.16.101:8501\u001b[0m\n",
      "\u001b[0m\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run fake_news_detector_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Full script second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fake_news_detector_app2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fake_news_detector_app2.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk import word_tokenize    \n",
    "from nltk.stem import SnowballStemmer  \n",
    "from string import punctuation\n",
    "import re\n",
    "import spacy\n",
    "#need to switch to es_core_news_md pacy model. es_core_news_lg too large for Heroku.\n",
    "import es_core_news_md\n",
    "from nltk import FreqDist\n",
    "from lexical_diversity import lex_div as ld\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "\n",
    "#### extract news with url ####\n",
    "\n",
    "def extract_news(url):\n",
    "    with st.spinner(\"Detectando...\"):\n",
    "        from newspaper import Article\n",
    "        import tldextract\n",
    "        \n",
    "        try:\n",
    "            article.download()\n",
    "        except:\n",
    "            st.error('**El formato introducido no es correcto.** Introduce unicamente la URL de un periódico.')\n",
    "            st.write('**¡Prueba con otra!**')\n",
    "            st.image('./error_darth_vader.png', use_column_width = True, width = None)\n",
    "        \n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        ext = tldextract.extract(url)\n",
    "        newspaper = ext.domain\n",
    "        image_url = article.top_image\n",
    "        \n",
    "        headline = article.title\n",
    "        text = article.text\n",
    "        \n",
    "        if article.authors == []:\n",
    "            author = []\n",
    "        else:\n",
    "            author = article.authors\n",
    "            \n",
    "        return headline, text, author, newspaper, image_url\n",
    "\n",
    "@st.cache(show_spinner = False)\n",
    "def get_nsyllables(text):\n",
    "    from syltippy import syllabize\n",
    "    \n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(r'[^ \\nA-Za-z0-9ÁÉÍÓÚÑáéíóúñ/]+', '', text)\n",
    "    \n",
    "    n_syllables = len(syllabize(text)[0])\n",
    "    \n",
    "    return n_syllables\n",
    "\n",
    "\n",
    "#### text features ####\n",
    "\n",
    "@st.cache(show_spinner = False)\n",
    "def get_news_features(headline, text):\n",
    "    \n",
    "    nlp = es_core_news_md.load()\n",
    "\n",
    "    ## headline ##\n",
    "    headline = re.sub(r\"http\\S+\", \"\", headline)\n",
    "    headline = re.sub(r\"http\", \"\", headline)\n",
    "    headline = re.sub(r\"@\\S+\", \"\", headline)\n",
    "    headline = re.sub(\"\\n\", \" \", headline)\n",
    "    headline = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", headline)\n",
    "    headline = headline.replace(r\"*NUMBER*\", \"número\")\n",
    "    headline = headline.replace(r\"*PHONE*\", \"número\")\n",
    "    headline = headline.replace(r\"*EMAIL*\", \"email\")\n",
    "    headline = headline.replace(r\"*URL*\", \"url\")\n",
    "    headline_lower = headline.lower()\n",
    "    doc_h = nlp(headline_lower)\n",
    "\n",
    "    list_tokens_h = []\n",
    "    list_tags_h = []\n",
    "\n",
    "    for sentence_h in doc_h.sents:\n",
    "        for token in sentence_h:\n",
    "            list_tokens_h.append(token.text)\n",
    "\n",
    "    fdist_h = FreqDist(list_tokens_h)\n",
    "    syllables_h = get_nsyllables(headline)\n",
    "    words_h = len(list_tokens_h)\n",
    "\n",
    "    # headline complexity features\n",
    "    avg_word_size_h = round(sum(len(word) for word in list_tokens_h) / words_h, 2)\n",
    "    avg_syllables_word_h = round(syllables_h / words_h, 2)\n",
    "    unique_words_h = round((len(fdist_h.hapaxes()) / words_h) * 100, 2)\n",
    "    mltd_h = round(ld.mtld(list_tokens_h), 2)\n",
    "    ttr_h = round(ld.ttr(list_tokens_h) * 100, 2)\n",
    "\n",
    "    ## text content##     \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"http\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "\n",
    "    # to later calculate upper case letters ratio\n",
    "    alph = list(filter(str.isalpha, text))\n",
    "    text_lower = text.lower()\n",
    "    doc = nlp(text_lower)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    list_entities = []\n",
    "    sents = 0\n",
    "\n",
    "    for entity in doc.ents:\n",
    "        list_entities.append(entity.label_)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "\n",
    "    # Calculate entities, pos, tag, freq, syllables, words and quotes\n",
    "    entities = len(list_entities)\n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "    syllables = get_nsyllables(text)\n",
    "    words = len(list_tokens)\n",
    "    quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "\n",
    "    # complexity features\n",
    "    avg_word_sentence = round(words / sents, 2)\n",
    "    avg_word_size = round(sum(len(word) for word in list_tokens) / words, 2)\n",
    "    avg_syllables_word = round(syllables / words, 2)\n",
    "    unique_words = round((len(fdist.hapaxes()) / words) * 100, 2)\n",
    "    ttr = round(ld.ttr(list_tokens) * 100, 2)\n",
    "\n",
    "    # readability spanish test\n",
    "    huerta_score = round(206.84 - (60 * avg_syllables_word) - (1.02 * avg_word_sentence), 2)\n",
    "    szigriszt_score = round(206.835 - ((62.3 * syllables) / words) - (words / sents), 2)\n",
    "\n",
    "    # stylometric features\n",
    "    mltd = round(ld.mtld(list_tokens), 2)\n",
    "    upper_case_ratio = round(sum(map(str.isupper, alph)) / len(alph) * 100, 2)\n",
    "    entity_ratio = round((entities / words) * 100, 2)\n",
    "    quotes_ratio = round((quotes / words) * 100, 2)\n",
    "    propn_ratio = round((n_pos['PROPN'] / words) * 100 , 2)\n",
    "    noun_ratio = round((n_pos['NOUN'] / words) * 100, 2) \n",
    "    pron_ratio = round((n_pos['PRON'] / words) * 100, 2)\n",
    "    adp_ratio = round((n_pos['ADP'] / words) * 100, 2)\n",
    "    det_ratio = round((n_pos['DET'] / words) * 100, 2)\n",
    "    punct_ratio = round((n_pos['PUNCT'] / words) * 100, 2)\n",
    "    verb_ratio = round((n_pos['VERB'] / words) * 100, 2)\n",
    "    adv_ratio = round((n_pos['ADV'] / words) * 100, 2)\n",
    "    sym_ratio = round((n_tag['SYM'] / words) * 100, 2)\n",
    "\n",
    "    # create df_features\n",
    "    df_features = pd.DataFrame({'text': text_lower, 'headline':headline_lower, 'words_h': words_h, 'word_size_h': [avg_word_size_h],\n",
    "                                'avg_syllables_word_h': [avg_syllables_word_h],'unique_words_h': [unique_words_h], \n",
    "                                'ttr_h': ttr_h, 'mltd_h': [mltd_h], 'sents': sents, 'words': words,\n",
    "                                'avg_words_sent': [avg_word_sentence], 'avg_word_size': [avg_word_size], \n",
    "                                'avg_syllables_word': avg_syllables_word, 'unique_words': [unique_words], \n",
    "                                'ttr': [ttr], 'huerta_score': [huerta_score], 'szigriszt_score': [szigriszt_score],\n",
    "                                'mltd': [mltd], 'upper_case_ratio': [upper_case_ratio], 'entity_ratio': [entity_ratio],\n",
    "                                'quotes': quotes, 'quotes_ratio': [quotes_ratio], 'propn_ratio': [propn_ratio], \n",
    "                                'noun_ratio': [noun_ratio], 'pron_ratio': [pron_ratio], 'adp_ratio': [adp_ratio],\n",
    "                                'det_ratio': [det_ratio], 'punct_ratio': [punct_ratio], 'verb_ratio': [verb_ratio],\n",
    "                                'adv_ratio': [adv_ratio], 'sym_ratio': [sym_ratio]})\n",
    "    \n",
    "    return df_features  \n",
    "\n",
    "#### TF-IDF Transformation ####\n",
    "\n",
    "#Stopword list to use\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "#Spanish stemmer:\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "@st.cache(show_spinner = False)\n",
    "def stem_tokens(tokens, stemmer):  \n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "#Punctuation to remove\n",
    "non_words = list(punctuation)\n",
    "\n",
    "#Adding spanish punctuation\n",
    "non_words.extend(['¿', '¡', '‘', '’', '“', '”'])  \n",
    "non_words.extend(map(str, range(10)))\n",
    "\n",
    "@st.cache(show_spinner = False)\n",
    "def tokenize(text):  \n",
    "    #Remove punctuation\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    #Tokenize\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    #Stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems\n",
    "\n",
    "\n",
    "#### predictions ####\n",
    "\n",
    "@st.cache(show_spinner = False)\n",
    "def get_predictions(pickle_file, df_features):\n",
    "    with st.spinner(\"Detectando...\"):\n",
    "        import pickle\n",
    "        model = pickle.load(open(pickle_file, 'rb'))\n",
    "        tfidf_vectorizer = pickle.load(open('../predictors/tfidf_vectorizer.pkl', 'rb'))\n",
    "        \n",
    "        \n",
    "        numeric_features = ['words_h', 'word_size_h', 'avg_syllables_word_h', 'unique_words_h', 'ttr_h', 'mltd_h', 'sents',\n",
    "                            'words', 'avg_words_sent', 'avg_word_size', 'avg_syllables_word', 'unique_words', 'ttr', 'mltd', \n",
    "                            'huerta_score', 'szigriszt_score','upper_case_ratio', 'entity_ratio', 'quotes', 'quotes_ratio',\n",
    "                            'propn_ratio', 'noun_ratio', 'adp_ratio', 'det_ratio', 'punct_ratio', 'pron_ratio', 'verb_ratio', \n",
    "                            'adv_ratio', 'sym_ratio']\n",
    "\n",
    "        \n",
    "        #TF-IDF vectorization\n",
    "        text_predict_vectorized = tfidf_vectorizer.transform(df_features['text'])\n",
    "        X_predict = hstack([csr_matrix(df_features[numeric_features].values), text_predict_vectorized[0:]])\n",
    "\n",
    "        # prediction\n",
    "        prediction = (model.predict(X_predict)[0])\n",
    "        prob_fake = (model.predict_proba(X_predict)[0][0])*100\n",
    "        prob_real = (model.predict_proba(X_predict)[0][1])*100\n",
    "    \n",
    "    return prob_fake, prob_real\n",
    "\n",
    "#### streamlit configuration ####\n",
    "\n",
    "\n",
    "# load pickle file \n",
    "pickle_file = './fake_news_predictorv4.pkl'\n",
    "\n",
    "# page configuration\n",
    "page_title = 'Fake News Detector'\n",
    "layout = 'wide'\n",
    "initial_sidebar_state = 'expanded'\n",
    "\n",
    "# display title and description\n",
    "st.title(\"Detector de\")\n",
    "st.image('./FAKE_NEWS_title.png', use_column_width = True, width = None, output_format = 'auto')\n",
    "\n",
    "# text input for headline and new's content\n",
    "try:\n",
    "    url = st.text_input(\"Pega el enlace de una noticia\")\n",
    "except ValueError:\n",
    "    st.error('Por favor introduce un enlace únicamente')\n",
    "    st.image('./error_darth_vader.png', use_column_width = True, width = None)\n",
    "\n",
    "\n",
    "## run functions##\n",
    "if (url != \"\"):\n",
    "    \n",
    "    headline, text, author, newspaper, image_url = extract_news(url)       \n",
    "    \n",
    "    if detect(text) == 'es' and len(detect_langs(text)) == 1:\n",
    "        df_features = get_news_features(headline, text)\n",
    "        \n",
    "        prob_fake, prob_real = get_predictions(pickle_file, df_features)\n",
    "\n",
    "        if prob_fake >= 65:\n",
    "            st.error('¡¡Esta noticia es **FALSA**!! :heavy_multiplication_x: \\nCon una probabilidad del %d%%.' % int(prob_fake))\n",
    "\n",
    "        elif (65 >= prob_fake >= 35):\n",
    "            st.warning('¡Esta noticia es **ENGAÑOSA**! :exclamation: \\nTiene una probabilidad de %d%% de ser verdadera.' % int(prob_real))\n",
    "\n",
    "        else:\n",
    "            st.success('Esta noticia es **VERDADERA** :white_check_mark: \\nCon una probabilidad del %d%%.' % int(prob_real))\n",
    "\n",
    "        st.title(headline)\n",
    "        st.write(newspaper)\n",
    "        st.image(image_url, use_column_width = True, width = None)\n",
    "        if author == []:\n",
    "            st.write('Autor no encontrado')\n",
    "        else:\n",
    "            st.write('Autor: ', author[0])\n",
    "        st.write('Noticia: ', text)\n",
    "        \n",
    "    else:\n",
    "        st.error('¡Esta noticia **no está en español** o tiene mucho contenido en otro idioma!')\n",
    "        st.write('**¡Prueba con otra!**')\n",
    "        st.image('./error_darth_vader.png', use_column_width = True, width = None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.43.219:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://31.4.241.222:8501\u001b[0m\n",
      "\u001b[0m\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "image/jpeg\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run fake_news_detector_app2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
