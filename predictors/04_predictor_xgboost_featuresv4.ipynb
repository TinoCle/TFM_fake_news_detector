{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor with complexity and stylometric/lexical features\n",
    "\n",
    "1. Load model\n",
    "2. Extract features\n",
    "3. Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, eval_metric='auc',\n",
       "              gamma=0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=1, nthread=1,\n",
       "              num_parallel_tree=1, random_state=43, reg_alpha=0, reg_lambda=1,\n",
       "              scale_pos_weight=0.9775967413441956, seed=43, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model = pickle.load(open('../predictors/fake_news_predictorv2.pkl', 'rb'))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract features\n",
    "\n",
    "We are going tu use the last feature extractor located on /feature_extraction, in this case the version v4. This extractor does:\n",
    "\n",
    "- Clean the text from the headline and the news content\n",
    "- Extract complexity features from headline\n",
    "- Extract complexity features from news text content\n",
    "- Extract stylometrix features from news text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import FreqDist\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "def get_news_features(headline, text):\n",
    "    nlp = spacy.load('es_core_news_lg')\n",
    "    \n",
    "    ## headline ##\n",
    "    headline = headline.replace(r\"http\\S+\", \"\")\n",
    "    headline = headline.replace(r\"http\", \"\")\n",
    "    headline = headline.replace(r\"@\\S+\", \"\")\n",
    "    headline = headline.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "    headline = headline.lower()\n",
    "    doc_h = nlp(headline)\n",
    "\n",
    "    list_tokens_h = []\n",
    "    list_tags_h = []\n",
    "    n_sents_h = 0\n",
    "\n",
    "    for sentence_h in doc_h.sents:\n",
    "        n_sents_h += 1\n",
    "        for token in sentence_h:\n",
    "            list_tokens_h.append(token.text)\n",
    "\n",
    "    fdist_h = FreqDist(list_tokens_h)\n",
    "\n",
    "    # headline complexity features\n",
    "    n_words_h = len(list_tokens_h)\n",
    "    word_size_h = sum(len(word) for word in list_tokens_h) / n_words_h\n",
    "    unique_words_h = (len(fdist_h.hapaxes()) / n_words_h) * 100\n",
    "    ttr_h = ld.ttr(list_tokens_h) * 100\n",
    "    mltd_h = ld.mtld(list_tokens_h)\n",
    "\n",
    "\n",
    "\n",
    "    ## text content ##   \n",
    "    text = text.replace(r\"http\\S+\", \"\")\n",
    "    text = text.replace(r\"http\", \"\")\n",
    "    text = text.replace(r\"@\\S+\", \"\")\n",
    "    text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    n_sents = 0\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        n_sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "\n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "\n",
    "    # complexity features\n",
    "    n_words = len(list_tokens)\n",
    "    avg_word_sentences = (float(n_words) / n_sents)\n",
    "    word_size = sum(len(word) for word in list_tokens) / n_words\n",
    "    unique_words = (len(fdist.hapaxes()) / n_words) * 100\n",
    "    ttr = ld.ttr(list_tokens) * 100\n",
    "    mltd = ld.mtld(list_tokens)\n",
    "\n",
    "    # lexical features\n",
    "    n_quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "    quotes_ratio = (n_quotes / n_words) * 100\n",
    "    propn_ratio = (n_pos['PROPN'] / n_words) * 100 \n",
    "    noun_ratio = (n_pos['NOUN'] / n_words) * 100 \n",
    "    adp_ratio = (n_pos['ADP'] / n_words) * 100\n",
    "    det_ratio = (n_pos['DET'] / n_words) * 100\n",
    "    punct_ratio = (n_pos['PUNCT'] / n_words) * 100 \n",
    "    pron_ratio = (n_pos['PRON'] / n_words) * 100\n",
    "    verb_ratio = (n_pos['VERB'] / n_words) * 100\n",
    "    adv_ratio = (n_pos['ADV'] / n_words) * 100\n",
    "    sym_ratio = (n_tag['SYM'] / n_words) * 100\n",
    "\n",
    "    # create df\n",
    "\n",
    "    df_features = pd.DataFrame({'n_sents': [n_sents], 'n_words': [n_words], 'avg_words_sents': [avg_word_sentences], \n",
    "                'word_size': [word_size], 'unique_words': [unique_words], 'ttr': [ttr], 'mltd': [mltd], 'n_words_h': [n_words_h],\n",
    "                'word_size_h': [word_size_h], 'unique_words_h': [unique_words_h], 'mltd_h': [mltd_h], 'n_quotes': [n_quotes],\n",
    "                'quotes_ratio': [quotes_ratio], 'propn_ratio': [propn_ratio], 'noun_ratio': [noun_ratio], 'adp_ratio': [adp_ratio],\n",
    "                'det_ratio': [det_ratio], 'punct_ratio': [punct_ratio], 'pron_ratio': [pron_ratio], 'verb_ratio': [verb_ratio],\n",
    "                'adv_ratio': [adv_ratio], 'sym_ratio': [sym_ratio]})\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3c79dfe73079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is a fake new \\nWith a probability: %.0f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_features' is not defined"
     ]
    }
   ],
   "source": [
    "X_predict = df_features\n",
    "\n",
    "if (model.predict(X_predict)[0]) == 0:\n",
    "    print('This is a fake new \\nWith a probability: %.0f' % ((model.predict_proba(X_predict)[0][0])*100), '%')\n",
    "else:\n",
    "    print('This is a legitimate new\\nWith a probability: %.0f' % ((model.predict_proba(X_predict)[0][1])*100), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('../predictors/fake_news_predictorv2.pkl', 'rb'))\n",
    "nlp = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import FreqDist\n",
    "from lexical_diversity import lex_div as ld\n",
    "import pickle\n",
    "\n",
    "from newspaper import Article\n",
    "\n",
    "from newspaper import Article\n",
    "\n",
    "headline = input('Insert headline: ')\n",
    "text= input('Insert text: ')\n",
    "\n",
    "df_features = get_news_features(headline, text)\n",
    "\n",
    "########## PREDICTIONS ##########\n",
    "X_predict = df_features\n",
    "\n",
    "if (model.predict(X_predict)[0]) == 0:\n",
    "    print('This is a fake new! \\nWith a probability: %.0f' % ((model.predict_proba(X_predict)[0][0])*100), '%')\n",
    "else:\n",
    "    print('This is a legitimate! new\\nWith a probability: %.0f' % ((model.predict_proba(X_predict)[0][1])*100), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_sents</th>\n",
       "      <th>n_words</th>\n",
       "      <th>avg_words_sents</th>\n",
       "      <th>word_size</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>ttr</th>\n",
       "      <th>mltd</th>\n",
       "      <th>n_words_h</th>\n",
       "      <th>word_size_h</th>\n",
       "      <th>unique_words_h</th>\n",
       "      <th>...</th>\n",
       "      <th>quotes_ratio</th>\n",
       "      <th>propn_ratio</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>adp_ratio</th>\n",
       "      <th>det_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>pron_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adv_ratio</th>\n",
       "      <th>sym_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>994</td>\n",
       "      <td>38.230769</td>\n",
       "      <td>4.77666</td>\n",
       "      <td>35.311871</td>\n",
       "      <td>44.969819</td>\n",
       "      <td>86.942711</td>\n",
       "      <td>19</td>\n",
       "      <td>5.526316</td>\n",
       "      <td>78.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201207</td>\n",
       "      <td>8.450704</td>\n",
       "      <td>19.71831</td>\n",
       "      <td>17.002012</td>\n",
       "      <td>12.877264</td>\n",
       "      <td>8.249497</td>\n",
       "      <td>1.710262</td>\n",
       "      <td>8.04829</td>\n",
       "      <td>2.313883</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_sents  n_words  avg_words_sents  word_size  unique_words        ttr  \\\n",
       "0       26      994        38.230769    4.77666     35.311871  44.969819   \n",
       "\n",
       "        mltd  n_words_h  word_size_h  unique_words_h  ...  quotes_ratio  \\\n",
       "0  86.942711         19     5.526316       78.947368  ...      0.201207   \n",
       "\n",
       "   propn_ratio  noun_ratio  adp_ratio  det_ratio  punct_ratio  pron_ratio  \\\n",
       "0     8.450704    19.71831  17.002012  12.877264     8.249497    1.710262   \n",
       "\n",
       "   verb_ratio  adv_ratio  sym_ratio  \n",
       "0     8.04829   2.313883        0.0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
