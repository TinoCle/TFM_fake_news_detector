{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and feature extraction v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text, extract stylometric features and create a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/corpus_spanish.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>641</td>\n",
       "      <td>True</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Caras</td>\n",
       "      <td>Sofía Castro y Alejandro Peña Pretelini: una i...</td>\n",
       "      <td>Sofía Castro y Alejandro Peña Pretelini: una i...</td>\n",
       "      <td>https://www.caras.com.mx/sofia-castro-alejandr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>Education</td>\n",
       "      <td>Heraldo</td>\n",
       "      <td>Un paso más cerca de hacer los exámenes 'online'</td>\n",
       "      <td>Un paso más cerca de hacer los exámenes 'onlin...</td>\n",
       "      <td>https://www.heraldo.es/noticias/suplementos/he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141</td>\n",
       "      <td>True</td>\n",
       "      <td>Science</td>\n",
       "      <td>HUFFPOST</td>\n",
       "      <td>Esto es lo que los científicos realmente piens...</td>\n",
       "      <td>Esto es lo que los científicos realmente piens...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/scientist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>394</td>\n",
       "      <td>True</td>\n",
       "      <td>Politics</td>\n",
       "      <td>El financiero</td>\n",
       "      <td>Inicia impresión de boletas para elección pres...</td>\n",
       "      <td>Inicia impresión de boletas para elección pres...</td>\n",
       "      <td>http://www.elfinanciero.com.mx/elecciones-2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139</td>\n",
       "      <td>True</td>\n",
       "      <td>Sport</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>A *NUMBER* día del Mundial</td>\n",
       "      <td>A *NUMBER* día del Mundial\\nFIFA.com sigue la ...</td>\n",
       "      <td>https://es.fifa.com/worldcup/news/a-1-dia-del-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id Category          Topic         Source  \\\n",
       "0  641     True  Entertainment          Caras   \n",
       "1    6     True      Education        Heraldo   \n",
       "2  141     True        Science       HUFFPOST   \n",
       "3  394     True       Politics  El financiero   \n",
       "4  139     True          Sport           FIFA   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Sofía Castro y Alejandro Peña Pretelini: una i...   \n",
       "1   Un paso más cerca de hacer los exámenes 'online'   \n",
       "2  Esto es lo que los científicos realmente piens...   \n",
       "3  Inicia impresión de boletas para elección pres...   \n",
       "4                         A *NUMBER* día del Mundial   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Sofía Castro y Alejandro Peña Pretelini: una i...   \n",
       "1  Un paso más cerca de hacer los exámenes 'onlin...   \n",
       "2  Esto es lo que los científicos realmente piens...   \n",
       "3  Inicia impresión de boletas para elección pres...   \n",
       "4  A *NUMBER* día del Mundial\\nFIFA.com sigue la ...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.caras.com.mx/sofia-castro-alejandr...  \n",
       "1  https://www.heraldo.es/noticias/suplementos/he...  \n",
       "2  https://www.huffingtonpost.com/entry/scientist...  \n",
       "3  http://www.elfinanciero.com.mx/elecciones-2018...  \n",
       "4  https://es.fifa.com/worldcup/news/a-1-dia-del-...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(971, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id           int64\n",
       "Category    object\n",
       "Topic       object\n",
       "Source      object\n",
       "Headline    object\n",
       "Text        object\n",
       "Link        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 971 entries, 0 to 970\n",
      "Data columns (total 7 columns):\n",
      "Id          971 non-null int64\n",
      "Category    971 non-null object\n",
      "Topic       971 non-null object\n",
      "Source      971 non-null object\n",
      "Headline    971 non-null object\n",
      "Text        971 non-null object\n",
      "Link        971 non-null object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 53.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are using `spacy`: The NLP *Ruby on Rails* \n",
    "\n",
    "[spacy](http://www.spacy.io/) is a library of natural language processing, robust, fast, easy to install and to use. It can be used with other NLP and Deep Learning Libraries.\n",
    "\n",
    "With its pre-trained models in spanish language, we can operate the typical NLP jobs: Sentences segmentation, tokenization, POS tag, etc...\n",
    "\n",
    "We are going to use the `es_core_news_lg` pre-trained model to make pos tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the pre trained model in spanish language\n",
    "\n",
    "nlp = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un paso más cerca de hacer los exámenes 'online'\n",
      "Cerca de *NUMBER* universitarios de seis universidades europeas participan este cuatrimestre en la última fase de pruebas de un programa informático que permitirá certificar la identidad y autoría de los estudiantes cuando realizan actividades 'online' como exámenes, trabajos u otras pruebas.\n",
      "El proyecto se encuentra en la tercera y última fase de pruebas piloto, en la que se pone a prueba la versión \"final\" del 'software', que incluye herramientas de reconocimiento facial, de voz y otras que capturan patrones de escritura, detectan plagio y analizan el lenguaje y el estilo de redacción, según sus impulsores.\n",
      "En dicha fase hay una primera prueba en la que están participando *NUMBER* estudiantes y en la que la Universitat Oberta de Catalunya (UOC) participa con *NUMBER* estudiantes, *NUMBER* profesores y *NUMBER* docentes; y una segunda prueba en la que participarán entre *NUMBER* y *NUMBER* estudiantes. \"El balance de las dos pruebas [anteriores] es muy positivo. En la primera examinamos todos los instrumentos de forma aislada, y en la segunda ya se probó la primera versión casi completa del sistema\", explican los responsables del proyecto.\n",
      "La iniciativa, coordinada por la UOC, recibe el nombre de TeSLA y ha sido impulsada por un consorcio formado por *NUMBER* organizaciones, entre universidades, grupos de investigación, empresas y agencias de calidad, bajo el paraguas de la Comisión Europea, con un presupuesto de *NUMBER* millones de euros.\n",
      "Además de la universidad española están implicadas en el desarrollo del programa y de las pruebas la Open University (OU, Reino Unido), la Open Universiteit Nederlands (OUNL, Países Bajos), la Anadolu University (AU, Turquía), la University of Jyväskylä (JYU, Finlandia), la University St. Kliment Ohridski de Sofía (SU, Bulgaria) y la Technical University of Sofia (TUS, Bulgaria).\n",
      "Junto con estas, el consorcio de TeSLA lo completan la University of Namur (UNamur), de Bélgica; el Imperial College of London, de Reino Unido, el Instituto Mines-Télécóm (IMT), de Francia; los centros de investigación Instituto Nacional de Astrofísica Óptica y Electrónica(INAOE), de México; el IDIAP Research Institute Foundation suizo, y las agencias de calidad ENQA, AQU Cataluña y EQANIE; además de tres empresas tecnológicas: LPLUS GmbH (Alemania), Protos Sistemas de Información, S. L. (España) y WFSW SA (Watchful), de Portugal.\n"
     ]
    }
   ],
   "source": [
    "text = df['Text'].iloc[1]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text for spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    \n",
    "    text = text.replace(r\"http\\S+\", \"\")\n",
    "    text = text.replace(r\"http\", \"\")\n",
    "    text = text.replace(r\"@\\S+\", \"\")\n",
    "    text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "    text = text.lower()\n",
    "    \n",
    "    # text processing\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = text_clean(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can easily iterate over the sentences list and scroll through the tokens to access their morpho-syntactic information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'DET__Definite=Ind|Gender=Masc|Number=Sing|PronType=Art': 4,\n",
       "         'NOUN__Gender=Masc|Number=Sing': 21,\n",
       "         'ADV__Degree=Cmp': 1,\n",
       "         'ADV': 5,\n",
       "         'ADP__AdpType=Prep': 56,\n",
       "         'VERB__VerbForm=Inf': 2,\n",
       "         'DET__Definite=Def|Gender=Masc|Number=Plur|PronType=Art': 5,\n",
       "         'NOUN__Gender=Masc|Number=Plur': 13,\n",
       "         'SYM': 9,\n",
       "         'PROPN': 81,\n",
       "         '_SP': 6,\n",
       "         'PUNCT': 2,\n",
       "         'NOUN': 11,\n",
       "         'NUM__NumForm=Digit|NumType=Card': 8,\n",
       "         'ADJ__Gender=Masc|Number=Plur': 2,\n",
       "         'NUM__Number=Plur|NumType=Card': 3,\n",
       "         'NOUN__Gender=Fem|Number=Plur': 14,\n",
       "         'ADJ__Gender=Fem|Number=Plur': 2,\n",
       "         'VERB__Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin': 7,\n",
       "         'DET__Gender=Masc|Number=Sing|PronType=Dem': 1,\n",
       "         'DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art': 23,\n",
       "         'ADJ__Gender=Fem|Number=Sing|NumType=Ord': 8,\n",
       "         'NOUN__Gender=Fem|Number=Sing': 19,\n",
       "         'ADJ__Gender=Masc|Number=Sing': 3,\n",
       "         'PRON__PronType=Int,Rel': 6,\n",
       "         'VERB__Mood=Ind|Number=Sing|Person=3|Tense=Fut|VerbForm=Fin': 1,\n",
       "         'CCONJ': 19,\n",
       "         'NOUN__Number=Plur': 5,\n",
       "         'SCONJ': 3,\n",
       "         'PUNCT__PunctType=Comm': 37,\n",
       "         'DET__Gender=Fem|Number=Plur|PronType=Ind': 1,\n",
       "         'PUNCT__PunctType=Peri': 9,\n",
       "         'DET__Definite=Def|Gender=Masc|Number=Sing|PronType=Art': 11,\n",
       "         'PRON__Case=Acc,Dat|Person=3|PrepCase=Npr|PronType=Prs|Reflex=Yes': 3,\n",
       "         'VERB__Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin': 5,\n",
       "         'NOUN__Number=Sing': 1,\n",
       "         'PUNCT__PunctType=Quot': 4,\n",
       "         'ADJ__Number=Sing': 3,\n",
       "         'ADP__AdpType=Preppron': 4,\n",
       "         'PRON__Gender=Fem|Number=Plur|PronType=Ind': 1,\n",
       "         'DET__Number=Plur|Person=3|Poss=Yes|PronType=Prs': 1,\n",
       "         'ADJ__Gender=Fem|Number=Sing|VerbForm=Part': 3,\n",
       "         'AUX__Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin': 3,\n",
       "         'DET__Definite=Ind|Gender=Fem|Number=Sing|PronType=Art': 2,\n",
       "         'AUX__Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin': 2,\n",
       "         'VERB__VerbForm=Ger': 1,\n",
       "         'PUNCT__PunctSide=Ini|PunctType=Brck': 12,\n",
       "         'PUNCT__PunctSide=Fin|PunctType=Brck': 13,\n",
       "         'ADJ__Number=Plur': 2,\n",
       "         'PUNCT__PunctType=Semi': 5,\n",
       "         'VERB__Mood=Ind|Number=Plur|Person=3|Tense=Fut|VerbForm=Fin': 1,\n",
       "         'DET__Definite=Def|Gender=Fem|Number=Plur|PronType=Art': 3,\n",
       "         'VERB__Mood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin': 1,\n",
       "         'DET__Gender=Masc|Number=Plur|PronType=Tot': 1,\n",
       "         'VERB__Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin': 1,\n",
       "         'ADJ__Gender=Fem|Number=Sing': 3,\n",
       "         'AUX__Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part': 1,\n",
       "         'VERB__Gender=Fem|Number=Sing|Tense=Past|VerbForm=Part': 1,\n",
       "         'ADJ__Gender=Masc|Number=Sing|VerbForm=Part': 1,\n",
       "         'NOUN__NumForm=Digit': 1,\n",
       "         'ADV__AdpType=Prep': 2,\n",
       "         'ADJ__Gender=Fem|Number=Plur|VerbForm=Part': 1,\n",
       "         'DET__Number=Sing|Person=3|Poss=Yes|PronType=Prs': 1,\n",
       "         'DET__Gender=Fem|Number=Plur|Number[psor]=Plur|Person=1|Poss=Yes|PronType=Prs': 1,\n",
       "         'ADJ__AdpType=Prep': 1,\n",
       "         'PRON__Gender=Fem|Number=Plur|PronType=Dem': 1,\n",
       "         'PRON__Case=Acc|Gender=Masc|Number=Sing|Person=3|PrepCase=Npr|PronType=Prs': 1,\n",
       "         'PUNCT__PunctType=Colo': 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "list_tokens = []\n",
    "list_pos = []\n",
    "list_tag = []\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    for token in sentence:\n",
    "        list_tokens.append(token.text)\n",
    "        list_pos.append(token.pos_)\n",
    "        list_tag.append(token.tag_)\n",
    "\n",
    "nltk.Counter(list_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486 9 54.0 4.255144032921811 32.71604938271605 44.23868312757202 41.28313642062075 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk import word_tokenize, sent_tokenize  \n",
    "from string import punctuation\n",
    "\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "text = text.replace(r\"http\\S+\", \"\")\n",
    "text = text.replace(r\"http\", \"\")\n",
    "text = text.replace(r\"@\\S+\", \"\")\n",
    "text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "text = text.lower()\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "list_tokens = []\n",
    "list_tag = []\n",
    "list_pos = []\n",
    "n_sents = 0\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    n_sents += 1\n",
    "    for token in sentence:\n",
    "        list_tokens.append(token.text)\n",
    "        list_tags.append(token.pos_)\n",
    "        \n",
    "n_tag = nltk.Counter(list_tag)\n",
    "n_pos = nltk.Counter(list_pos)\n",
    "fdist = FreqDist(list_tokens)\n",
    "        \n",
    "# complexity features\n",
    "n_words = len(list_tokens)\n",
    "avg_word_sentences = (float(n_words) / n_sents)\n",
    "word_size = sum(len(word) for word in list_tokens) / n_words\n",
    "unique_words = (len(fdist.hapaxes()) / n_words) * 100\n",
    "ttr = ld.ttr(list_tokens) * 100\n",
    "mltd = ld.mtld(list_tokens)\n",
    "\n",
    "# lexical features\n",
    "n_quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "quotes_ratio = (n_quotes / n_words) * 100\n",
    "propn_ratio = (n_pos['PROPN'] / n_words) * 100 \n",
    "noun_ratio = (n_pos['NOUN'] / n_words) * 100 \n",
    "adp_ratio = (n_pos['ADP'] / n_words) * 100\n",
    "det_ratio = (n_pos['DET'] / n_words) * 100\n",
    "punct_ratio = (n_pos['PUNCT'] / n_words) * 100 \n",
    "pron_ratio = (n_pos['PRON'] / n_words) * 100\n",
    "verb_ratio = (n_pos['VERB'] / n_words) * 100\n",
    "adv_ratio = (n_pos['ADV'] / n_words) * 100\n",
    "sym_ratio = (n_tag['SYM'] / n_words) * 100\n",
    "\n",
    "print(n_words, n_sents, avg_word_sentences, word_size, unique_words, ttr, mltd, n_quotes, quotes_ratio, propn_ratio, noun_ratio, adp_ratio, det_ratio, punct_ratio, \n",
    "      pron_ratio, verb_ratio, adv_ratio, sym_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply it to the full corpus with iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_headline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_headline' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import FreqDist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lexical_diversity import lex_div as ld\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "df = pd.read_csv('../data/corpus_spanish.csv')\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df['Label'] = labelencoder.fit_transform(df['Category'])\n",
    "\n",
    "# empty lists and df\n",
    "df_features = pd.DataFrame()\n",
    "list_text = []\n",
    "list_nsentences = []\n",
    "list_nwords = []\n",
    "list_words_sent = []\n",
    "list_word_size = []\n",
    "list_unique_words = []\n",
    "list_ttr = []\n",
    "list_mltd = []\n",
    "list_nquotes = []\n",
    "list_quotes_ratio = []\n",
    "list_propn_ratio = [] \n",
    "list_noun_ratio = []\n",
    "list_adp_ratio = []\n",
    "list_det_ratio = []\n",
    "list_punct_ratio = []\n",
    "list_pron_ratio = []\n",
    "list_verb_ratio = []\n",
    "list_adv_ratio = []\n",
    "list_sym_ratio = []\n",
    "\n",
    "# df iteration\n",
    "for n, row in df.iterrows():\n",
    "    ## text content##   \n",
    "    text = df['Text'].iloc[n]  \n",
    "    text = text.replace(r\"http\\S+\", \"\")\n",
    "    text = text.replace(r\"http\", \"\")\n",
    "    text = text.replace(r\"@\\S+\", \"\")\n",
    "    text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    n_sents = 0\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        n_sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "            \n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "\n",
    "    # complexity features\n",
    "    n_words = len(list_tokens)\n",
    "    avg_word_sentences = (float(n_words) / n_sents)\n",
    "    word_size = sum(len(word) for word in list_tokens) / n_words\n",
    "    unique_words = (len(fdist.hapaxes()) / n_words) * 100\n",
    "    ttr = ld.ttr(list_tokens) * 100\n",
    "    mltd = ld.mtld(list_tokens)\n",
    "\n",
    "    # lexical features\n",
    "    n_quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "    quotes_ratio = (n_quotes / n_words) * 100\n",
    "    propn_ratio = (n_pos['PROPN'] / n_words) * 100 \n",
    "    noun_ratio = (n_pos['NOUN'] / n_words) * 100 \n",
    "    adp_ratio = (n_pos['ADP'] / n_words) * 100\n",
    "    det_ratio = (n_pos['DET'] / n_words) * 100\n",
    "    punct_ratio = (n_pos['PUNCT'] / n_words) * 100 \n",
    "    pron_ratio = (n_pos['PRON'] / n_words) * 100\n",
    "    verb_ratio = (n_pos['VERB'] / n_words) * 100\n",
    "    adv_ratio = (n_pos['ADV'] / n_words) * 100\n",
    "    sym_ratio = (n_tag['SYM'] / n_words) * 100\n",
    "    \n",
    "    # appending on lists\n",
    "    list_text.append(text)\n",
    "    list_nsentences.append(n_sents)\n",
    "    list_nwords.append(n_words)\n",
    "    list_words_sent.append(avg_word_sentences)\n",
    "    list_word_size.append(word_size)\n",
    "    list_unique_words.append(unique_words)\n",
    "    list_ttr.append(ttr)\n",
    "    list_mltd.append(mltd)\n",
    "    list_nquotes.append(n_quotes)\n",
    "    list_quotes_ratio.append(quotes_ratio)\n",
    "    list_propn_ratio.append(propn_ratio)\n",
    "    list_noun_ratio.append(noun_ratio)\n",
    "    list_adp_ratio.append(adp_ratio)\n",
    "    list_det_ratio.append(det_ratio)\n",
    "    list_punct_ratio.append(punct_ratio)\n",
    "    list_pron_ratio.append(pron_ratio)\n",
    "    list_verb_ratio.append(verb_ratio)\n",
    "    list_adv_ratio.append(adv_ratio)\n",
    "    list_sym_ratio.append(sym_ratio)\n",
    "    \n",
    "# dataframe\n",
    "df_features['text'] = list_text\n",
    "df_features['headline'] = list_headline\n",
    "df_features['n_sents'] = list_nsentences\n",
    "df_features['n_words'] = list_nwords\n",
    "df_features['avg_words_sents'] = list_words_sent\n",
    "df_features['word_size'] = list_word_size\n",
    "df_features['unique_words'] = list_unique_words\n",
    "df_features['ttr'] = list_ttr\n",
    "df_features['mltd'] = list_mltd\n",
    "df_features['n_quotes'] = list_nquotes\n",
    "df_features['quotes_ratio'] = list_quotes_ratio\n",
    "df_features['propn_ratio'] = list_propn_ratio\n",
    "df_features['noun_ratio'] = list_noun_ratio\n",
    "df_features['adp_ratio'] = list_adp_ratio\n",
    "df_features['det_ratio'] = list_det_ratio\n",
    "df_features['punct_ratio'] = list_punct_ratio\n",
    "df_features['pron_ratio'] = list_pron_ratio\n",
    "df_features['verb_ratio'] = list_verb_ratio\n",
    "df_features['adv_ratio'] = list_adv_ratio\n",
    "df_features['sym_ratio'] = list_sym_ratio\n",
    "df_features['label'] = df['Label']\n",
    "\n",
    "df_features.to_csv('../data/spanish_corpus_features_v3.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
