{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and feature extraction v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text, extract stylometric features and create a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/corpus_spanish.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>641</td>\n",
       "      <td>True</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Caras</td>\n",
       "      <td>Sofía Castro y Alejandro Peña Pretelini: una i...</td>\n",
       "      <td>Sofía Castro y Alejandro Peña Pretelini: una i...</td>\n",
       "      <td>https://www.caras.com.mx/sofia-castro-alejandr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>Education</td>\n",
       "      <td>Heraldo</td>\n",
       "      <td>Un paso más cerca de hacer los exámenes 'online'</td>\n",
       "      <td>Un paso más cerca de hacer los exámenes 'onlin...</td>\n",
       "      <td>https://www.heraldo.es/noticias/suplementos/he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141</td>\n",
       "      <td>True</td>\n",
       "      <td>Science</td>\n",
       "      <td>HUFFPOST</td>\n",
       "      <td>Esto es lo que los científicos realmente piens...</td>\n",
       "      <td>Esto es lo que los científicos realmente piens...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/scientist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>394</td>\n",
       "      <td>True</td>\n",
       "      <td>Politics</td>\n",
       "      <td>El financiero</td>\n",
       "      <td>Inicia impresión de boletas para elección pres...</td>\n",
       "      <td>Inicia impresión de boletas para elección pres...</td>\n",
       "      <td>http://www.elfinanciero.com.mx/elecciones-2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139</td>\n",
       "      <td>True</td>\n",
       "      <td>Sport</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>A *NUMBER* día del Mundial</td>\n",
       "      <td>A *NUMBER* día del Mundial\\nFIFA.com sigue la ...</td>\n",
       "      <td>https://es.fifa.com/worldcup/news/a-1-dia-del-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id Category          Topic         Source  \\\n",
       "0  641     True  Entertainment          Caras   \n",
       "1    6     True      Education        Heraldo   \n",
       "2  141     True        Science       HUFFPOST   \n",
       "3  394     True       Politics  El financiero   \n",
       "4  139     True          Sport           FIFA   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Sofía Castro y Alejandro Peña Pretelini: una i...   \n",
       "1   Un paso más cerca de hacer los exámenes 'online'   \n",
       "2  Esto es lo que los científicos realmente piens...   \n",
       "3  Inicia impresión de boletas para elección pres...   \n",
       "4                         A *NUMBER* día del Mundial   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Sofía Castro y Alejandro Peña Pretelini: una i...   \n",
       "1  Un paso más cerca de hacer los exámenes 'onlin...   \n",
       "2  Esto es lo que los científicos realmente piens...   \n",
       "3  Inicia impresión de boletas para elección pres...   \n",
       "4  A *NUMBER* día del Mundial\\nFIFA.com sigue la ...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.caras.com.mx/sofia-castro-alejandr...  \n",
       "1  https://www.heraldo.es/noticias/suplementos/he...  \n",
       "2  https://www.huffingtonpost.com/entry/scientist...  \n",
       "3  http://www.elfinanciero.com.mx/elecciones-2018...  \n",
       "4  https://es.fifa.com/worldcup/news/a-1-dia-del-...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(971, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id           int64\n",
       "Category    object\n",
       "Topic       object\n",
       "Source      object\n",
       "Headline    object\n",
       "Text        object\n",
       "Link        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 971 entries, 0 to 970\n",
      "Data columns (total 7 columns):\n",
      "Id          971 non-null int64\n",
      "Category    971 non-null object\n",
      "Topic       971 non-null object\n",
      "Source      971 non-null object\n",
      "Headline    971 non-null object\n",
      "Text        971 non-null object\n",
      "Link        971 non-null object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 53.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are using `spacy`: The NLP *Ruby on Rails* \n",
    "\n",
    "[spacy](http://www.spacy.io/) is a library of natural language processing, robust, fast, easy to install and to use. It can be used with other NLP and Deep Learning Libraries.\n",
    "\n",
    "With its pre-trained models in spanish language, we can operate the typical NLP jobs: Sentences segmentation, tokenization, POS tag, etc...\n",
    "\n",
    "We are going to use the `es_core_news_lg` pre-trained model to make pos tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-76a01d9c502b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for pipe11: \n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install build-essential python-dev git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es_core_news_lg==2.3.1 from https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-2.3.1/es_core_news_lg-2.3.1.tar.gz#egg=es_core_news_lg==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-2.3.1/es_core_news_lg-2.3.1.tar.gz (573.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 573.1MB 1.6MB/s ta 0:00:0111��█████████                      | 178.2MB 52.8MB/s eta 0:00:08��█████▌                    | 206.2MB 27.4MB/s eta 0:00:140MB 23.3MB/s eta 0:00:13.5MB 54.1MB/s eta 0:00:05 341.3MB 109.5MB/s eta 0:00:03██████▏            | 343.6MB 106.8MB/s eta 0:00:03�█████▍            | 346.5MB 90.2MB/s eta 0:00:03��██████████████████           | 376.0MB 25.2MB/s eta 0:00:08��██████████████████           | 377.9MB 29.5MB/s eta 0:00:07/s eta 0:00:08/s eta 0:00:07/s eta 0:00:05B/s eta 0:00:02█████ | 554.9MB 13.9MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting spacy<2.4.0,>=2.3.0 (from es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/18/db/499f374339b522b6618234b93f25d2990692795ccce3152519ccc508586c/spacy-2.3.2.tar.gz (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 2.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/31/247b34db5ab06afaf5512481e77860fb4cd7a0c0ddff9d2566651c8c2f07/murmurhash-1.0.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/8d/d095bbb109a004351c85c83bc853782fc27692693b305dd7b170c36a1262/cymem-2.0.3.tar.gz (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 45.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/27/32d860083e0708e36a2266bed865dba4b55c991a84688932122e48ef65b4/preshed-3.0.2-cp27-cp27mu-manylinux1_x86_64.whl (113kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 9.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting thinc==7.4.1 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/5d/4343b3a79565af88ba2d53818d97995c3c239288f2565b826865f376d271/thinc-7.4.1.tar.gz (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 2.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting blis<0.5.0,>=0.4.0 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/db/db/bfae863870f79260e57e293dd835e848e8450d2a2c9e273795b13060ff86/blis-0.4.1-cp27-cp27mu-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.7MB 2.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/d1/a23917773a5759b36d1dc8433d15fb40700ca29d5ba924d6350c38a8ef8e/wasabi-0.7.1.tar.gz\n",
      "Collecting srsly<1.1.0,>=1.0.2 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/28/ffb9f0b940041aeaec2194e840b5ffe19d0ae252de89579fa8b810174d9f/srsly-1.0.2.tar.gz (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 5.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 6.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting setuptools (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/b7/182161210a13158cd3ccc41ee19aadef54496b74f2817cc147006ec932b4/setuptools-44.1.1-py2.py3-none-any.whl (583kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 1.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.15.0 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/3a/5f/47e578b3ae79e2624e205445ab77a1848acdaa2929a00eeef6b16eaaeb20/numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl (17.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.0MB 1.9MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 462kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pathlib==1.0.1 (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/aa/9b065a76b9af472437a0059f77e8f962fe350438b927cb80184c32f075eb/pathlib-1.0.1.tar.gz (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting importlib-metadata>=0.20; python_version < \"3.8\" (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/58/cdea07eb51fc2b906db0968a94700866fc46249bdc75cac23f9d13168929/importlib_metadata-1.7.0-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 1.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting chardet<4,>=3.0.2 (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 2.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 2.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting idna<3,>=2.5 (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting zipp>=0.5 (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/96/0a/67556e9b7782df7118c1f49bdc494da5e5e429c93aa77965f33e81287c8c/zipp-1.2.0-py2.py3-none-any.whl\n",
      "Collecting configparser>=3.5; python_version < \"3\" (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
      "Collecting contextlib2; python_version < \"3\" (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/85/60/370352f7ef6aa96c52fb001831622f50f923c1d575427d021b8ab3311236/contextlib2-0.6.0.post1-py2.py3-none-any.whl\n",
      "Collecting pathlib2; python_version < \"3\" (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://files.pythonhosted.org/packages/e9/45/9c82d3666af4ef9f221cbb954e1d77ddbb513faf552aea6df5f37f1a4859/pathlib2-2.3.5-py2.py3-none-any.whl\n",
      "Collecting six (from pathlib2; python_version < \"3\"->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting scandir; python_version < \"3.5\" (from pathlib2; python_version < \"3\"->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/df/f5/9c052db7bd54d0cbf1bc0bb6554362bba1012d03e5888950a4f5c5dadc4e/scandir-1.10.0.tar.gz\n",
      "Installing collected packages: murmurhash, cymem, preshed, numpy, blis, wasabi, pathlib, srsly, contextlib2, zipp, configparser, six, scandir, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc, setuptools, urllib3, chardet, certifi, idna, requests, spacy, es-core-news-lg\n",
      "  Running setup.py install for cymem ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for wasabi ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for pathlib ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for srsly ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for scandir ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for thinc ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for spacy ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for es-core-news-lg ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed blis-0.4.1 catalogue-1.0.0 certifi-2020.6.20 chardet-3.0.4 configparser-4.0.2 contextlib2-0.6.0.post1 cymem-2.0.3 es-core-news-lg-2.3.1 idna-2.10 importlib-metadata-1.7.0 murmurhash-1.0.2 numpy-1.16.6 pathlib-1.0.1 pathlib2-2.3.5 plac-1.1.3 preshed-3.0.2 requests-2.24.0 scandir-1.10.0 setuptools-44.1.1 six-1.15.0 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 tqdm-4.48.2 urllib3-1.25.10 wasabi-0.7.1 zipp-1.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f8bcbc7fc4b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# cargamos el modelo entrenado en español\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp_spacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'es_core_news_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# cargamos el modelo entrenado en español\n",
    "nlp_spacy = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and complexity features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "We are using this encoding technique for the target label instead one hot encoding, reasons:\n",
    "\n",
    " - The categorical features are binary\n",
    " - Not problem with features being ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.5 s, sys: 281 ms, total: 5.78 s\n",
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk import word_tokenize, sent_tokenize  \n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df['Label'] = labelencoder.fit_transform(df['Category'])\n",
    "\n",
    "df_features = pd.DataFrame()\n",
    "\n",
    "list_text = []\n",
    "list_headline = []\n",
    "list_sentences_t = []\n",
    "list_words_t = []\n",
    "list_words_sent_t = []\n",
    "list_word_size_t = []\n",
    "list_ttr_t = []\n",
    "list_sentences_h = []\n",
    "list_words_h = []\n",
    "list_words_sent_h = []\n",
    "list_word_size_h = []\n",
    "list_ttr_h = []\n",
    "\n",
    "for n, row in df.iterrows():\n",
    "    \n",
    "    headline = df['Headline'].iloc[n]\n",
    "    text = df['Text'].iloc[n]\n",
    "    \n",
    "    text = text.replace(r\"http\\S+\", \"\")\n",
    "    text = text.replace(r\"http\", \"\")\n",
    "    text = text.replace(r\"@\\S+\", \"\")\n",
    "    text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "    text = text.lower()\n",
    "        \n",
    "    headline = headline.replace(r\"http\\S+\", \"\")\n",
    "    headline = headline.replace(r\"http\", \"\")\n",
    "    headline = headline.replace(r\"@\\S+\", \"\")\n",
    "    headline = headline.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "    headline = headline.lower()\n",
    "\n",
    "    sent_tokens_text = nltk.sent_tokenize(text)\n",
    "    sent_tokens_headline = nltk.sent_tokenize(headline)\n",
    "\n",
    "    # number of sentences\n",
    "    n_sentences_text = len(sent_tokens_text)\n",
    "    n_sentences_headline = len(sent_tokens_headline)\n",
    "\n",
    "    word_tokens_text = nltk.word_tokenize(text)\n",
    "    word_tokens_headline = nltk.word_tokenize(headline)\n",
    "\n",
    "    stop_words = stopwords.words('spanish')\n",
    "    stop_words.extend(list(punctuation))\n",
    "    stop_words.extend(['¿', '¡', '\"', '``']) \n",
    "    stop_words.extend(map(str,range(10)))\n",
    "\n",
    "    filtered_tokens_text = [n for n in word_tokens_text if n not in stop_words]\n",
    "    filtered_tokens_headline = [n for n in word_tokens_headline if n not in stop_words]\n",
    "\n",
    "    # number of tokens/words\n",
    "    n_words_text = len(filtered_tokens_text)\n",
    "    n_words_headline = len(filtered_tokens_headline)\n",
    "\n",
    "    # average words per sentence\n",
    "    avg_word_sentences_text = (float(n_words_text) / n_sentences_text)\n",
    "#     avg_word_sentences_headline = (float(n_words_headline) / n_sentences_headline)\n",
    "\n",
    "    # average word size\n",
    "    word_size_text = sum(len(word) for word in filtered_tokens_text) / n_words_text\n",
    "    word_size_headline = sum(len(word) for word in filtered_tokens_headline) / n_words_headline\n",
    "\n",
    "    # type token ratio\n",
    "    types_text = nltk.Counter(filtered_tokens_text)\n",
    "    ttr_text = (len(types_text) / n_words_text) * 100\n",
    "    \n",
    "    types_headline = nltk.Counter(filtered_tokens_headline)\n",
    "    ttr_headline = (len(types_headline) / n_words_headline) * 100\n",
    "    \n",
    "    # text\n",
    "    list_text.append(text)\n",
    "    list_sentences_t.append(n_sentences_text)\n",
    "    list_words_t.append(n_words_text)\n",
    "    list_words_sent_t.append(avg_word_sentences_text)\n",
    "    list_word_size_t.append(word_size_text)\n",
    "    list_ttr_t.append(ttr_text)\n",
    "    \n",
    "    # headline\n",
    "    list_headline.append(headline)    \n",
    "#     list_sentences_h.append(n_sentences_headline) #irrelevant\n",
    "    list_words_h.append(n_words_headline)\n",
    "#     list_words_sent_h.append(avg_word_sentences_headline) # irrelevant\n",
    "    list_word_size_h.append(word_size_headline)\n",
    "    list_ttr_h.append(ttr_headline)\n",
    "\n",
    "df_features['headline'] = list_headline\n",
    "df_features['text'] = list_text\n",
    "df_features['n_sentences_text'] = list_sentences_t\n",
    "df_features['n_words_text'] = list_words_t\n",
    "df_features['avg_words_sent_text'] = list_words_sent_t\n",
    "df_features['avg_word_size_text'] = list_word_size_t\n",
    "df_features['ttr_text'] = list_ttr_t\n",
    "# df_features['n_sentences_headline'] # list_sentences_h # irrelevant\n",
    "df_features['n_words_headline'] = list_words_h\n",
    "# df_features['avg_words_sent_headline'] = list_words_sent_h # irrelevant\n",
    "df_features['avg_word_size_headline'] = list_word_size_h\n",
    "df_features['ttr_headline'] = list_ttr_h\n",
    "df_features['label'] = df['Label']\n",
    "\n",
    "df_features.to_csv('../data/spanish_corpus_features_v2.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>n_sentences_text</th>\n",
       "      <th>n_words_text</th>\n",
       "      <th>avg_words_sent_text</th>\n",
       "      <th>avg_word_size_text</th>\n",
       "      <th>ttr_text</th>\n",
       "      <th>n_words_headline</th>\n",
       "      <th>avg_word_size_headline</th>\n",
       "      <th>ttr_headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sofía castro y alejandro peña pretelini: una i...</td>\n",
       "      <td>sofía castro y alejandro peña pretelini: una i...</td>\n",
       "      <td>5</td>\n",
       "      <td>123</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>6.398374</td>\n",
       "      <td>69.105691</td>\n",
       "      <td>8</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>un paso más cerca de hacer los exámenes 'online'</td>\n",
       "      <td>un paso más cerca de hacer los exámenes 'onlin...</td>\n",
       "      <td>8</td>\n",
       "      <td>224</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>7.205357</td>\n",
       "      <td>77.232143</td>\n",
       "      <td>5</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esto es lo que los científicos realmente piens...</td>\n",
       "      <td>esto es lo que los científicos realmente piens...</td>\n",
       "      <td>29</td>\n",
       "      <td>467</td>\n",
       "      <td>16.103448</td>\n",
       "      <td>7.573876</td>\n",
       "      <td>64.668094</td>\n",
       "      <td>4</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inicia impresión de boletas para elección pres...</td>\n",
       "      <td>inicia impresión de boletas para elección pres...</td>\n",
       "      <td>10</td>\n",
       "      <td>167</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>7.964072</td>\n",
       "      <td>63.473054</td>\n",
       "      <td>5</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a *number* día del mundial</td>\n",
       "      <td>a *number* día del mundial\\nfifa.com sigue la ...</td>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>7.368421</td>\n",
       "      <td>84.210526</td>\n",
       "      <td>3</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interpol ordena detención inmediata de osorio ...</td>\n",
       "      <td>interpol ordena detención inmediata de osorio ...</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>7.732143</td>\n",
       "      <td>89.285714</td>\n",
       "      <td>8</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"los ninis\" más ricos y poderosos del país: hi...</td>\n",
       "      <td>\"los ninis\" más ricos y poderosos del país: hi...</td>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>6.358025</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>7</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gobierno de alfredo del mazo inició con récord...</td>\n",
       "      <td>para todo sacan lo del populismo, ni siquiera ...</td>\n",
       "      <td>11</td>\n",
       "      <td>183</td>\n",
       "      <td>16.636364</td>\n",
       "      <td>6.677596</td>\n",
       "      <td>79.234973</td>\n",
       "      <td>6</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conapred investiga acto de racismo en el pumas...</td>\n",
       "      <td>conapred investiga acto de racismo en el pumas...</td>\n",
       "      <td>6</td>\n",
       "      <td>105</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>6.419048</td>\n",
       "      <td>74.285714</td>\n",
       "      <td>7</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cristiano ronaldo acepta dos años de prisión</td>\n",
       "      <td>cristiano ronaldo acepta dos años de prisión\\n...</td>\n",
       "      <td>16</td>\n",
       "      <td>270</td>\n",
       "      <td>16.875000</td>\n",
       "      <td>6.918519</td>\n",
       "      <td>59.259259</td>\n",
       "      <td>6</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  sofía castro y alejandro peña pretelini: una i...   \n",
       "1   un paso más cerca de hacer los exámenes 'online'   \n",
       "2  esto es lo que los científicos realmente piens...   \n",
       "3  inicia impresión de boletas para elección pres...   \n",
       "4                         a *number* día del mundial   \n",
       "5  interpol ordena detención inmediata de osorio ...   \n",
       "6  \"los ninis\" más ricos y poderosos del país: hi...   \n",
       "7  gobierno de alfredo del mazo inició con récord...   \n",
       "8  conapred investiga acto de racismo en el pumas...   \n",
       "9       cristiano ronaldo acepta dos años de prisión   \n",
       "\n",
       "                                                text  n_sentences_text  \\\n",
       "0  sofía castro y alejandro peña pretelini: una i...                 5   \n",
       "1  un paso más cerca de hacer los exámenes 'onlin...                 8   \n",
       "2  esto es lo que los científicos realmente piens...                29   \n",
       "3  inicia impresión de boletas para elección pres...                10   \n",
       "4  a *number* día del mundial\\nfifa.com sigue la ...                 4   \n",
       "5  interpol ordena detención inmediata de osorio ...                 3   \n",
       "6  \"los ninis\" más ricos y poderosos del país: hi...                 5   \n",
       "7  para todo sacan lo del populismo, ni siquiera ...                11   \n",
       "8  conapred investiga acto de racismo en el pumas...                 6   \n",
       "9  cristiano ronaldo acepta dos años de prisión\\n...                16   \n",
       "\n",
       "   n_words_text  avg_words_sent_text  avg_word_size_text   ttr_text  \\\n",
       "0           123            24.600000            6.398374  69.105691   \n",
       "1           224            28.000000            7.205357  77.232143   \n",
       "2           467            16.103448            7.573876  64.668094   \n",
       "3           167            16.700000            7.964072  63.473054   \n",
       "4            57            14.250000            7.368421  84.210526   \n",
       "5            56            18.666667            7.732143  89.285714   \n",
       "6            81            16.200000            6.358025  81.481481   \n",
       "7           183            16.636364            6.677596  79.234973   \n",
       "8           105            17.500000            6.419048  74.285714   \n",
       "9           270            16.875000            6.918519  59.259259   \n",
       "\n",
       "   n_words_headline  avg_word_size_headline  ttr_headline  label  \n",
       "0                 8                7.500000         100.0      1  \n",
       "1                 5                5.800000         100.0      1  \n",
       "2                 4                9.500000         100.0      1  \n",
       "3                 5                8.400000         100.0      1  \n",
       "4                 3                5.333333         100.0      1  \n",
       "5                 8                7.750000         100.0      0  \n",
       "6                 7                4.857143         100.0      0  \n",
       "7                 6                6.500000         100.0      1  \n",
       "8                 7                6.000000         100.0      1  \n",
       "9                 6                6.000000         100.0      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting because the type token ratio seems to be 100 in every headline, except this 31 headlines:\n",
    "Looking at them we realize there are the double type token ration less than 100 in fake news than the real ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>n_sentences_text</th>\n",
       "      <th>n_words_text</th>\n",
       "      <th>avg_words_sent_text</th>\n",
       "      <th>avg_word_size_text</th>\n",
       "      <th>ttr_text</th>\n",
       "      <th>n_words_headline</th>\n",
       "      <th>avg_word_size_headline</th>\n",
       "      <th>ttr_headline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       headline  text  n_sentences_text  n_words_text  avg_words_sent_text  \\\n",
       "label                                                                        \n",
       "0            20    20                20            20                   20   \n",
       "1            11    11                11            11                   11   \n",
       "\n",
       "       avg_word_size_text  ttr_text  n_words_headline  avg_word_size_headline  \\\n",
       "label                                                                           \n",
       "0                      20        20                20                      20   \n",
       "1                      11        11                11                      11   \n",
       "\n",
       "       ttr_headline  \n",
       "label                \n",
       "0                20  \n",
       "1                11  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[df_features['ttr_headline'] < 100.0].groupby('label').count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
