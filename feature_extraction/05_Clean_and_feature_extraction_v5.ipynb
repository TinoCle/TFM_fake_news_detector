{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and feature extraction v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text, extract stylometric, lexical and complexity features and create a new csv\n",
    "\n",
    "## We are using `spacy`: The NLP *Ruby on Rails* \n",
    "\n",
    "[spacy](http://www.spacy.io/) is a library of natural language processing, robust, fast, easy to install and to use. It can be used with other NLP and Deep Learning Libraries.\n",
    "\n",
    "With its pre-trained models in spanish language, we can operate the typical NLP jobs: Sentences segmentation, tokenization, POS tag, etc...\n",
    "\n",
    "We are going to use the `es_core_news_lg` pre-trained model to make pos tagging:\n",
    "\n",
    "### Also extracting headline features \n",
    "\n",
    "## V5 with syllabizer, avg_syllables, and readability spanish tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/corpus_spanish.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>641</td>\n",
       "      <td>True</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Caras</td>\n",
       "      <td>Sofía Castro y Alejandro Peña Pretelini: una i...</td>\n",
       "      <td>Sofía Castro y Alejandro Peña Pretelini: una i...</td>\n",
       "      <td>https://www.caras.com.mx/sofia-castro-alejandr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>Education</td>\n",
       "      <td>Heraldo</td>\n",
       "      <td>Un paso más cerca de hacer los exámenes 'online'</td>\n",
       "      <td>Un paso más cerca de hacer los exámenes 'onlin...</td>\n",
       "      <td>https://www.heraldo.es/noticias/suplementos/he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141</td>\n",
       "      <td>True</td>\n",
       "      <td>Science</td>\n",
       "      <td>HUFFPOST</td>\n",
       "      <td>Esto es lo que los científicos realmente piens...</td>\n",
       "      <td>Esto es lo que los científicos realmente piens...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/scientist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>394</td>\n",
       "      <td>True</td>\n",
       "      <td>Politics</td>\n",
       "      <td>El financiero</td>\n",
       "      <td>Inicia impresión de boletas para elección pres...</td>\n",
       "      <td>Inicia impresión de boletas para elección pres...</td>\n",
       "      <td>http://www.elfinanciero.com.mx/elecciones-2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139</td>\n",
       "      <td>True</td>\n",
       "      <td>Sport</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>A *NUMBER* día del Mundial</td>\n",
       "      <td>A *NUMBER* día del Mundial\\nFIFA.com sigue la ...</td>\n",
       "      <td>https://es.fifa.com/worldcup/news/a-1-dia-del-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id Category          Topic         Source  \\\n",
       "0  641     True  Entertainment          Caras   \n",
       "1    6     True      Education        Heraldo   \n",
       "2  141     True        Science       HUFFPOST   \n",
       "3  394     True       Politics  El financiero   \n",
       "4  139     True          Sport           FIFA   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Sofía Castro y Alejandro Peña Pretelini: una i...   \n",
       "1   Un paso más cerca de hacer los exámenes 'online'   \n",
       "2  Esto es lo que los científicos realmente piens...   \n",
       "3  Inicia impresión de boletas para elección pres...   \n",
       "4                         A *NUMBER* día del Mundial   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Sofía Castro y Alejandro Peña Pretelini: una i...   \n",
       "1  Un paso más cerca de hacer los exámenes 'onlin...   \n",
       "2  Esto es lo que los científicos realmente piens...   \n",
       "3  Inicia impresión de boletas para elección pres...   \n",
       "4  A *NUMBER* día del Mundial\\nFIFA.com sigue la ...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.caras.com.mx/sofia-castro-alejandr...  \n",
       "1  https://www.heraldo.es/noticias/suplementos/he...  \n",
       "2  https://www.huffingtonpost.com/entry/scientist...  \n",
       "3  http://www.elfinanciero.com.mx/elecciones-2018...  \n",
       "4  https://es.fifa.com/worldcup/news/a-1-dia-del-...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(971, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id           int64\n",
       "Category    object\n",
       "Topic       object\n",
       "Source      object\n",
       "Headline    object\n",
       "Text        object\n",
       "Link        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 971 entries, 0 to 970\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Id        971 non-null    int64 \n",
      " 1   Category  971 non-null    object\n",
      " 2   Topic     971 non-null    object\n",
      " 3   Source    971 non-null    object\n",
      " 4   Headline  971 non-null    object\n",
      " 5   Text      971 non-null    object\n",
      " 6   Link      971 non-null    object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 53.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>555</td>\n",
       "      <td>True</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN español</td>\n",
       "      <td>Trump enviará tropas de la Guardia Nacional a ...</td>\n",
       "      <td>Trump enviará tropas de la Guardia Nacional a ...</td>\n",
       "      <td>https://cnnespanol.cnn.com/2018/04/04/la-casa-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>633</td>\n",
       "      <td>True</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Milenio</td>\n",
       "      <td>Yuawi López prepara canción para el Mundial Ru...</td>\n",
       "      <td>Yuawi López prepara canción para el Mundial Ru...</td>\n",
       "      <td>http://www.milenio.com/virales/yuawi-lopez-pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>563</td>\n",
       "      <td>Fake</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Censura 0</td>\n",
       "      <td>Angélica Rivera: \"AMLO no llegará a ser presid...</td>\n",
       "      <td>Angélica Rivera: \"AMLO no llegará a ser presid...</td>\n",
       "      <td>http://censura0.com/2018/04/03/angelica-rivera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>278</td>\n",
       "      <td>Fake</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>El Ruinaversal</td>\n",
       "      <td>Disney compra los derechos para explotar la im...</td>\n",
       "      <td>Disney compra los derechos para explotar la im...</td>\n",
       "      <td>http://www.elruinaversal.com/2017/06/20/disney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>235</td>\n",
       "      <td>True</td>\n",
       "      <td>Economy</td>\n",
       "      <td>El financiero</td>\n",
       "      <td>AMLO aceleraría el consumo y el crecimiento ec...</td>\n",
       "      <td>AMLO aceleraría el consumo y el crecimiento ec...</td>\n",
       "      <td>http://www.elfinanciero.com.mx/economia/amlo-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id Category          Topic          Source  \\\n",
       "525  555     True       Politics     CNN español   \n",
       "746  633     True  Entertainment         Milenio   \n",
       "60   563     Fake       Politics       Censura 0   \n",
       "10   278     Fake  Entertainment  El Ruinaversal   \n",
       "152  235     True        Economy   El financiero   \n",
       "\n",
       "                                              Headline  \\\n",
       "525  Trump enviará tropas de la Guardia Nacional a ...   \n",
       "746  Yuawi López prepara canción para el Mundial Ru...   \n",
       "60   Angélica Rivera: \"AMLO no llegará a ser presid...   \n",
       "10   Disney compra los derechos para explotar la im...   \n",
       "152  AMLO aceleraría el consumo y el crecimiento ec...   \n",
       "\n",
       "                                                  Text  \\\n",
       "525  Trump enviará tropas de la Guardia Nacional a ...   \n",
       "746  Yuawi López prepara canción para el Mundial Ru...   \n",
       "60   Angélica Rivera: \"AMLO no llegará a ser presid...   \n",
       "10   Disney compra los derechos para explotar la im...   \n",
       "152  AMLO aceleraría el consumo y el crecimiento ec...   \n",
       "\n",
       "                                                  Link  \n",
       "525  https://cnnespanol.cnn.com/2018/04/04/la-casa-...  \n",
       "746  http://www.milenio.com/virales/yuawi-lopez-pre...  \n",
       "60   http://censura0.com/2018/04/03/angelica-rivera...  \n",
       "10   http://www.elruinaversal.com/2017/06/20/disney...  \n",
       "152  http://www.elfinanciero.com.mx/economia/amlo-a...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply it to the full corpus with iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried several syllabizers for spanish and this is the chosen solution. Believe me, i spent a whole day.\n",
    "# I had to replace all symbols, punctuations and it includes accentuation from other languages like ä, à, etc...\n",
    "# It's a bit inconsistent with words from others languages, acronyms and abbreviations. However it performs really well\n",
    "\n",
    "def get_nsyllables(text):\n",
    "    from syltippy import syllabize\n",
    "\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(r'[^ \\nA-Za-z0-9ÁÉÍÓÚÑáéíóúñ/]+', '', text)\n",
    "    \n",
    "    n_syllables = len(syllabize(text)[0])\n",
    "    \n",
    "    return n_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 1.72 s, total: 1min 11s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lexical_diversity import lex_div as ld\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "df = pd.read_csv('../data/corpus_spanish.csv')\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df['Label'] = labelencoder.fit_transform(df['Category'])\n",
    "\n",
    "# empty lists and df\n",
    "df_features = pd.DataFrame()\n",
    "list_text = []\n",
    "list_sentences = []\n",
    "list_words = []\n",
    "list_words_sent = []\n",
    "list_word_size = []\n",
    "list_avg_syllables_word = []\n",
    "list_unique_words = []\n",
    "list_ttr = []\n",
    "list_mltd = []\n",
    "list_entity_ratio = []\n",
    "list_i_fernandez_huerta = []\n",
    "list_i_szigriszt_pazos = []\n",
    "list_nquotes = []\n",
    "list_quotes_ratio = []\n",
    "list_propn_ratio = [] \n",
    "list_noun_ratio = []\n",
    "list_adp_ratio = []\n",
    "list_det_ratio = []\n",
    "list_punct_ratio = []\n",
    "list_pron_ratio = []\n",
    "list_verb_ratio = []\n",
    "list_adv_ratio = []\n",
    "list_sym_ratio = []\n",
    "\n",
    "list_headline = []\n",
    "list_words_h = []\n",
    "list_word_size_h = []\n",
    "list_avg_syllables_word_h = []\n",
    "list_ttr_h = []\n",
    "list_mltd_h = []\n",
    "list_unique_words_h = []\n",
    "\n",
    "# df iteration\n",
    "for n, row in df.iterrows():\n",
    "    \n",
    "    ## headline ##\n",
    "    headline = df['Headline'].iloc[n]\n",
    "    headline = re.sub(r\"http\\S+\", \"\", headline)\n",
    "    headline = re.sub(r\"http\", \"\", headline)\n",
    "    headline = re.sub(r\"@\\S+\", \"\", headline)\n",
    "    headline = re.sub(\"\\n\", \" \", headline)\n",
    "    headline = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", headline)\n",
    "    headline = headline.replace(r\"*NUMBER*\", \"número\")\n",
    "    headline = headline.replace(r\"*PHONE*\", \"número\")\n",
    "    headline = headline.replace(r\"*EMAIL*\", \"email\")\n",
    "    headline = headline.replace(r\"*URL*\", \"url\")\n",
    "    headline_new = headline.lower()\n",
    "    doc_h = nlp(headline_new)\n",
    "    \n",
    "    list_tokens_h = []\n",
    "    list_tags_h = []\n",
    "    n_sents_h = 0\n",
    "\n",
    "    for sentence_h in doc_h.sents:\n",
    "        n_sents_h += 1\n",
    "        for token in sentence_h:\n",
    "            list_tokens_h.append(token.text)\n",
    "\n",
    "    fdist_h = FreqDist(list_tokens_h)\n",
    "    n_syllables_h = get_nsyllables(headline)\n",
    "    n_words_h = len(list_tokens_h)\n",
    "    \n",
    "    # headline complexity features\n",
    "    avg_word_size_h = round(sum(len(word) for word in list_tokens_h) / n_words_h, 2)\n",
    "    avg_syllables_word_h = round(n_syllables_h / n_words_h, 2)\n",
    "    unique_words_h = round((len(fdist_h.hapaxes()) / n_words_h) * 100, 2)\n",
    "    ttr_h = round(ld.ttr(list_tokens_h) * 100, 2)\n",
    "    mltd_h = round(ld.mtld(list_tokens_h), 2)\n",
    "    \n",
    "    ## text content##   \n",
    "    text = df['Text'].iloc[n]  \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"http\", \"\", text)\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "    text_new = text.lower()\n",
    "    doc = nlp(text_new)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    list_entities = []\n",
    "    n_sents = 0\n",
    "    \n",
    "    for entity in doc.ents:\n",
    "        list_entities.append(entity.label_)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        n_sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "    \n",
    "    # Calculate entities, pos, tag, freq, syllables, wordsand quotes\n",
    "    n_entities = len(list_entities)\n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "    n_syllables = get_nsyllables(text)\n",
    "    n_words = len(list_tokens)\n",
    "    n_quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "\n",
    "    # complexity features\n",
    "    avg_word_sentence = round(n_words / n_sents, 2)\n",
    "    avg_word_size = round(sum(len(word) for word in list_tokens) / n_words, 2)\n",
    "    avg_syllables_word = round(n_syllables / n_words, 2)\n",
    "    unique_words = round((len(fdist.hapaxes()) / n_words) * 100, 2)\n",
    "    ttr = round(ld.ttr(list_tokens) * 100, 2)\n",
    "    mltd = round(ld.mtld(list_tokens), 2)\n",
    "\n",
    "    # readability spanish test\n",
    "    i_fernandez_huerta = round(206.84 - (60 * avg_syllables_word) - (1.02 * avg_word_sentence), 2)\n",
    "    i_szigriszt_pazos = round(206.835 - ((62.3 * n_syllables) / n_words) - (n_words / n_sents), 2)\n",
    "\n",
    "    # stylometric features\n",
    "    entity_ratio = round((n_entities / n_words) * 100, 2)\n",
    "    quotes_ratio = round((n_quotes / n_words) * 100, 2)\n",
    "    propn_ratio = round((n_pos['PROPN'] / n_words) * 100 , 2)\n",
    "    noun_ratio = round((n_pos['NOUN'] / n_words) * 100, 2) \n",
    "    adp_ratio = round((n_pos['ADP'] / n_words) * 100, 2)\n",
    "    det_ratio = round((n_pos['DET'] / n_words) * 100, 2)\n",
    "    punct_ratio = round((n_pos['PUNCT'] / n_words) * 100, 2)\n",
    "    pron_ratio = round((n_pos['PRON'] / n_words) * 100, 2)\n",
    "    verb_ratio = round((n_pos['VERB'] / n_words) * 100, 2)\n",
    "    adv_ratio = round((n_pos['ADV'] / n_words) * 100, 2)\n",
    "    sym_ratio = round((n_tag['SYM'] / n_words) * 100, 2)\n",
    "    \n",
    "    # appending on lists\n",
    "    # headline\n",
    "    list_headline.append(headline_new)\n",
    "    list_words_h.append(n_words_h)\n",
    "    list_word_size_h.append(avg_word_size_h)\n",
    "    list_avg_syllables_word_h.append(avg_syllables_word_h)\n",
    "    list_unique_words_h.append(unique_words_h)\n",
    "    list_ttr_h.append(ttr_h)\n",
    "    list_mltd_h.append(mltd_h)\n",
    "    \n",
    "    # text\n",
    "    list_text.append(text_new)\n",
    "    list_sentences.append(n_sents)\n",
    "    list_words.append(n_words)\n",
    "    list_words_sent.append(avg_word_sentence)\n",
    "    list_word_size.append(avg_word_size)\n",
    "    list_avg_syllables_word.append(avg_syllables_word)\n",
    "    list_unique_words.append(unique_words)\n",
    "    list_ttr.append(ttr)\n",
    "    list_mltd.append(mltd)\n",
    "    list_i_fernandez_huerta.append(i_fernandez_huerta)\n",
    "    list_i_szigriszt_pazos.append(i_szigriszt_pazos)\n",
    "    list_entity_ratio.append(entity_ratio)\n",
    "    list_nquotes.append(n_quotes)\n",
    "    list_quotes_ratio.append(quotes_ratio)\n",
    "    list_propn_ratio.append(propn_ratio)\n",
    "    list_noun_ratio.append(noun_ratio)\n",
    "    list_adp_ratio.append(adp_ratio)\n",
    "    list_det_ratio.append(det_ratio)\n",
    "    list_punct_ratio.append(punct_ratio)\n",
    "    list_pron_ratio.append(pron_ratio)\n",
    "    list_verb_ratio.append(verb_ratio)\n",
    "    list_adv_ratio.append(adv_ratio)\n",
    "    list_sym_ratio.append(sym_ratio)\n",
    "    \n",
    "# dataframe\n",
    "df_features['text'] = list_text\n",
    "df_features['headline'] = list_headline\n",
    "\n",
    "# headline\n",
    "df_features['words_h'] = list_words_h\n",
    "df_features['word_size_h'] = list_word_size_h\n",
    "df_features['avg_syllables_word_h'] = list_avg_syllables_word_h\n",
    "df_features['unique_words_h'] = list_unique_words_h\n",
    "df_features['ttr_h'] = list_ttr_h\n",
    "df_features['mltd_h'] = list_mltd_h\n",
    "\n",
    "# text\n",
    "df_features['sents'] = list_sentences\n",
    "df_features['words'] = list_words\n",
    "df_features['avg_words_sent'] = list_words_sent\n",
    "df_features['avg_word_size'] = list_word_size\n",
    "df_features['avg_syllables_word'] = list_avg_syllables_word\n",
    "df_features['unique_words'] = list_unique_words\n",
    "df_features['ttr'] = list_ttr\n",
    "df_features['mltd'] = list_mltd\n",
    "df_features['i_fernandez_huerta'] = list_i_fernandez_huerta\n",
    "df_features['i_szigriszt_pazos'] = list_i_szigriszt_pazos\n",
    "df_features['entity_ratio'] = list_entity_ratio\n",
    "df_features['n_quotes'] = list_nquotes\n",
    "df_features['quotes_ratio'] = list_quotes_ratio\n",
    "df_features['propn_ratio'] = list_propn_ratio\n",
    "df_features['noun_ratio'] = list_noun_ratio\n",
    "df_features['adp_ratio'] = list_adp_ratio\n",
    "df_features['det_ratio'] = list_det_ratio\n",
    "df_features['punct_ratio'] = list_punct_ratio\n",
    "df_features['pron_ratio'] = list_pron_ratio\n",
    "df_features['verb_ratio'] = list_verb_ratio\n",
    "df_features['adv_ratio'] = list_adv_ratio\n",
    "df_features['sym_ratio'] = list_sym_ratio\n",
    "\n",
    "df_features['label'] = df['Label']\n",
    "\n",
    "df_features.to_csv('../data/spanish_corpus_features_v5.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>headline</th>\n",
       "      <th>words_h</th>\n",
       "      <th>word_size_h</th>\n",
       "      <th>avg_syllables_word_h</th>\n",
       "      <th>unique_words_h</th>\n",
       "      <th>ttr_h</th>\n",
       "      <th>mltd_h</th>\n",
       "      <th>sents</th>\n",
       "      <th>words</th>\n",
       "      <th>avg_words_sent</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>avg_syllables_word</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>ttr</th>\n",
       "      <th>mltd</th>\n",
       "      <th>i_fernandez_huerta</th>\n",
       "      <th>i_szigriszt_pazos</th>\n",
       "      <th>entity_ratio</th>\n",
       "      <th>n_quotes</th>\n",
       "      <th>quotes_ratio</th>\n",
       "      <th>propn_ratio</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>adp_ratio</th>\n",
       "      <th>det_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>pron_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adv_ratio</th>\n",
       "      <th>sym_ratio</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sofía castro y alejandro peña pretelini: una i...</td>\n",
       "      <td>sofía castro y alejandro peña pretelini: una i...</td>\n",
       "      <td>12</td>\n",
       "      <td>5.58</td>\n",
       "      <td>2.50</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>244</td>\n",
       "      <td>40.67</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.78</td>\n",
       "      <td>36.07</td>\n",
       "      <td>50.82</td>\n",
       "      <td>55.73</td>\n",
       "      <td>58.56</td>\n",
       "      <td>55.10</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.57</td>\n",
       "      <td>15.98</td>\n",
       "      <td>14.34</td>\n",
       "      <td>11.89</td>\n",
       "      <td>9.02</td>\n",
       "      <td>5.74</td>\n",
       "      <td>6.56</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>un paso más cerca de hacer los exámenes 'onlin...</td>\n",
       "      <td>un paso más cerca de hacer los exámenes 'online'</td>\n",
       "      <td>11</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.55</td>\n",
       "      <td>81.82</td>\n",
       "      <td>90.91</td>\n",
       "      <td>33.88</td>\n",
       "      <td>9</td>\n",
       "      <td>462</td>\n",
       "      <td>51.33</td>\n",
       "      <td>4.42</td>\n",
       "      <td>1.79</td>\n",
       "      <td>34.42</td>\n",
       "      <td>46.10</td>\n",
       "      <td>44.90</td>\n",
       "      <td>47.08</td>\n",
       "      <td>44.25</td>\n",
       "      <td>9.52</td>\n",
       "      <td>4</td>\n",
       "      <td>0.87</td>\n",
       "      <td>16.67</td>\n",
       "      <td>17.10</td>\n",
       "      <td>12.99</td>\n",
       "      <td>11.69</td>\n",
       "      <td>18.83</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.46</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esto es lo que los científicos realmente piens...</td>\n",
       "      <td>esto es lo que los científicos realmente piens...</td>\n",
       "      <td>12</td>\n",
       "      <td>4.83</td>\n",
       "      <td>1.67</td>\n",
       "      <td>83.33</td>\n",
       "      <td>91.67</td>\n",
       "      <td>40.32</td>\n",
       "      <td>29</td>\n",
       "      <td>956</td>\n",
       "      <td>32.97</td>\n",
       "      <td>4.91</td>\n",
       "      <td>1.95</td>\n",
       "      <td>26.67</td>\n",
       "      <td>39.33</td>\n",
       "      <td>78.16</td>\n",
       "      <td>56.21</td>\n",
       "      <td>52.59</td>\n",
       "      <td>2.93</td>\n",
       "      <td>39</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.81</td>\n",
       "      <td>18.62</td>\n",
       "      <td>14.33</td>\n",
       "      <td>13.08</td>\n",
       "      <td>11.72</td>\n",
       "      <td>2.62</td>\n",
       "      <td>10.56</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inicia impresión de boletas para elección pres...</td>\n",
       "      <td>inicia impresión de boletas para elección pres...</td>\n",
       "      <td>7</td>\n",
       "      <td>6.86</td>\n",
       "      <td>2.71</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>347</td>\n",
       "      <td>34.70</td>\n",
       "      <td>4.97</td>\n",
       "      <td>2.02</td>\n",
       "      <td>24.21</td>\n",
       "      <td>39.19</td>\n",
       "      <td>54.51</td>\n",
       "      <td>50.25</td>\n",
       "      <td>46.28</td>\n",
       "      <td>2.88</td>\n",
       "      <td>4</td>\n",
       "      <td>1.15</td>\n",
       "      <td>4.32</td>\n",
       "      <td>25.94</td>\n",
       "      <td>19.31</td>\n",
       "      <td>16.14</td>\n",
       "      <td>9.80</td>\n",
       "      <td>2.88</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a número día del mundial fifa.com sigue la cue...</td>\n",
       "      <td>a número día del mundial</td>\n",
       "      <td>5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>29.25</td>\n",
       "      <td>4.85</td>\n",
       "      <td>2.00</td>\n",
       "      <td>53.85</td>\n",
       "      <td>64.96</td>\n",
       "      <td>55.90</td>\n",
       "      <td>57.01</td>\n",
       "      <td>52.99</td>\n",
       "      <td>8.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.11</td>\n",
       "      <td>19.66</td>\n",
       "      <td>19.66</td>\n",
       "      <td>15.38</td>\n",
       "      <td>6.84</td>\n",
       "      <td>3.42</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  sofía castro y alejandro peña pretelini: una i...   \n",
       "1  un paso más cerca de hacer los exámenes 'onlin...   \n",
       "2  esto es lo que los científicos realmente piens...   \n",
       "3  inicia impresión de boletas para elección pres...   \n",
       "4  a número día del mundial fifa.com sigue la cue...   \n",
       "\n",
       "                                            headline  words_h  word_size_h  \\\n",
       "0  sofía castro y alejandro peña pretelini: una i...       12         5.58   \n",
       "1   un paso más cerca de hacer los exámenes 'online'       11         3.64   \n",
       "2  esto es lo que los científicos realmente piens...       12         4.83   \n",
       "3  inicia impresión de boletas para elección pres...        7         6.86   \n",
       "4                           a número día del mundial        5         4.00   \n",
       "\n",
       "   avg_syllables_word_h  unique_words_h   ttr_h  mltd_h  sents  words  \\\n",
       "0                  2.50          100.00  100.00    0.00      6    244   \n",
       "1                  1.55           81.82   90.91   33.88      9    462   \n",
       "2                  1.67           83.33   91.67   40.32     29    956   \n",
       "3                  2.71          100.00  100.00    0.00     10    347   \n",
       "4                  1.80          100.00  100.00    0.00      4    117   \n",
       "\n",
       "   avg_words_sent  avg_word_size  avg_syllables_word  unique_words    ttr  \\\n",
       "0           40.67           4.30                1.78         36.07  50.82   \n",
       "1           51.33           4.42                1.79         34.42  46.10   \n",
       "2           32.97           4.91                1.95         26.67  39.33   \n",
       "3           34.70           4.97                2.02         24.21  39.19   \n",
       "4           29.25           4.85                2.00         53.85  64.96   \n",
       "\n",
       "    mltd  i_fernandez_huerta  i_szigriszt_pazos  entity_ratio  n_quotes  \\\n",
       "0  55.73               58.56              55.10          7.38         0   \n",
       "1  44.90               47.08              44.25          9.52         4   \n",
       "2  78.16               56.21              52.59          2.93        39   \n",
       "3  54.51               50.25              46.28          2.88         4   \n",
       "4  55.90               57.01              52.99          8.55         0   \n",
       "\n",
       "   quotes_ratio  propn_ratio  noun_ratio  adp_ratio  det_ratio  punct_ratio  \\\n",
       "0          0.00        15.57       15.98      14.34      11.89         9.02   \n",
       "1          0.87        16.67       17.10      12.99      11.69        18.83   \n",
       "2          4.08         4.81       18.62      14.33      13.08        11.72   \n",
       "3          1.15         4.32       25.94      19.31      16.14         9.80   \n",
       "4          0.00        11.11       19.66      19.66      15.38         6.84   \n",
       "\n",
       "   pron_ratio  verb_ratio  adv_ratio  sym_ratio  label  \n",
       "0        5.74        6.56       3.28       0.00      1  \n",
       "1        1.30        3.46       1.73       1.30      1  \n",
       "2        2.62       10.56       4.81       0.21      1  \n",
       "3        2.88        7.78       0.29       0.00      1  \n",
       "4        3.42        4.27       1.71       0.00      1  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
