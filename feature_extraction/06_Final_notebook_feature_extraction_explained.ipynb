{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "This is the final notebook explaining the methodology for text feature extraction. We will explain each of the different features extracted and then we will explain the process and methodology to extract all the features and create a new dataset.\n",
    "\n",
    "## Index\n",
    "\n",
    "- [1. Features](#1.-Feature-explanation)\n",
    "\n",
    " - [1.1. Complexity features](#1.1.-Complexity-features)\n",
    " - [1.2. Stylometric features](#1.2.-Stylometric-features)\n",
    "\n",
    "\n",
    "- [2. Requisites](#2.-Requisites)\n",
    "\n",
    "\n",
    "- [3. Feature extraction for training](#3.-Feature-extraction-for-training)\n",
    "\n",
    "\n",
    "- [4. Feature extraction function for predictions](#4.-Feature-extraction-function-for-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature explanation\n",
    "\n",
    "On this section we will explain the features that we are going to extract from the News Headline and News Content text. These features are language-independent, for example, they do not consider specific terms from a language, in this case spanish.\n",
    "\n",
    "Our objective is to extract features based on high-level structures. To accomplish this objective, we are going to extract features from 2 categories: Complexity and Stylometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Complexity features\n",
    "\n",
    "The objective of these features is to capture te overall intricacy of the news, in sentence and word level. To achive this, we use metrics like average word size, words count per sentence and type token ratio:\n",
    "\n",
    "**avg_words_sentence**: Average words per sentence\n",
    "\n",
    "**avg_word_size**: Average word size\n",
    "\n",
    "**avg_syllables_word**: Average syllables per word\n",
    "\n",
    "**unique_words**: Hapaxes or unique words that only appears once in a text\n",
    "\n",
    "**ttr**: Type token ratio\n",
    "\n",
    "### Bonus ###\n",
    "\n",
    "Spanish readability tests:\n",
    "\n",
    "**huerta_score**: Fernández Huerta's redability score (Reading comprehension of the text), spanish adaptation of the Flesch equation\n",
    "\n",
    "$$Perspicuity = 206.84 - 0.60 \\times Average Syllables Word - 1.02 \\times Average Words Sentence$$\n",
    "\n",
    "**szigriszt_score**: Szigriszt Pazos perspicuity score (Legibility and clarity of the text), a modern spanish adaptation of the Flesch equation.\n",
    "\n",
    "$$Perspicuity = 206.835 - \\frac{62.3 \\times TotalSyllables}{Words} - \\frac{Words}{Sentences}$$\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Stylometric features\n",
    "For stylometric or lexical features, we use NLP techniques to extract grammatical and lexical information for each text. We are using Spacy POS tagging techniques to track different word style frequencies:\n",
    "\n",
    "**mltd**: Measure of Textual Lexical Diversity, based on McCarthy and Jarvis (2010).\n",
    "\n",
    "**upper_case_ratio**: Uppercase letters to all letters ratio\n",
    "\n",
    "**entityratio**: Ratio of named Entities to the text size\n",
    "\n",
    "**quotes_ratio**: Ratio of quotes marks to text size\n",
    "\n",
    "**propn_ratio**: Proper Noun tag frequency\n",
    "\n",
    "**noun_ratio**: Noun tag frequency\n",
    "\n",
    "**pron_ratio**: Pronoun tag frequency\n",
    "\n",
    "**adp_ratio**: Adposition tag frequency\n",
    "\n",
    "**det_ratio**: Determinant tag frequency\n",
    "\n",
    "**punct_ratio**: Punctuation tag frequency\n",
    "\n",
    "**verb_ratio**: Verb tag frequency\n",
    "\n",
    "**adv_ratio**: Adverb tag frequency\n",
    "\n",
    "**sym_ratio**: Symbol tag frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Requisites\n",
    "\n",
    "*For Python 3 installations use ___!pip3 install___ and ___python3 *___\n",
    "\n",
    "[NLTK package](https://pypi.org/project/nltk/)\n",
    "\n",
    "`!pip install nltk`\n",
    "\n",
    "`import nltk`\n",
    "\n",
    "\n",
    "[Spacy spanish package](https://spacy.io/models/es)\n",
    "\n",
    "`!pip install spacy`\n",
    "\n",
    "`python -m spacy download es_core_news_lg`\n",
    "\n",
    "`import spacy`\n",
    "\n",
    "\n",
    "[lexical_diversity package](https://pypi.org/project/lexical-diversity/)\n",
    "\n",
    "`!pip install lexical-diversity`\n",
    "\n",
    "`from lexical_diversity import lex_div as ld`\n",
    "\n",
    "\n",
    "[Syltippy](https://github.com/nur-ag/syltippy)\n",
    "\n",
    "Syltippy is a simple, user friendly word syllabization package for spanish language with no additional dependencies.\n",
    "\n",
    "`!pip install syltippy`\n",
    "\n",
    "`from syltippy import syllabize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature extraction for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried several syllabizers for spanish and this is the chosen solution. Believe me, i spent a whole day.\n",
    "# I had to replace all symbols, punctuations and it includes accentuation from other languages like ä, à, etc...\n",
    "# It's a bit inconsistent with words from others languages, acronyms and abreviations. However it performs really well for our case!!!\n",
    "\n",
    "def get_nsyllables(text):\n",
    "    from syltippy import syllabize\n",
    "\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(r'[^ \\nA-Za-z0-9ÁÉÍÓÚÑáéíóúñ/]+', '', text)\n",
    "    \n",
    "    n_syllables = len(syllabize(text)[0])\n",
    "    \n",
    "    return n_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 30s, sys: 3.05 s, total: 5min 33s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lexical_diversity import lex_div as ld\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "df = pd.read_csv('../data/corpus_spanish_v3.csv')\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df['Label'] = labelencoder.fit_transform(df['Category'])\n",
    "\n",
    "# empty lists and df\n",
    "df_features = pd.DataFrame()\n",
    "list_text = []\n",
    "list_sentences = []\n",
    "list_words = []\n",
    "list_words_sent = []\n",
    "list_word_size = []\n",
    "list_avg_syllables_word = []\n",
    "list_unique_words = []\n",
    "list_ttr = []\n",
    "list_huerta_score = []\n",
    "list_szigriszt_score = []\n",
    "list_mltd = []\n",
    "list_entity_ratio = []\n",
    "list_upper_case_ratio = []\n",
    "list_quotes = []\n",
    "list_quotes_ratio = []\n",
    "list_propn_ratio = [] \n",
    "list_noun_ratio = []\n",
    "list_adp_ratio = []\n",
    "list_det_ratio = []\n",
    "list_punct_ratio = []\n",
    "list_pron_ratio = []\n",
    "list_verb_ratio = []\n",
    "list_adv_ratio = []\n",
    "list_sym_ratio = []\n",
    "\n",
    "list_headline = []\n",
    "list_words_h = []\n",
    "list_word_size_h = []\n",
    "list_avg_syllables_word_h = []\n",
    "list_ttr_h = []\n",
    "list_mltd_h = []\n",
    "list_unique_words_h = []\n",
    "\n",
    "# df iteration\n",
    "for n, row in df.iterrows():\n",
    "    \n",
    "    ## headline ##\n",
    "    headline = df['Headline'].iloc[n]\n",
    "    headline = re.sub(r\"http\\S+\", \"\", headline)\n",
    "    headline = re.sub(r\"http\", \"\", headline)\n",
    "    headline = re.sub(r\"@\\S+\", \"\", headline)\n",
    "    headline = re.sub(\"\\n\", \" \", headline)\n",
    "    headline = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", headline)\n",
    "    headline = headline.replace(r\"*NUMBER*\", \"número\")\n",
    "    headline = headline.replace(r\"*PHONE*\", \"número\")\n",
    "    headline = headline.replace(r\"*EMAIL*\", \"email\")\n",
    "    headline = headline.replace(r\"*URL*\", \"url\")\n",
    "    headline_new = headline.lower()\n",
    "    doc_h = nlp(headline_new)\n",
    "    \n",
    "    list_tokens_h = []\n",
    "    list_tags_h = []\n",
    "\n",
    "    for sentence_h in doc_h.sents:\n",
    "        for token in sentence_h:\n",
    "            list_tokens_h.append(token.text)\n",
    "\n",
    "    fdist_h = FreqDist(list_tokens_h)\n",
    "    syllables_h = get_nsyllables(headline)\n",
    "    words_h = len(list_tokens_h)\n",
    "    \n",
    "    # headline complexity features\n",
    "    avg_word_size_h = round(sum(len(word) for word in list_tokens_h) / words_h, 2)\n",
    "    avg_syllables_word_h = round(syllables_h / words_h, 2)\n",
    "    unique_words_h = round((len(fdist_h.hapaxes()) / words_h) * 100, 2)\n",
    "    ttr_h = round(ld.ttr(list_tokens_h) * 100, 2)\n",
    "    mltd_h = round(ld.mtld(list_tokens_h), 2)\n",
    "    \n",
    "    ## text content##   \n",
    "    text = df['Text'].iloc[n]  \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"http\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "    \n",
    "    # to later calculate upper case letters ratio\n",
    "    alph = list(filter(str.isalpha, text))\n",
    "    text_new = text.lower()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    list_entities = []\n",
    "    sents = 0\n",
    "    \n",
    "    for entity in doc.ents:\n",
    "        list_entities.append(entity.label_)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "    \n",
    "    # Calculate entities, pos, tag, freq, syllables, words and quotes\n",
    "    entities = len(list_entities)\n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "    syllables = get_nsyllables(text)\n",
    "    words = len(list_tokens)\n",
    "    quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "\n",
    "    # complexity features\n",
    "    avg_word_sentence = round(words / sents, 2)\n",
    "    avg_word_size = round(sum(len(word) for word in list_tokens) / words, 2)\n",
    "    avg_syllables_word = round(syllables / words, 2)\n",
    "    unique_words = round((len(fdist.hapaxes()) / words) * 100, 2)\n",
    "    ttr = round(ld.ttr(list_tokens) * 100, 2)\n",
    "    mltd = round(ld.mtld(list_tokens), 2)\n",
    "\n",
    "    # readability spanish test\n",
    "    huerta_score = round(206.84 - (60 * avg_syllables_word) - (1.02 * avg_word_sentence), 2)\n",
    "    szigriszt_score = round(206.835 - ((62.3 * syllables) / words) - (words / sents), 2)\n",
    "\n",
    "    # stylometric features\n",
    "    upper_case_ratio = round(sum(map(str.isupper, alph)) / len(alph) * 100, 2)\n",
    "    entity_ratio = round((entities / words) * 100, 2)\n",
    "    quotes_ratio = round((quotes / words) * 100, 2)\n",
    "    propn_ratio = round((n_pos['PROPN'] / words) * 100 , 2)\n",
    "    noun_ratio = round((n_pos['NOUN'] / words) * 100, 2) \n",
    "    adp_ratio = round((n_pos['ADP'] / words) * 100, 2)\n",
    "    det_ratio = round((n_pos['DET'] / words) * 100, 2)\n",
    "    punct_ratio = round((n_pos['PUNCT'] / words) * 100, 2)\n",
    "    pron_ratio = round((n_pos['PRON'] / words) * 100, 2)\n",
    "    verb_ratio = round((n_pos['VERB'] / words) * 100, 2)\n",
    "    adv_ratio = round((n_pos['ADV'] / words) * 100, 2)\n",
    "    sym_ratio = round((n_tag['SYM'] / words) * 100, 2)\n",
    "    \n",
    "    # appending on lists\n",
    "    # headline\n",
    "    list_headline.append(headline_new)\n",
    "    list_words_h.append(words_h)\n",
    "    list_word_size_h.append(avg_word_size_h)\n",
    "    list_avg_syllables_word_h.append(avg_syllables_word_h)\n",
    "    list_unique_words_h.append(unique_words_h)\n",
    "    list_ttr_h.append(ttr_h)\n",
    "    list_mltd_h.append(mltd_h)\n",
    "    \n",
    "    # text\n",
    "    list_text.append(text_new)\n",
    "    list_sentences.append(sents)\n",
    "    list_words.append(words)\n",
    "    list_words_sent.append(avg_word_sentence)\n",
    "    list_word_size.append(avg_word_size)\n",
    "    list_avg_syllables_word.append(avg_syllables_word)\n",
    "    list_unique_words.append(unique_words)\n",
    "    list_ttr.append(ttr)\n",
    "    list_huerta_score.append(huerta_score)\n",
    "    list_szigriszt_score.append(szigriszt_score)\n",
    "    list_mltd.append(mltd)\n",
    "    list_entity_ratio.append(entity_ratio)\n",
    "    list_upper_case_ratio.append(upper_case_ratio)\n",
    "    list_quotes.append(quotes)\n",
    "    list_quotes_ratio.append(quotes_ratio)\n",
    "    list_propn_ratio.append(propn_ratio)\n",
    "    list_noun_ratio.append(noun_ratio)\n",
    "    list_adp_ratio.append(adp_ratio)\n",
    "    list_det_ratio.append(det_ratio)\n",
    "    list_punct_ratio.append(punct_ratio)\n",
    "    list_pron_ratio.append(pron_ratio)\n",
    "    list_verb_ratio.append(verb_ratio)\n",
    "    list_adv_ratio.append(adv_ratio)\n",
    "    list_sym_ratio.append(sym_ratio)\n",
    "    \n",
    "# dataframe\n",
    "df_features['topic'] = df['Topic']\n",
    "df_features['text'] = list_text\n",
    "df_features['headline'] = list_headline\n",
    "\n",
    "# headline\n",
    "df_features['words_h'] = list_words_h\n",
    "df_features['word_size_h'] = list_word_size_h\n",
    "df_features['avg_syllables_word_h'] = list_avg_syllables_word_h\n",
    "df_features['unique_words_h'] = list_unique_words_h\n",
    "df_features['ttr_h'] = list_ttr_h\n",
    "df_features['mltd_h'] = list_mltd_h\n",
    "\n",
    "# text\n",
    "df_features['sents'] = list_sentences\n",
    "df_features['words'] = list_words\n",
    "df_features['avg_words_sent'] = list_words_sent\n",
    "df_features['avg_word_size'] = list_word_size\n",
    "df_features['avg_syllables_word'] = list_avg_syllables_word\n",
    "df_features['unique_words'] = list_unique_words\n",
    "df_features['ttr'] = list_ttr\n",
    "df_features['mltd'] = list_mltd\n",
    "df_features['huerta_score'] = list_huerta_score\n",
    "df_features['szigriszt_score'] = list_szigriszt_score\n",
    "df_features['upper_case_ratio'] = list_upper_case_ratio\n",
    "df_features['entity_ratio'] = list_entity_ratio\n",
    "df_features['quotes'] = list_quotes\n",
    "df_features['quotes_ratio'] = list_quotes_ratio\n",
    "df_features['propn_ratio'] = list_propn_ratio\n",
    "df_features['noun_ratio'] = list_noun_ratio\n",
    "df_features['adp_ratio'] = list_adp_ratio\n",
    "df_features['det_ratio'] = list_det_ratio\n",
    "df_features['punct_ratio'] = list_punct_ratio\n",
    "df_features['pron_ratio'] = list_pron_ratio\n",
    "df_features['verb_ratio'] = list_verb_ratio\n",
    "df_features['adv_ratio'] = list_adv_ratio\n",
    "df_features['sym_ratio'] = list_sym_ratio\n",
    "\n",
    "df_features['label'] = df['Label']\n",
    "\n",
    "df_features.to_csv('../data/spanish_corpus_features_v6.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>headline</th>\n",
       "      <th>words_h</th>\n",
       "      <th>word_size_h</th>\n",
       "      <th>avg_syllables_word_h</th>\n",
       "      <th>unique_words_h</th>\n",
       "      <th>ttr_h</th>\n",
       "      <th>mltd_h</th>\n",
       "      <th>sents</th>\n",
       "      <th>words</th>\n",
       "      <th>avg_words_sent</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>avg_syllables_word</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>ttr</th>\n",
       "      <th>mltd</th>\n",
       "      <th>huerta_score</th>\n",
       "      <th>szigriszt_score</th>\n",
       "      <th>upper_case_ratio</th>\n",
       "      <th>entity_ratio</th>\n",
       "      <th>quotes</th>\n",
       "      <th>quotes_ratio</th>\n",
       "      <th>propn_ratio</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>adp_ratio</th>\n",
       "      <th>det_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>pron_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adv_ratio</th>\n",
       "      <th>sym_ratio</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>el pasado jueves 5 de noviembre la superintend...</td>\n",
       "      <td>nueva sanción a doña gallina por discriminar g...</td>\n",
       "      <td>9</td>\n",
       "      <td>5.89</td>\n",
       "      <td>2.22</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>351</td>\n",
       "      <td>39.00</td>\n",
       "      <td>4.41</td>\n",
       "      <td>1.79</td>\n",
       "      <td>42.45</td>\n",
       "      <td>54.42</td>\n",
       "      <td>76.65</td>\n",
       "      <td>59.66</td>\n",
       "      <td>56.01</td>\n",
       "      <td>2.13</td>\n",
       "      <td>3.13</td>\n",
       "      <td>11</td>\n",
       "      <td>3.13</td>\n",
       "      <td>6.27</td>\n",
       "      <td>17.95</td>\n",
       "      <td>14.53</td>\n",
       "      <td>13.68</td>\n",
       "      <td>9.69</td>\n",
       "      <td>3.99</td>\n",
       "      <td>6.27</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>la rae estudia incluir «machirulo» en el dicci...</td>\n",
       "      <td>la rae estudia incluir «machirulo» en el dicci...</td>\n",
       "      <td>10</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.80</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18</td>\n",
       "      <td>437</td>\n",
       "      <td>24.28</td>\n",
       "      <td>4.23</td>\n",
       "      <td>1.73</td>\n",
       "      <td>30.66</td>\n",
       "      <td>44.39</td>\n",
       "      <td>63.54</td>\n",
       "      <td>78.27</td>\n",
       "      <td>74.78</td>\n",
       "      <td>2.94</td>\n",
       "      <td>5.49</td>\n",
       "      <td>23</td>\n",
       "      <td>5.26</td>\n",
       "      <td>7.55</td>\n",
       "      <td>13.50</td>\n",
       "      <td>10.98</td>\n",
       "      <td>13.73</td>\n",
       "      <td>17.16</td>\n",
       "      <td>1.83</td>\n",
       "      <td>8.70</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>el alto comisionado de naciones unidas para lo...</td>\n",
       "      <td>save the children y acnur alertan de riesgos q...</td>\n",
       "      <td>16</td>\n",
       "      <td>4.62</td>\n",
       "      <td>1.81</td>\n",
       "      <td>87.50</td>\n",
       "      <td>93.75</td>\n",
       "      <td>71.68</td>\n",
       "      <td>27</td>\n",
       "      <td>1276</td>\n",
       "      <td>47.26</td>\n",
       "      <td>4.47</td>\n",
       "      <td>1.82</td>\n",
       "      <td>24.92</td>\n",
       "      <td>35.50</td>\n",
       "      <td>76.24</td>\n",
       "      <td>49.43</td>\n",
       "      <td>45.91</td>\n",
       "      <td>3.42</td>\n",
       "      <td>4.39</td>\n",
       "      <td>54</td>\n",
       "      <td>4.23</td>\n",
       "      <td>6.82</td>\n",
       "      <td>15.44</td>\n",
       "      <td>16.85</td>\n",
       "      <td>11.91</td>\n",
       "      <td>10.66</td>\n",
       "      <td>3.61</td>\n",
       "      <td>10.97</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>el colegio de abogados ha entregado en la maña...</td>\n",
       "      <td>colegio de abogados de granada entrega distinc...</td>\n",
       "      <td>13</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.23</td>\n",
       "      <td>76.92</td>\n",
       "      <td>84.62</td>\n",
       "      <td>23.66</td>\n",
       "      <td>10</td>\n",
       "      <td>587</td>\n",
       "      <td>58.70</td>\n",
       "      <td>4.39</td>\n",
       "      <td>1.85</td>\n",
       "      <td>28.79</td>\n",
       "      <td>40.89</td>\n",
       "      <td>55.69</td>\n",
       "      <td>35.97</td>\n",
       "      <td>32.87</td>\n",
       "      <td>4.42</td>\n",
       "      <td>8.01</td>\n",
       "      <td>16</td>\n",
       "      <td>2.73</td>\n",
       "      <td>16.35</td>\n",
       "      <td>15.67</td>\n",
       "      <td>17.55</td>\n",
       "      <td>13.29</td>\n",
       "      <td>10.39</td>\n",
       "      <td>2.21</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>era todo un misterio el paradero de la familia...</td>\n",
       "      <td>espera de tres años a instalación de internet ...</td>\n",
       "      <td>14</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.00</td>\n",
       "      <td>64.29</td>\n",
       "      <td>78.57</td>\n",
       "      <td>18.29</td>\n",
       "      <td>15</td>\n",
       "      <td>424</td>\n",
       "      <td>28.27</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.73</td>\n",
       "      <td>41.27</td>\n",
       "      <td>53.54</td>\n",
       "      <td>83.89</td>\n",
       "      <td>74.20</td>\n",
       "      <td>70.57</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.54</td>\n",
       "      <td>11</td>\n",
       "      <td>2.59</td>\n",
       "      <td>4.01</td>\n",
       "      <td>16.04</td>\n",
       "      <td>13.68</td>\n",
       "      <td>14.15</td>\n",
       "      <td>12.97</td>\n",
       "      <td>4.48</td>\n",
       "      <td>10.14</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3969</th>\n",
       "      <td>qué creen? acabo de descubrir que los administ...</td>\n",
       "      <td>twitter apoya a amlo</td>\n",
       "      <td>4</td>\n",
       "      <td>4.25</td>\n",
       "      <td>2.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>83</td>\n",
       "      <td>13.83</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.63</td>\n",
       "      <td>66.27</td>\n",
       "      <td>74.70</td>\n",
       "      <td>65.39</td>\n",
       "      <td>94.93</td>\n",
       "      <td>91.67</td>\n",
       "      <td>5.09</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.46</td>\n",
       "      <td>6.02</td>\n",
       "      <td>9.64</td>\n",
       "      <td>3.61</td>\n",
       "      <td>19.28</td>\n",
       "      <td>4.82</td>\n",
       "      <td>9.64</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>ver todos europa press nueva york.- el grupo c...</td>\n",
       "      <td>el grupo chino hna compra el 25% del gigante h...</td>\n",
       "      <td>15</td>\n",
       "      <td>5.33</td>\n",
       "      <td>1.93</td>\n",
       "      <td>86.67</td>\n",
       "      <td>93.33</td>\n",
       "      <td>63.00</td>\n",
       "      <td>15</td>\n",
       "      <td>572</td>\n",
       "      <td>38.13</td>\n",
       "      <td>4.69</td>\n",
       "      <td>1.81</td>\n",
       "      <td>33.57</td>\n",
       "      <td>46.15</td>\n",
       "      <td>70.47</td>\n",
       "      <td>59.35</td>\n",
       "      <td>55.86</td>\n",
       "      <td>5.11</td>\n",
       "      <td>9.44</td>\n",
       "      <td>3</td>\n",
       "      <td>0.52</td>\n",
       "      <td>15.21</td>\n",
       "      <td>19.58</td>\n",
       "      <td>17.48</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.44</td>\n",
       "      <td>1.22</td>\n",
       "      <td>6.47</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>un curandero infecta de sida a medio centenar ...</td>\n",
       "      <td>un curandero infecta de sida a medio centenar ...</td>\n",
       "      <td>12</td>\n",
       "      <td>4.67</td>\n",
       "      <td>2.00</td>\n",
       "      <td>83.33</td>\n",
       "      <td>91.67</td>\n",
       "      <td>40.32</td>\n",
       "      <td>10</td>\n",
       "      <td>367</td>\n",
       "      <td>36.70</td>\n",
       "      <td>4.46</td>\n",
       "      <td>1.85</td>\n",
       "      <td>36.78</td>\n",
       "      <td>49.86</td>\n",
       "      <td>67.32</td>\n",
       "      <td>58.41</td>\n",
       "      <td>54.87</td>\n",
       "      <td>4.45</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6</td>\n",
       "      <td>1.63</td>\n",
       "      <td>11.72</td>\n",
       "      <td>21.53</td>\n",
       "      <td>20.71</td>\n",
       "      <td>14.17</td>\n",
       "      <td>10.08</td>\n",
       "      <td>0.82</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3972</th>\n",
       "      <td>política primeras páginas de los diarios llega...</td>\n",
       "      <td>primeras páginas de los diarios llegados esta ...</td>\n",
       "      <td>11</td>\n",
       "      <td>5.55</td>\n",
       "      <td>2.09</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>591</td>\n",
       "      <td>49.25</td>\n",
       "      <td>3.97</td>\n",
       "      <td>1.54</td>\n",
       "      <td>23.69</td>\n",
       "      <td>38.07</td>\n",
       "      <td>70.94</td>\n",
       "      <td>64.20</td>\n",
       "      <td>61.55</td>\n",
       "      <td>9.02</td>\n",
       "      <td>10.66</td>\n",
       "      <td>62</td>\n",
       "      <td>10.49</td>\n",
       "      <td>11.68</td>\n",
       "      <td>15.23</td>\n",
       "      <td>13.71</td>\n",
       "      <td>15.74</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.02</td>\n",
       "      <td>8.12</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>confirmado: delfina gómez será expulsada de mo...</td>\n",
       "      <td>confirmado: delfina gómez será expulsada de mo...</td>\n",
       "      <td>14</td>\n",
       "      <td>5.29</td>\n",
       "      <td>2.21</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>440</td>\n",
       "      <td>44.00</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.76</td>\n",
       "      <td>36.82</td>\n",
       "      <td>48.86</td>\n",
       "      <td>69.60</td>\n",
       "      <td>56.36</td>\n",
       "      <td>53.10</td>\n",
       "      <td>7.09</td>\n",
       "      <td>5.45</td>\n",
       "      <td>8</td>\n",
       "      <td>1.82</td>\n",
       "      <td>5.68</td>\n",
       "      <td>13.86</td>\n",
       "      <td>13.41</td>\n",
       "      <td>12.27</td>\n",
       "      <td>9.55</td>\n",
       "      <td>3.86</td>\n",
       "      <td>12.95</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3974 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     el pasado jueves 5 de noviembre la superintend...   \n",
       "1     la rae estudia incluir «machirulo» en el dicci...   \n",
       "2     el alto comisionado de naciones unidas para lo...   \n",
       "3     el colegio de abogados ha entregado en la maña...   \n",
       "4     era todo un misterio el paradero de la familia...   \n",
       "...                                                 ...   \n",
       "3969  qué creen? acabo de descubrir que los administ...   \n",
       "3970  ver todos europa press nueva york.- el grupo c...   \n",
       "3971  un curandero infecta de sida a medio centenar ...   \n",
       "3972  política primeras páginas de los diarios llega...   \n",
       "3973  confirmado: delfina gómez será expulsada de mo...   \n",
       "\n",
       "                                               headline  words_h  word_size_h  \\\n",
       "0     nueva sanción a doña gallina por discriminar g...        9         5.89   \n",
       "1     la rae estudia incluir «machirulo» en el dicci...       10         4.50   \n",
       "2     save the children y acnur alertan de riesgos q...       16         4.62   \n",
       "3     colegio de abogados de granada entrega distinc...       13         5.00   \n",
       "4     espera de tres años a instalación de internet ...       14         4.43   \n",
       "...                                                 ...      ...          ...   \n",
       "3969                               twitter apoya a amlo        4         4.25   \n",
       "3970  el grupo chino hna compra el 25% del gigante h...       15         5.33   \n",
       "3971  un curandero infecta de sida a medio centenar ...       12         4.67   \n",
       "3972  primeras páginas de los diarios llegados esta ...       11         5.55   \n",
       "3973  confirmado: delfina gómez será expulsada de mo...       14         5.29   \n",
       "\n",
       "      avg_syllables_word_h  unique_words_h   ttr_h  mltd_h  sents  words  \\\n",
       "0                     2.22          100.00  100.00    0.00      9    351   \n",
       "1                     1.80          100.00  100.00    0.00     18    437   \n",
       "2                     1.81           87.50   93.75   71.68     27   1276   \n",
       "3                     2.23           76.92   84.62   23.66     10    587   \n",
       "4                     2.00           64.29   78.57   18.29     15    424   \n",
       "...                    ...             ...     ...     ...    ...    ...   \n",
       "3969                  2.00          100.00  100.00    0.00      6     83   \n",
       "3970                  1.93           86.67   93.33   63.00     15    572   \n",
       "3971                  2.00           83.33   91.67   40.32     10    367   \n",
       "3972                  2.09          100.00  100.00    0.00     12    591   \n",
       "3973                  2.21          100.00  100.00    0.00     10    440   \n",
       "\n",
       "      avg_words_sent  avg_word_size  avg_syllables_word  unique_words    ttr  \\\n",
       "0              39.00           4.41                1.79         42.45  54.42   \n",
       "1              24.28           4.23                1.73         30.66  44.39   \n",
       "2              47.26           4.47                1.82         24.92  35.50   \n",
       "3              58.70           4.39                1.85         28.79  40.89   \n",
       "4              28.27           4.16                1.73         41.27  53.54   \n",
       "...              ...            ...                 ...           ...    ...   \n",
       "3969           13.83           4.27                1.63         66.27  74.70   \n",
       "3970           38.13           4.69                1.81         33.57  46.15   \n",
       "3971           36.70           4.46                1.85         36.78  49.86   \n",
       "3972           49.25           3.97                1.54         23.69  38.07   \n",
       "3973           44.00           4.33                1.76         36.82  48.86   \n",
       "\n",
       "       mltd  huerta_score  szigriszt_score  upper_case_ratio  entity_ratio  \\\n",
       "0     76.65         59.66            56.01              2.13          3.13   \n",
       "1     63.54         78.27            74.78              2.94          5.49   \n",
       "2     76.24         49.43            45.91              3.42          4.39   \n",
       "3     55.69         35.97            32.87              4.42          8.01   \n",
       "4     83.89         74.20            70.57              1.76          3.54   \n",
       "...     ...           ...              ...               ...           ...   \n",
       "3969  65.39         94.93            91.67              5.09          8.43   \n",
       "3970  70.47         59.35            55.86              5.11          9.44   \n",
       "3971  67.32         58.41            54.87              4.45          6.81   \n",
       "3972  70.94         64.20            61.55              9.02         10.66   \n",
       "3973  69.60         56.36            53.10              7.09          5.45   \n",
       "\n",
       "      quotes  quotes_ratio  propn_ratio  noun_ratio  adp_ratio  det_ratio  \\\n",
       "0         11          3.13         6.27       17.95      14.53      13.68   \n",
       "1         23          5.26         7.55       13.50      10.98      13.73   \n",
       "2         54          4.23         6.82       15.44      16.85      11.91   \n",
       "3         16          2.73        16.35       15.67      17.55      13.29   \n",
       "4         11          2.59         4.01       16.04      13.68      14.15   \n",
       "...      ...           ...          ...         ...        ...        ...   \n",
       "3969       0          0.00        14.46        6.02       9.64       3.61   \n",
       "3970       3          0.52        15.21       19.58      17.48      10.84   \n",
       "3971       6          1.63        11.72       21.53      20.71      14.17   \n",
       "3972      62         10.49        11.68       15.23      13.71      15.74   \n",
       "3973       8          1.82         5.68       13.86      13.41      12.27   \n",
       "\n",
       "      punct_ratio  pron_ratio  verb_ratio  adv_ratio  sym_ratio  label  \n",
       "0            9.69        3.99        6.27       2.56       0.00      0  \n",
       "1           17.16        1.83        8.70       3.66       0.00      1  \n",
       "2           10.66        3.61       10.97       2.66       0.47      1  \n",
       "3           10.39        2.21        7.67       0.85       0.00      1  \n",
       "4           12.97        4.48       10.14       4.72       0.24      0  \n",
       "...           ...         ...         ...        ...        ...    ...  \n",
       "3969        19.28        4.82        9.64       3.61       0.00      0  \n",
       "3970         9.44        1.22        6.47       1.75       0.00      1  \n",
       "3971        10.08        0.82        6.81       1.91       0.00      1  \n",
       "3972        22.00        1.02        8.12       1.86       2.03      1  \n",
       "3973         9.55        3.86       12.95       5.68       0.00      0  \n",
       "\n",
       "[3974 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature extraction function for predictions\n",
    "\n",
    "To make predictions we need to extracte features from a given news headline and news text content. So we are going to pack the code above to extract the features for our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 24.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "def get_news_features(headline, text):\n",
    "    \n",
    "    nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "    ## headline ##\n",
    "    headline = re.sub(r\"http\\S+\", \"\", headline)\n",
    "    headline = re.sub(r\"http\", \"\", headline)\n",
    "    headline = re.sub(r\"@\\S+\", \"\", headline)\n",
    "    headline = re.sub(\"\\n\", \" \", headline)\n",
    "    headline = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", headline)\n",
    "    headline = headline.replace(r\"*NUMBER*\", \"número\")\n",
    "    headline = headline.replace(r\"*PHONE*\", \"número\")\n",
    "    headline = headline.replace(r\"*EMAIL*\", \"email\")\n",
    "    headline = headline.replace(r\"*URL*\", \"url\")\n",
    "    headline_new = headline.lower()\n",
    "    doc_h = nlp(headline_new)\n",
    "\n",
    "    list_tokens_h = []\n",
    "    list_tags_h = []\n",
    "\n",
    "    for sentence_h in doc_h.sents:\n",
    "        for token in sentence_h:\n",
    "            list_tokens_h.append(token.text)\n",
    "\n",
    "    fdist_h = FreqDist(list_tokens_h)\n",
    "    syllables_h = get_nsyllables(headline)\n",
    "    words_h = len(list_tokens_h)\n",
    "\n",
    "    # headline complexity features\n",
    "    avg_word_size_h = round(sum(len(word) for word in list_tokens_h) / words_h, 2)\n",
    "    avg_syllables_word_h = round(syllables_h / words_h, 2)\n",
    "    unique_words_h = round((len(fdist_h.hapaxes()) / words_h) * 100, 2)\n",
    "    mltd_h = round(ld.mtld(list_tokens_h), 2)\n",
    "\n",
    "    ## text content##     \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"http\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "\n",
    "    # to later calculate upper case letters ratio\n",
    "    alph = list(filter(str.isalpha, text))\n",
    "    text_new = text.lower()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    list_entities = []\n",
    "    sents = 0\n",
    "\n",
    "    for entity in doc.ents:\n",
    "        list_entities.append(entity.label_)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "\n",
    "    # Calculate entities, pos, tag, freq, syllables, words and quotes\n",
    "    entities = len(list_entities)\n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "    syllables = get_nsyllables(text)\n",
    "    words = len(list_tokens)\n",
    "    quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "\n",
    "    # complexity features\n",
    "    avg_word_sentence = round(words / sents, 2)\n",
    "    avg_word_size = round(sum(len(word) for word in list_tokens) / words, 2)\n",
    "    avg_syllables_word = round(syllables / words, 2)\n",
    "    unique_words = round((len(fdist.hapaxes()) / words) * 100, 2)\n",
    "    ttr = round(ld.ttr(list_tokens) * 100, 2)\n",
    "\n",
    "    # readability spanish test\n",
    "    huerta_score = round(206.84 - (60 * avg_syllables_word) - (1.02 * avg_word_sentence), 2)\n",
    "    szigriszt_score = round(206.835 - ((62.3 * syllables) / words) - (words / sents), 2)\n",
    "\n",
    "    # stylometric features\n",
    "    mltd = round(ld.mtld(list_tokens), 2)\n",
    "    upper_case_ratio = round(sum(map(str.isupper, alph)) / len(alph) * 100, 2)\n",
    "    entity_ratio = round((entities / words) * 100, 2)\n",
    "    quotes_ratio = round((quotes / words) * 100, 2)\n",
    "    propn_ratio = round((n_pos['PROPN'] / words) * 100 , 2)\n",
    "    noun_ratio = round((n_pos['NOUN'] / words) * 100, 2) \n",
    "    pron_ratio = round((n_pos['PRON'] / words) * 100, 2)\n",
    "    adp_ratio = round((n_pos['ADP'] / words) * 100, 2)\n",
    "    det_ratio = round((n_pos['DET'] / words) * 100, 2)\n",
    "    punct_ratio = round((n_pos['PUNCT'] / words) * 100, 2)\n",
    "    verb_ratio = round((n_pos['VERB'] / words) * 100, 2)\n",
    "    adv_ratio = round((n_pos['ADV'] / words) * 100, 2)\n",
    "    sym_ratio = round((n_tag['SYM'] / words) * 100, 2)\n",
    "\n",
    "    # create df_features\n",
    "    df_features = pd.DataFrame({'words_h': [words_h], 'avg_word_size_h': [avg_word_size_h],'avg_syllables_word': [avg_syllables_word_h],\n",
    "                                'unique_words_h': [unique_words_h], 'mltd_h': [mltd_h], 'sents': [sents], 'words': [words], \n",
    "                                'avg_word_sentence': [avg_word_sentence], 'avg_word_size': [avg_word_size], \n",
    "                                'avg_syllables_word': avg_syllables_word, 'unique_words': [unique_words], \n",
    "                                'ttr': [ttr], 'huerta_score': [huerta_score], 'szigriszt_score': [szigriszt_score],\n",
    "                                'mltd': [mltd], 'upper_case_ratio': [upper_case_ratio], 'entity_ratio': [entity_ratio], \n",
    "                                'quotes': [quotes], 'quotes_ratio': [quotes_ratio], 'propn_ratio': [propn_ratio], \n",
    "                                'noun_ratio': [noun_ratio], 'pron_ratio': [pron_ratio], 'adp_ratio': [adp_ratio],\n",
    "                                'det_ratio': [det_ratio], 'punct_ratio': [punct_ratio], 'verb_ratio': [verb_ratio],\n",
    "                                'adv_ratio': [adv_ratio], 'sym_ratio': [sym_ratio]})\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert news headline:El Gobierno ha presentado hoy al Niño de Schrödinger, que va y no va al colegio\n",
      "insert news content:La ministra de Educación y Formación Profesional, Isabel Celaá, ha presentado esta mañana al Niño de Schrödinger, fruto de un proyecto en el que han colaborado varias universidades españolas y que viene a resolver el problema de la vuelta a los colegios en plena ola de contagios por coronavirus.  «Va y no va al colegio y está expuesto al virus pero al mismo tiempo no lo está», ha explicado Celaá, insistiendo en que se trata de «una paradoja avalada científicamente».  La ministra ha mostrado a los medios al niño, cuyo nombre es Fernando Campos Leza, describiéndolo como «un alumno perfectamente sano y normal que ahora mismo, estando aquí con nosotros, está al mismo tiempo en casa, donde permanecerá mientras vaya al colegio con normalidad junto al resto de niños de Schrödinger».  A partir de mañana y hasta el inicio del nuevo curso escolar, los padres deberán adaptar a sus hijos para que sean también niños de Schrödinger. Para ello, se les facilitará y no se les facilitará una caja.  Esta misma tarde, la ministra de Trabajo, Yolanda Díaz, presentará a los padres de Schrödinger, capaces de compatibilizar sus obligaciones profesionales con las familiares yendo a la oficina y al mismo tiempo quedándose en su domicilio particular para mayor seguridad, atendiendo y no atendiendo a sus niños de Schrödinger mientras permanecen en casa y en clase.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words_h</th>\n",
       "      <th>avg_word_size_h</th>\n",
       "      <th>avg_syllables_word</th>\n",
       "      <th>unique_words_h</th>\n",
       "      <th>mltd_h</th>\n",
       "      <th>sents</th>\n",
       "      <th>words</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>ttr</th>\n",
       "      <th>huerta_score</th>\n",
       "      <th>szigriszt_score</th>\n",
       "      <th>mltd</th>\n",
       "      <th>upper_case_ratio</th>\n",
       "      <th>entity_ratio</th>\n",
       "      <th>quotes</th>\n",
       "      <th>quotes_ratio</th>\n",
       "      <th>propn_ratio</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>pron_ratio</th>\n",
       "      <th>adp_ratio</th>\n",
       "      <th>det_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adv_ratio</th>\n",
       "      <th>sym_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>258</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.47</td>\n",
       "      <td>35.66</td>\n",
       "      <td>65.99</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.76</td>\n",
       "      <td>76.47</td>\n",
       "      <td>88.24</td>\n",
       "      <td>101.3</td>\n",
       "      <td>98.22</td>\n",
       "      <td>40.46</td>\n",
       "      <td>6.35</td>\n",
       "      <td>11.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.65</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.65</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.88</td>\n",
       "      <td>17.65</td>\n",
       "      <td>11.76</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   words_h  avg_word_size_h  avg_syllables_word  unique_words_h  mltd_h  \\\n",
       "0      258              4.4                1.47           35.66   65.99   \n",
       "\n",
       "   sents  words  avg_word_sentence  avg_word_size  unique_words    ttr  \\\n",
       "0      1     17               17.0           3.76         76.47  88.24   \n",
       "\n",
       "   huerta_score  szigriszt_score   mltd  upper_case_ratio  entity_ratio  \\\n",
       "0         101.3            98.22  40.46              6.35         11.76   \n",
       "\n",
       "   quotes  quotes_ratio  propn_ratio  noun_ratio  pron_ratio  adp_ratio  \\\n",
       "0       0           0.0        17.65        5.88         0.0      17.65   \n",
       "\n",
       "   det_ratio  punct_ratio  verb_ratio  adv_ratio  sym_ratio  \n",
       "0       5.88         5.88       17.65      11.76        0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline = input('Insert news headline:')\n",
    "text = input('insert news content:')\n",
    "\n",
    "get_news_features(text, headline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
