{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and feature extraction v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text, extract stylometric, lexical and complexity features and create a new csv\n",
    "\n",
    "## We are using `spacy`: The NLP *Ruby on Rails* \n",
    "\n",
    "[spacy](http://www.spacy.io/) is a library of natural language processing, robust, fast, easy to install and to use. It can be used with other NLP and Deep Learning Libraries.\n",
    "\n",
    "With its pre-trained models in spanish language, we can operate the typical NLP jobs: Sentences segmentation, tokenization, POS tag, etc...\n",
    "\n",
    "We are going to use the `es_core_news_lg` pre-trained model to make pos tagging:\n",
    "\n",
    "## Also extracting headline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/corpus_spanish.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>641</td>\n",
       "      <td>True</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Caras</td>\n",
       "      <td>Sofía Castro y Alejandro Peña Pretelini: una i...</td>\n",
       "      <td>Sofía Castro y Alejandro Peña Pretelini: una i...</td>\n",
       "      <td>https://www.caras.com.mx/sofia-castro-alejandr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>Education</td>\n",
       "      <td>Heraldo</td>\n",
       "      <td>Un paso más cerca de hacer los exámenes 'online'</td>\n",
       "      <td>Un paso más cerca de hacer los exámenes 'onlin...</td>\n",
       "      <td>https://www.heraldo.es/noticias/suplementos/he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141</td>\n",
       "      <td>True</td>\n",
       "      <td>Science</td>\n",
       "      <td>HUFFPOST</td>\n",
       "      <td>Esto es lo que los científicos realmente piens...</td>\n",
       "      <td>Esto es lo que los científicos realmente piens...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/scientist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>394</td>\n",
       "      <td>True</td>\n",
       "      <td>Politics</td>\n",
       "      <td>El financiero</td>\n",
       "      <td>Inicia impresión de boletas para elección pres...</td>\n",
       "      <td>Inicia impresión de boletas para elección pres...</td>\n",
       "      <td>http://www.elfinanciero.com.mx/elecciones-2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139</td>\n",
       "      <td>True</td>\n",
       "      <td>Sport</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>A *NUMBER* día del Mundial</td>\n",
       "      <td>A *NUMBER* día del Mundial\\nFIFA.com sigue la ...</td>\n",
       "      <td>https://es.fifa.com/worldcup/news/a-1-dia-del-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id Category          Topic         Source  \\\n",
       "0  641     True  Entertainment          Caras   \n",
       "1    6     True      Education        Heraldo   \n",
       "2  141     True        Science       HUFFPOST   \n",
       "3  394     True       Politics  El financiero   \n",
       "4  139     True          Sport           FIFA   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Sofía Castro y Alejandro Peña Pretelini: una i...   \n",
       "1   Un paso más cerca de hacer los exámenes 'online'   \n",
       "2  Esto es lo que los científicos realmente piens...   \n",
       "3  Inicia impresión de boletas para elección pres...   \n",
       "4                         A *NUMBER* día del Mundial   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Sofía Castro y Alejandro Peña Pretelini: una i...   \n",
       "1  Un paso más cerca de hacer los exámenes 'onlin...   \n",
       "2  Esto es lo que los científicos realmente piens...   \n",
       "3  Inicia impresión de boletas para elección pres...   \n",
       "4  A *NUMBER* día del Mundial\\nFIFA.com sigue la ...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.caras.com.mx/sofia-castro-alejandr...  \n",
       "1  https://www.heraldo.es/noticias/suplementos/he...  \n",
       "2  https://www.huffingtonpost.com/entry/scientist...  \n",
       "3  http://www.elfinanciero.com.mx/elecciones-2018...  \n",
       "4  https://es.fifa.com/worldcup/news/a-1-dia-del-...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(971, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id           int64\n",
       "Category    object\n",
       "Topic       object\n",
       "Source      object\n",
       "Headline    object\n",
       "Text        object\n",
       "Link        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 971 entries, 0 to 970\n",
      "Data columns (total 7 columns):\n",
      "Id          971 non-null int64\n",
      "Category    971 non-null object\n",
      "Topic       971 non-null object\n",
      "Source      971 non-null object\n",
      "Headline    971 non-null object\n",
      "Text        971 non-null object\n",
      "Link        971 non-null object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 53.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 8 31.5 4.190476190476191 34.92063492063492 50.0 58.60055865921788 15.079365079365079 15.079365079365079 13.88888888888889 11.507936507936508 9.523809523809524 5.555555555555555 6.349206349206349 3.1746031746031744\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk import word_tokenize, sent_tokenize  \n",
    "from string import punctuation\n",
    "\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "text = text.replace(r\"http\\S+\", \"\")\n",
    "text = text.replace(r\"http\", \"\")\n",
    "text = text.replace(r\"@\\S+\", \"\")\n",
    "text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "text = text.lower()\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "list_tokens = []\n",
    "list_tags = []\n",
    "n_sents = 0\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    n_sents += 1\n",
    "    for token in sentence:\n",
    "        list_tokens.append(token.text)\n",
    "        list_tags.append(token.pos_)\n",
    "\n",
    "n_tags = nltk.Counter(list_tags)\n",
    "fdist = FreqDist(list_tokens)\n",
    "        \n",
    "# complexity features\n",
    "n_words = len(list_tokens)\n",
    "avg_word_sentences = (float(n_words) / n_sents)\n",
    "word_size = sum(len(word) for word in list_tokens) / n_words\n",
    "unique_words = (len(fdist.hapaxes()) / n_words) * 100\n",
    "ttr = ld.ttr(list_tokens) * 100\n",
    "mltd = ld.mtld(list_tokens)\n",
    "\n",
    "# lexical features\n",
    "propn_ratio = (n_tags['PROPN'] / n_words) * 100 \n",
    "noun_ratio = (n_tags['NOUN'] / n_words) * 100 \n",
    "adp_ratio = (n_tags['ADP'] / n_words) * 100\n",
    "det_ratio = (n_tags['DET'] / n_words) * 100\n",
    "punct_ratio = (n_tags['PUNCT'] / n_words) * 100 \n",
    "pron_ratio = (n_tags['PRON'] / n_words) * 100\n",
    "verb_ratio = (n_tags['VERB'] / n_words) * 100\n",
    "adv_ratio = (n_tags['ADV'] / n_words) * 100\n",
    "\n",
    "print(n_words, n_sents, avg_word_sentences, word_size, unique_words, ttr, mltd, propn_ratio, noun_ratio, adp_ratio, det_ratio, punct_ratio, \n",
    "      pron_ratio, verb_ratio, adv_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply it to the full corpus with iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 4.95 s, total: 1min 43s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import FreqDist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lexical_diversity import lex_div as ld\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "df = pd.read_csv('../data/corpus_spanish.csv')\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df['Label'] = labelencoder.fit_transform(df['Category'])\n",
    "\n",
    "# empty lists and df\n",
    "df_features = pd.DataFrame()\n",
    "list_text = []\n",
    "list_nsentences = []\n",
    "list_nwords = []\n",
    "list_words_sent = []\n",
    "list_word_size = []\n",
    "list_unique_words = []\n",
    "list_ttr = []\n",
    "list_mltd = []\n",
    "list_nquotes = []\n",
    "list_quotes_ratio = []\n",
    "list_propn_ratio = [] \n",
    "list_noun_ratio = []\n",
    "list_adp_ratio = []\n",
    "list_det_ratio = []\n",
    "list_punct_ratio = []\n",
    "list_pron_ratio = []\n",
    "list_verb_ratio = []\n",
    "list_adv_ratio = []\n",
    "list_sym_ratio = []\n",
    "\n",
    "list_headline = []\n",
    "list_words_h = []\n",
    "list_word_size_h = []\n",
    "list_ttr_h = []\n",
    "list_mltd_h = []\n",
    "list_unique_words_h = []\n",
    "\n",
    "# df iteration\n",
    "for n, row in df.iterrows():\n",
    "    \n",
    "    ## headline ##\n",
    "    text_h = df['Headline'].iloc[n]\n",
    "    text_h = text_h.replace(r\"http\\S+\", \"\")\n",
    "    text_h = text_h.replace(r\"http\", \"\")\n",
    "    text_h = text_h.replace(r\"@\\S+\", \"\")\n",
    "    text_h = text_h.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "    text_h = text_h.lower()\n",
    "    doc_h = nlp(text_h)\n",
    "\n",
    "    list_tokens_h = []\n",
    "    list_tags_h = []\n",
    "    n_sents_h = 0\n",
    "\n",
    "    for sentence_h in doc_h.sents:\n",
    "        n_sents_h += 1\n",
    "        for token in sentence_h:\n",
    "            list_tokens_h.append(token.text)\n",
    "\n",
    "    fdist_h = FreqDist(list_tokens_h)\n",
    "    \n",
    "    # headline complexity features\n",
    "    n_words_h = len(list_tokens_h)\n",
    "    word_size_h = sum(len(word) for word in list_tokens_h) / n_words_h\n",
    "    unique_words_h = (len(fdist_h.hapaxes()) / n_words_h) * 100\n",
    "    ttr_h = ld.ttr(list_tokens_h) * 100\n",
    "    mltd_h = ld.mtld(list_tokens_h)\n",
    "    \n",
    "    ## text content##   \n",
    "    text = df['Text'].iloc[n]  \n",
    "    text = text.replace(r\"http\\S+\", \"\")\n",
    "    text = text.replace(r\"http\", \"\")\n",
    "    text = text.replace(r\"@\\S+\", \"\")\n",
    "    text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    n_sents = 0\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        n_sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "            \n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "\n",
    "    # complexity features\n",
    "    n_words = len(list_tokens)\n",
    "    avg_word_sentences = (float(n_words) / n_sents)\n",
    "    word_size = sum(len(word) for word in list_tokens) / n_words\n",
    "    unique_words = (len(fdist.hapaxes()) / n_words) * 100\n",
    "    ttr = ld.ttr(list_tokens) * 100\n",
    "    mltd = ld.mtld(list_tokens)\n",
    "\n",
    "    # lexical features\n",
    "    n_quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "    quotes_ratio = (n_quotes / n_words) * 100\n",
    "    propn_ratio = (n_pos['PROPN'] / n_words) * 100 \n",
    "    noun_ratio = (n_pos['NOUN'] / n_words) * 100 \n",
    "    adp_ratio = (n_pos['ADP'] / n_words) * 100\n",
    "    det_ratio = (n_pos['DET'] / n_words) * 100\n",
    "    punct_ratio = (n_pos['PUNCT'] / n_words) * 100 \n",
    "    pron_ratio = (n_pos['PRON'] / n_words) * 100\n",
    "    verb_ratio = (n_pos['VERB'] / n_words) * 100\n",
    "    adv_ratio = (n_pos['ADV'] / n_words) * 100\n",
    "    sym_ratio = (n_tag['SYM'] / n_words) * 100\n",
    "    \n",
    "    # appending on lists\n",
    "    list_text.append(text)\n",
    "    list_nsentences.append(n_sents)\n",
    "    list_nwords.append(n_words)\n",
    "    list_words_sent.append(avg_word_sentences)\n",
    "    list_word_size.append(word_size)\n",
    "    list_unique_words.append(unique_words)\n",
    "    list_ttr.append(ttr)\n",
    "    list_mltd.append(mltd)\n",
    "    list_headline.append(text_h)\n",
    "    list_words_h.append(n_words_h)\n",
    "    list_word_size_h.append(word_size_h)\n",
    "    list_unique_words_h.append(unique_words_h)\n",
    "    list_ttr_h.append(ttr_h)\n",
    "    list_mltd_h.append(mltd_h)\n",
    "    list_nquotes.append(n_quotes)\n",
    "    list_quotes_ratio.append(quotes_ratio)\n",
    "    list_propn_ratio.append(propn_ratio)\n",
    "    list_noun_ratio.append(noun_ratio)\n",
    "    list_adp_ratio.append(adp_ratio)\n",
    "    list_det_ratio.append(det_ratio)\n",
    "    list_punct_ratio.append(punct_ratio)\n",
    "    list_pron_ratio.append(pron_ratio)\n",
    "    list_verb_ratio.append(verb_ratio)\n",
    "    list_adv_ratio.append(adv_ratio)\n",
    "    list_sym_ratio.append(sym_ratio)\n",
    "    \n",
    "# dataframe\n",
    "df_features['text'] = list_text\n",
    "df_features['headline'] = list_headline\n",
    "df_features['n_sents'] = list_nsentences\n",
    "df_features['n_words'] = list_nwords\n",
    "df_features['avg_words_sents'] = list_words_sent\n",
    "df_features['word_size'] = list_word_size\n",
    "df_features['unique_words'] = list_unique_words\n",
    "df_features['ttr'] = list_ttr\n",
    "df_features['mltd'] = list_mltd\n",
    "df_features['n_words_h'] = list_words_h\n",
    "df_features['word_size_h'] = list_word_size_h\n",
    "df_features['unique_words_h'] = list_unique_words_h\n",
    "df_features['ttr_h'] = list_ttr_h\n",
    "df_features['mltd_h'] = list_mltd_h\n",
    "df_features['n_quotes'] = list_nquotes\n",
    "df_features['quotes_ratio'] = list_quotes_ratio\n",
    "df_features['propn_ratio'] = list_propn_ratio\n",
    "df_features['noun_ratio'] = list_noun_ratio\n",
    "df_features['adp_ratio'] = list_adp_ratio\n",
    "df_features['det_ratio'] = list_det_ratio\n",
    "df_features['punct_ratio'] = list_punct_ratio\n",
    "df_features['pron_ratio'] = list_pron_ratio\n",
    "df_features['verb_ratio'] = list_verb_ratio\n",
    "df_features['adv_ratio'] = list_adv_ratio\n",
    "df_features['sym_ratio'] = list_sym_ratio\n",
    "df_features['label'] = df['Label']\n",
    "\n",
    "df_features.to_csv('../data/spanish_corpus_features_v4.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>headline</th>\n",
       "      <th>n_sents</th>\n",
       "      <th>n_words</th>\n",
       "      <th>avg_words_sents</th>\n",
       "      <th>word_size</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>ttr</th>\n",
       "      <th>mltd</th>\n",
       "      <th>n_words_h</th>\n",
       "      <th>...</th>\n",
       "      <th>propn_ratio</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>adp_ratio</th>\n",
       "      <th>det_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>pron_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adv_ratio</th>\n",
       "      <th>sym_ratio</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sofía castro y alejandro peña pretelini: una i...</td>\n",
       "      <td>sofía castro y alejandro peña pretelini: una i...</td>\n",
       "      <td>8</td>\n",
       "      <td>252</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>4.190476</td>\n",
       "      <td>34.920635</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>58.600559</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>15.079365</td>\n",
       "      <td>15.079365</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>11.507937</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>6.349206</td>\n",
       "      <td>3.174603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>un paso más cerca de hacer los exámenes 'onlin...</td>\n",
       "      <td>un paso más cerca de hacer los exámenes 'online'</td>\n",
       "      <td>9</td>\n",
       "      <td>486</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>4.255144</td>\n",
       "      <td>32.716049</td>\n",
       "      <td>44.238683</td>\n",
       "      <td>41.283136</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>15.226337</td>\n",
       "      <td>12.345679</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>18.930041</td>\n",
       "      <td>1.234568</td>\n",
       "      <td>3.292181</td>\n",
       "      <td>1.646091</td>\n",
       "      <td>1.851852</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esto es lo que los científicos realmente piens...</td>\n",
       "      <td>esto es lo que los científicos realmente piens...</td>\n",
       "      <td>31</td>\n",
       "      <td>980</td>\n",
       "      <td>31.612903</td>\n",
       "      <td>4.815306</td>\n",
       "      <td>26.020408</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>80.551467</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>4.795918</td>\n",
       "      <td>17.857143</td>\n",
       "      <td>13.979592</td>\n",
       "      <td>12.755102</td>\n",
       "      <td>11.836735</td>\n",
       "      <td>2.551020</td>\n",
       "      <td>10.306122</td>\n",
       "      <td>4.693878</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inicia impresión de boletas para elección pres...</td>\n",
       "      <td>inicia impresión de boletas para elección pres...</td>\n",
       "      <td>11</td>\n",
       "      <td>369</td>\n",
       "      <td>33.545455</td>\n",
       "      <td>4.728997</td>\n",
       "      <td>22.764228</td>\n",
       "      <td>37.398374</td>\n",
       "      <td>50.995314</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>4.065041</td>\n",
       "      <td>22.493225</td>\n",
       "      <td>18.157182</td>\n",
       "      <td>15.176152</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>2.710027</td>\n",
       "      <td>7.317073</td>\n",
       "      <td>0.271003</td>\n",
       "      <td>1.355014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a *number* día del mundial\\nfifa.com sigue la ...</td>\n",
       "      <td>a *number* día del mundial</td>\n",
       "      <td>5</td>\n",
       "      <td>130</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>4.461538</td>\n",
       "      <td>48.461538</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>47.081602</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>14.615385</td>\n",
       "      <td>14.615385</td>\n",
       "      <td>17.692308</td>\n",
       "      <td>13.846154</td>\n",
       "      <td>9.230769</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>2.307692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interpol ordena detención inmediata de osorio ...</td>\n",
       "      <td>interpol ordena detención inmediata de osorio ...</td>\n",
       "      <td>4</td>\n",
       "      <td>116</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>4.793103</td>\n",
       "      <td>48.275862</td>\n",
       "      <td>63.793103</td>\n",
       "      <td>63.025950</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>12.068966</td>\n",
       "      <td>16.379310</td>\n",
       "      <td>18.965517</td>\n",
       "      <td>12.931034</td>\n",
       "      <td>12.068966</td>\n",
       "      <td>2.586207</td>\n",
       "      <td>5.172414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"los ninis\" más ricos y poderosos del país: hi...</td>\n",
       "      <td>\"los ninis\" más ricos y poderosos del país: hi...</td>\n",
       "      <td>5</td>\n",
       "      <td>211</td>\n",
       "      <td>42.200000</td>\n",
       "      <td>3.668246</td>\n",
       "      <td>35.071090</td>\n",
       "      <td>50.236967</td>\n",
       "      <td>50.913142</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>6.635071</td>\n",
       "      <td>16.113744</td>\n",
       "      <td>11.848341</td>\n",
       "      <td>11.374408</td>\n",
       "      <td>9.004739</td>\n",
       "      <td>2.369668</td>\n",
       "      <td>6.161137</td>\n",
       "      <td>5.687204</td>\n",
       "      <td>0.473934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>para todo sacan lo del populismo, ni siquiera ...</td>\n",
       "      <td>gobierno de alfredo del mazo inició con récord...</td>\n",
       "      <td>12</td>\n",
       "      <td>416</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>4.139423</td>\n",
       "      <td>32.932692</td>\n",
       "      <td>46.875000</td>\n",
       "      <td>66.856042</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>6.009615</td>\n",
       "      <td>17.067308</td>\n",
       "      <td>15.625000</td>\n",
       "      <td>9.855769</td>\n",
       "      <td>12.980769</td>\n",
       "      <td>4.807692</td>\n",
       "      <td>10.576923</td>\n",
       "      <td>2.403846</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conapred investiga acto de racismo en el pumas...</td>\n",
       "      <td>conapred investiga acto de racismo en el pumas...</td>\n",
       "      <td>6</td>\n",
       "      <td>227</td>\n",
       "      <td>37.833333</td>\n",
       "      <td>4.101322</td>\n",
       "      <td>35.682819</td>\n",
       "      <td>51.541850</td>\n",
       "      <td>66.635851</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10.572687</td>\n",
       "      <td>15.859031</td>\n",
       "      <td>13.656388</td>\n",
       "      <td>11.013216</td>\n",
       "      <td>13.215859</td>\n",
       "      <td>3.083700</td>\n",
       "      <td>9.251101</td>\n",
       "      <td>2.643172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cristiano ronaldo acepta dos años de prisión\\n...</td>\n",
       "      <td>cristiano ronaldo acepta dos años de prisión</td>\n",
       "      <td>17</td>\n",
       "      <td>590</td>\n",
       "      <td>34.705882</td>\n",
       "      <td>4.276271</td>\n",
       "      <td>24.576271</td>\n",
       "      <td>35.932203</td>\n",
       "      <td>46.584855</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>5.084746</td>\n",
       "      <td>17.966102</td>\n",
       "      <td>14.237288</td>\n",
       "      <td>12.711864</td>\n",
       "      <td>10.677966</td>\n",
       "      <td>1.864407</td>\n",
       "      <td>8.474576</td>\n",
       "      <td>3.050847</td>\n",
       "      <td>2.372881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  sofía castro y alejandro peña pretelini: una i...   \n",
       "1  un paso más cerca de hacer los exámenes 'onlin...   \n",
       "2  esto es lo que los científicos realmente piens...   \n",
       "3  inicia impresión de boletas para elección pres...   \n",
       "4  a *number* día del mundial\\nfifa.com sigue la ...   \n",
       "5  interpol ordena detención inmediata de osorio ...   \n",
       "6  \"los ninis\" más ricos y poderosos del país: hi...   \n",
       "7  para todo sacan lo del populismo, ni siquiera ...   \n",
       "8  conapred investiga acto de racismo en el pumas...   \n",
       "9  cristiano ronaldo acepta dos años de prisión\\n...   \n",
       "\n",
       "                                            headline  n_sents  n_words  \\\n",
       "0  sofía castro y alejandro peña pretelini: una i...        8      252   \n",
       "1   un paso más cerca de hacer los exámenes 'online'        9      486   \n",
       "2  esto es lo que los científicos realmente piens...       31      980   \n",
       "3  inicia impresión de boletas para elección pres...       11      369   \n",
       "4                         a *number* día del mundial        5      130   \n",
       "5  interpol ordena detención inmediata de osorio ...        4      116   \n",
       "6  \"los ninis\" más ricos y poderosos del país: hi...        5      211   \n",
       "7  gobierno de alfredo del mazo inició con récord...       12      416   \n",
       "8  conapred investiga acto de racismo en el pumas...        6      227   \n",
       "9       cristiano ronaldo acepta dos años de prisión       17      590   \n",
       "\n",
       "   avg_words_sents  word_size  unique_words        ttr       mltd  n_words_h  \\\n",
       "0        31.500000   4.190476     34.920635  50.000000  58.600559         12   \n",
       "1        54.000000   4.255144     32.716049  44.238683  41.283136         11   \n",
       "2        31.612903   4.815306     26.020408  38.571429  80.551467         12   \n",
       "3        33.545455   4.728997     22.764228  37.398374  50.995314          7   \n",
       "4        26.000000   4.461538     48.461538  60.000000  47.081602          7   \n",
       "5        29.000000   4.793103     48.275862  63.793103  63.025950         13   \n",
       "6        42.200000   3.668246     35.071090  50.236967  50.913142         14   \n",
       "7        34.666667   4.139423     32.932692  46.875000  66.856042         11   \n",
       "8        37.833333   4.101322     35.682819  51.541850  66.635851         10   \n",
       "9        34.705882   4.276271     24.576271  35.932203  46.584855          7   \n",
       "\n",
       "   ...  propn_ratio  noun_ratio  adp_ratio  det_ratio  punct_ratio  \\\n",
       "0  ...    15.079365   15.079365  13.888889  11.507937     9.523810   \n",
       "1  ...    16.666667   15.226337  12.345679  11.111111    18.930041   \n",
       "2  ...     4.795918   17.857143  13.979592  12.755102    11.836735   \n",
       "3  ...     4.065041   22.493225  18.157182  15.176152    11.111111   \n",
       "4  ...    14.615385   14.615385  17.692308  13.846154     9.230769   \n",
       "5  ...    12.068966   16.379310  18.965517  12.931034    12.068966   \n",
       "6  ...     6.635071   16.113744  11.848341  11.374408     9.004739   \n",
       "7  ...     6.009615   17.067308  15.625000   9.855769    12.980769   \n",
       "8  ...    10.572687   15.859031  13.656388  11.013216    13.215859   \n",
       "9  ...     5.084746   17.966102  14.237288  12.711864    10.677966   \n",
       "\n",
       "   pron_ratio  verb_ratio  adv_ratio  sym_ratio  label  \n",
       "0    5.555556    6.349206   3.174603   0.000000      1  \n",
       "1    1.234568    3.292181   1.646091   1.851852      1  \n",
       "2    2.551020   10.306122   4.693878   0.510204      1  \n",
       "3    2.710027    7.317073   0.271003   1.355014      1  \n",
       "4    3.076923    4.615385   1.538462   2.307692      1  \n",
       "5    2.586207    5.172414   0.000000   0.862069      0  \n",
       "6    2.369668    6.161137   5.687204   0.473934      0  \n",
       "7    4.807692   10.576923   2.403846   0.480769      1  \n",
       "8    3.083700    9.251101   2.643172   0.000000      1  \n",
       "9    1.864407    8.474576   3.050847   2.372881      1  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
